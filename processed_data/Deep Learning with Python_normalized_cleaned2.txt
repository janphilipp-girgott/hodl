second edition fran ois chollet manning deep learning with python deep learning with python second edition fran ois chollet manning shelter island for online information and ordering of this and other manning books please visit www.manning.com. the publisher offers discounts on this book when ordered in quantity. for more information please contact special sales department manning publications co. 20 baldwin road po box 761 shelter island ny 11964 email orders@manning.com 2021 by manning publications co. all rights reserved. no part of this publication may be reproduced stored in a retrieval system or transmitted in any form or by means electronic mechanical photocopying or otherwise without prior written permission of the publisher. many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. where those designations appear in the book and manning publications was aware of a trademark claim the designations have been printed in initial caps or all caps. recognizing the importance of preserving what has been written it is manning s policy to have the books we publish printed on acid-free paper and we exert our best efforts to that end. recognizing also our responsibility to conserve the resources of our planet manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine. the author and publisher have made every effort to ensure that the information in this book was correct at press time. the author and publisher do not assume and hereby disclaim any liability to any party for any loss damage or disruption caused by errors or omissions whether such errors or omissions result from negligence accident or any other cause or from any usage of the information herein. development editor jennifer stout technical development editor frances buontempo manning publications co. review editor aleksandar dragosavljevic 20 baldwin road production editor keri hales po box 761 copy editor andy carroll shelter island ny 11964 proofreaders katie tennant and melody dolab technical proofreader karsten str b k typesetter dennis dalinnik cover designer marija tudor isbn 9781617296864 printed in the united states of america to my son sylvain i hope you ll read this book someday! brief contents 1 what is deep learning? 1 2 the mathematical building blocks of neural networks 26 3 introduction to keras and tensorflow 68 4 getting started with neural networks classification and regression 95 5 fundamentals of machine learning 121 6 the universal workflow of machine learning 153 7 working with keras a deep dive 172 8 introduction to deep learning for computer vision 201 9 advanced deep learning for computer vision 238 10 deep learning for timeseries 280 11 deep learning for text 309 12 generative deep learning 364 13 best practices for the real world 412 14 conclusions 431 vii contents preface xvii acknowledgments xix about this book xx about the author xxiii about the cover illustration xxiv 1 what is deep learning? 1 1.1 artificial intelligence machine learning and deep learning 2 artificial intelligence 2 machine learning 3 learning rules and representations from data 4 the deep in deep learning 7 understanding how deep learning works in three figures 8 what deep learning has achieved so far 10 don t believe the short-term hype 11 the promise of ai 12 1.2 before deep learning a brief history of machine learning 13 probabilistic modeling 13 early neural networks 14 kernel methods 14 decision trees random forests and gradient boosting machines 15 back to neural networks 16 what makes deep learning different 17 the modern machine learning landscape 18 ix contents 1.3 why deep learning? why now? 20 hardware 20 data 21 algorithms 22 a new wave of investment 23 the democratization of deep learning 24 will it last? 24 2 the mathematical building blocks of neural networks 26 2.1 a first look at a neural network 27 2.2 data representations for neural networks 31 scalars (rank-0 tensors) 31 vectors (rank-1 tensors) 31 matrices (rank-2 tensors) 32 rank-3 and higher-rank tensors 32 key attributes 32 manipulating tensors in numpy 34 the notion of data batches 35 real-world examples of data tensors 35 vector data 35 timeseries data or sequence data 36 image data 37 video data 37 2.3 the gears of neural networks tensor operations 38 element-wise operations 38 broadcasting 40 tensor product 41 tensor reshaping 43 geometric interpretation of tensor operations 44 a geometric interpretation of deep learning 47 2.4 the engine of neural networks gradient-based optimization 48 what s a derivative? 49 derivative of a tensor operation the gradient 51 stochastic gradient descent 52 chaining derivatives the backpropagation algorithm 55 2.5 looking back at our first example 61 reimplementing our first example from scratch in tensorflow 63 running one training step 64 the full training loop 65 evaluating the model 66 3 introduction to keras and tensorflow 68 3.1 what s tensorflow? 69 3.2 what s keras? 69 3.3 keras and tensorflow a brief history 71 3.4 setting up a deep learning workspace 71 jupyter notebooks the preferred way to run deep learning experiments 72 using colaboratory 73 3.5 first steps with tensorflow 75 constant tensors and variables 76 tensor operations doing math in tensorflow 78 a second look at the gradienttape api 78 an end-to-end example a linear classifier in pure tensorflow 79 contents 3.6 anatomy of a neural network understanding core keras apis 84 layers the building blocks of deep learning 84 from layers to models 87 the compile step configuring the learning process 88 picking a loss function 90 understanding the fit() method 91 monitoring loss and metrics on validation data 91 inference using a model after training 93 4 getting started with neural networks classification and regression 95 4.1 classifying movie reviews a binary classification example 97 the imdb dataset 97 preparing the data 98 building your model 99 validating your approach 102 using a trained model to generate predictions on new data 105 further experiments 105 wrapping up 106 4.2 classifying newswires a multiclass classification example 106 the reuters dataset 106 preparing the data 107 building your model 108 validating your approach 109 generating predictions on new data 111 a different way to handle the labels and the loss 112 the importance of having sufficiently large intermediate layers 112 further experiments 113 wrapping up 113 4.3 predicting house prices a regression example 113 the boston housing price dataset 114 preparing the data 114 building your model 115 validating your approach using k-fold validation 115 generating predictions on new data 119 wrapping up 119 5 fundamentals of machine learning 121 5.1 generalization the goal of machine learning 121 underfitting and overfitting 122 the nature of generalization in deep learning 127 5.2 evaluating machine learning models 133 training validation and test sets 133 beating a common-sense baseline 136 things to keep in mind about model evaluation 137 5.3 improving model fit 138 tuning key gradient descent parameters 138 leveraging better architecture priors 139 increasing model capacity 140 5.4 improving generalization 142 dataset curation 142 feature engineering 143 using early stopping 144 regularizing your model 145 6 the universal workflow of machine learning 153 6.1 define the task 155 frame the problem 155 collect a dataset 156 understand your data 160 choose a measure of success 160 6.2 develop a model 161 prepare the data 161 choose an evaluation protocol 162 beat a baseline 163 scale up develop a model that overfits 164 regularize and tune your model 165 6.3 deploy the model 165 explain your work to stakeholders and set expectations 165 ship an inference model 166 monitor your model in the wild 169 maintain your model 170 7 working with keras a deep dive 172 7.1 a spectrum of workflows 173 7.2 different ways to build keras models 173 the sequential model 174 the functional api 176 subclassing the model class 182 mixing and matching different components 184 remember use the right tool for the job 185 7.3 using built-in training and evaluation loops 185 writing your own metrics 186 using callbacks 187 writing your own callbacks 189 monitoring and visualization with tensorboard 190 7.4 writing your own training and evaluation loops 192 training versus inference 194 low-level usage of metrics 195 a complete training and evaluation loop 195 make it fast with tf.function 197 leveraging fit() with a custom training loop 198 8 introduction to deep learning for computer vision 201 8.1 introduction to convnets 202 the convolution operation 204 the max-pooling operation 209 8.2 training a convnet from scratch on a small dataset 211 the relevance of deep learning for small-data problems 212 downloading the data 212 building the model 215 data preprocessing 217 using data augmentation 221 8.3 leveraging a pretrained model 224 feature extraction with a pretrained model 225 fine-tuning a pretrained model 234 9 advanced deep learning for computer vision 238 9.1 three essential computer vision tasks 238 9.2 an image segmentation example 240 9.3 modern convnet architecture patterns 248 modularity hierarchy and reuse 249 residual connections 251 batch normalization 255 depthwise separable convolutions 257 putting it together a mini xception-like model 259 9.4 interpreting what convnets learn 261 visualizing intermediate activations 262 visualizing convnet filters 268 visualizing heatmaps of class activation 273 10 deep learning for timeseries 280 10.1 different kinds of timeseries tasks 280 10.2 a temperature-forecasting example 281 preparing the data 285 a common-sense non-machine learning baseline 288 let s try a basic machine learning model 289 let s try a 1d convolutional model 290 a first recurrent baseline 292 10.3 understanding recurrent neural networks 293 a recurrent layer in keras 296 10.4 advanced use of recurrent neural networks 300 using recurrent dropout to fight overfitting 300 stacking recurrent layers 303 using bidirectional rnns 304 going even further 307 11 deep learning for text 309 11.1 natural language processing the bird s eye view 309 11.2 preparing text data 311 text standardization 312 text splitting (tokenization) 313 vocabulary indexing 314 using the textvectorization layer 316 11.3 two approaches for representing groups of words sets and sequences 319 preparing the imdb movie reviews data 320 processing words as a set the bag-of-words approach 322 processing words as a sequence the sequence model approach 327 11.4 the transformer architecture 336 understanding self-attention 337 multi-head attention 341 the transformer encoder 342 when to use sequence models over bag-of-words models 349 11.5 beyond text classification sequence-to-sequence learning 350 a machine translation example 351 sequence-to-sequence learning with rnns 354 sequence-to-sequence learning with transformer 358 12 generative deep learning 364 12.1 text generation 366 a brief history of generative deep learning for sequence generation 366 how do you generate sequence data? 367 the importance of the sampling strategy 368 implementing text generation with keras 369 a text-generation callback with variable-temperature sampling 372 wrapping up 376 12.2 deepdream 376 implementing deepdream in keras 377 wrapping up 383 12.3 neural style transfer 383 the content loss 384 the style loss 384 neural style transfer in keras 385 wrapping up 391 12.4 generating images with variational autoencoders 391 sampling from latent spaces of images 391 concept vectors for image editing 393 variational autoencoders 393 implementing a vae with keras 396 wrapping up 401 12.5 introduction to generative adversarial networks 401 a schematic gan implementation 402 a bag of tricks 403 getting our hands on the celeba dataset 404 the discriminator 405 the generator 407 the adversarial network 408 wrapping up 410 13 best practices for the real world 412 13.1 getting the most out of your models 413 hyperparameter optimization 413 model ensembling 420 13.2 scaling-up model training 421 speeding up training on gpu with mixed precision 422 multi-gpu training 425 tpu training 428 contents 14 conclusions 431 14.1 key concepts in review 432 various approaches to ai 432 what makes deep learning special within the field of machine learning 432 how to think about deep learning 433 key enabling technologies 434 the universal machine learning workflow 435 key network architectures 436 the space of possibilities 440 14.2 the limitations of deep learning 442 the risk of anthropomorphizing machine learning models 443 automatons vs. intelligent agents 445 local generalization vs. extreme generalization 446 the purpose of intelligence 448 climbing the spectrum of generalization 449 14.3 setting the course toward greater generality in ai 450 on the importance of setting the right objective the shortcut rule 450 a new target 452 14.4 implementing intelligence the missing ingredients 454 intelligence as sensitivity to abstract analogies 454 the two poles of abstraction 455 the missing half of the picture 458 14.5 the future of deep learning 459 models as programs 460 blending together deep learning and program synthesis 461 lifelong learning and modular subroutine reuse 463 the long-term vision 465 14.6 staying up to date in a fast-moving field 466 practice on real-world problems using kaggle 466 read about the latest developments on arxiv 466 explore the keras ecosystem 467 14.7 final words 467 index 469 preface if you ve picked up this book you re probably aware of the extraordinary progress that deep learning has represented for the field of artificial intelligence in the recent past. we went from near-unusable computer vision and natural language processing to highly performant systems deployed at scale in products you use every day. the consequences of this sudden progress extend to almost every industry. we re already applying deep learning to an amazing range of important problems across domains as different as medical imaging agriculture autonomous driving education disaster prevention and manufacturing. yet i believe deep learning is still in its early days. it has only realized a small frac.tion of its potential so far. over time it will make its way to every problem where it can help a transformation that will take place over multiple decades. in order to begin deploying deep learning technology to every problem that it could solve we need to make it accessible to as many people as possible including non-experts people who aren t researchers or graduate students. for deep learning to reach its full potential we need to radically democratize it. and today i believe that we re at the cusp of a historical transition where deep learning is moving out of aca.demic labs and the r&d departments of large tech companies to become a ubiquitous part of the toolbox of every developer out there not unlike the trajectory of web development in the late 1990s. almost anyone can now build a website or web app for their business or community of a kind that would have required a small team of special.ist engineers in 1998. in the not-so-distant future anyone with an idea and basic coding skills will be able to build smart applications that learn from data. xvii when i released the first version of the keras deep learning framework in march 2015 the democratization of ai wasn t what i had in mind. i had been doing research in machine learning for several years and had built keras to help me with my own experiments. but since 2015 hundreds of thousands of newcomers have entered the field of deep learning many of them picked up keras as their tool of choice. as i watched scores of smart people use keras in unexpected powerful ways i came to care deeply about the accessibility and democratization of ai. i realized that the fur.ther we spread these technologies the more useful and valuable they become. accessi.bility quickly became an explicit goal in the development of keras and over a few short years the keras developer community has made fantastic achievements on this front. we ve put deep learning into the hands of hundreds of thousands of people who in turn are using it to solve problems that were until recently thought to be unsolvable. the book you re holding is another step on the way to making deep learning avail.able to as many people as possible. keras had always needed a companion course to simultaneously cover the fundamentals of deep learning deep learning best practices and keras usage patterns. in 2016 and 2017 i did my best to produce such a course which became the first edition of this book released in december 2017. it quickly became a machine learning best seller that sold over 50000 copies and was translated into 12 languages. however the field of deep learning advances fast. since the release of the first edi.tion many important developments have taken place the release of tensorflow 2 the growing popularity of the transformer architecture and more. and so in late 2019 i set out to update my book. i originally thought quite naively that it would fea.ture about 50% new content and would end up being roughly the same length as the first edition. in practice after two years of work it turned out to be over a third lon.ger with about 75% novel content. more than a refresh it is a whole new book. i wrote it with a focus on making the concepts behind deep learning and their implementation as approachable as possible. doing so didn t require me to dumb down anything i strongly believe that there are no difficult ideas in deep learning. i hope you ll find this book valuable and that it will enable you to begin building intelli.gent applications and solve the problems that matter to you. acknowledgments first of all i d like to thank the keras community for making this book possible. over the past six years keras has grown to have hundreds of open source contributors and more than one million users. your contributions and feedback have turned keras into what it is today. on a more personal note i d like to thank my wife for her endless support during the development of keras and the writing of this book. i d also like to thank google for backing the keras project. it has been fantastic to see keras adopted as tensorflow s high-level api. a smooth integration between keras and tensorflow greatly benefits both tensorflow users and keras users and makes deep learning accessible to most. i want to thank the people at manning who made this book possible publisher marjan bace and everyone on the editorial and production teams including michael stephens jennifer stout aleksandar dragosavljevic and many others who worked behind the scenes. many thanks go to the technical peer reviewers billy o callaghan christian weisstanner conrad taylor daniela zapata riesco david jacobs edmon begoli edmund ronald phd hao liu jared duncan kee nam ken fricklas kjell jansson milan arenac nguyen cao nikos kanakaris oliver korten raushan jha sayak paul sergio govoni shashank polasa todd cook and viton vitanis and all the other people who sent us feedback on the draft on the book. on the technical side special thanks go to frances buontempo who served as the book s technical editor and karsten str b k who served as the book s technical proofreader. xix about this book this book was written for anyone who wishes to explore deep learning from scratch or broaden their understanding of deep learning. whether you re a practicing machine learning engineer a software developer or a college student you ll find value in these pages. you ll explore deep learning in an approachable way starting simply then work.ing up to state-of-the-art techniques. you ll find that this book strikes a balance between intuition theory and hands-on practice. it avoids mathematical notation preferring instead to explain the core ideas of machine learning and deep learning via detailed code snippets and intuitive mental models. you ll learn from abundant code examples that include extensive commentary practical recommendations and simple high-level explanations of everything you need to know to start using deep learning to solve con.crete problems. the code examples use the python deep learning framework keras with tensor-flow 2 as its numerical engine. they demonstrate modern keras and tensorflow 2 best practices as of 2021. after reading this book you ll have a solid understand of what deep learning is when it s applicable and what its limitations are. you ll be familiar with the standard workflow for approaching and solving machine learning problems and you ll know how to address commonly encountered issues. you ll be able to use keras to tackle real-world problems ranging from computer vision to natural language processing image classification image segmentation timeseries forecasting text classification machine translation text generation and more. xx who should read this book this book is written for people with python programming experience who want to get started with machine learning and deep learning. but this book can also be valuable to many different types of readers if you re a data scientist familiar with machine learning this book will provide you with a solid practical introduction to deep learning the fastest-growing and most significant subfield of machine learning. if you re a deep learning researcher or practitioner looking to get started with the keras framework you ll find this book to be the ideal keras crash course. if you re a graduate student studying deep learning in a formal setting you ll find this book to be a practical complement to your education helping you build intuition around the behavior of deep neural networks and familiarizing you with key best practices. even technically minded people who don t code regularly will find this book useful as an introduction to both basic and advanced deep learning concepts. in order to understand the code examples you ll need reasonable python profi.ciency. additionally familiarity with the numpy library will be helpful although it isn t required. you don t need previous experience with machine learning or deep learning this book covers from scratch all the necessary basics. you don t need an advanced mathematics background either high school level mathematics should suffice in order to follow along. about the code this book contains many examples of source code both in numbered listings and in line with normal text. in both cases source code is formatted in a fixed-width font likethis to separate it from ordinary text. in many cases the original source code has been reformatted we ve added line breaks and reworked indentation to accommodate the available page space in the book. additionally comments in the source code have often been removed from the listings when the code is described in the text. code annotations accompany many of the listings highlighting important concepts. all code examples in this book are available from the manning website at https// www.manning.com/books/deep-learning-with-python-second-edition and as jupyter notebooks on github at https//github.com/fchollet/deep-learning-with-python.notebooks. they can be run directly in your browser via google colaboratory a hosted jupyter notebook environment that you can use for free. an internet connec.tion and a desktop web browser are all you need to get started with deep learning. livebook discussion forum purchase of deep learning with python second edition includes free access to a private web forum run by manning publications where you can make comments about the about this book book ask technical questions and receive help from the author and from other users. to access the forum go to https//livebook.manning.com/#!/book/deep-learning.with-python-second-edition/discussion. you can also learn more about manning s forums and the rules of conduct at https//livebook.manning.com/#!/discussion. manning s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and the author can take place. it is not a commitment to any specific amount of participation on the part of the author whose contribution to the forum remains voluntary (and unpaid). we sug.gest you try asking the author some challenging questions lest his interest stray! the forum and the archives of previous discussions will be accessible from the publisher s website as long as the book is in print. about the author fran ois chollet is the creator of keras one of the most widely used deep learning frameworks. he is currently a soft.ware engineer at google where he leads the keras team. in addition he does research on abstraction reasoning and how to achieve greater generality in artificial intelligence. xxiii about the cover illustration the figure on the cover of deep learning with python second edition is captioned habit of a persian lady in 1568. the illustration is taken from thomas jefferys a collection of the dresses of different nations ancient and modern (four volumes) london published between 1757 and 1772. the title page states that these are hand-colored copperplate engravings heightened with gum arabic. thomas jefferys (1719 1771) was called geographer to king george iii. he was an english cartographer who was the leading map supplier of his day. he engraved and printed maps for government and other official bodies and produced a wide range of commercial maps and atlases especially of north america. his work as a map maker sparked an interest in local dress customs of the lands he surveyed and mapped which are brilliantly displayed in this collection. fascination with faraway lands and travel for pleasure were relatively new phenomena in the late eighteenth century and collections such as this one were popular introducing both the tourist as well as the armchair traveler to the inhabitants of other countries. the diversity of the drawings in jefferys volumes speaks vividly of the uniqueness and individuality of the world s nations some 200 years ago. dress codes have changed since then and the diversity by region and country so rich at the time has faded away. it s now often hard to tell the inhabitants of one continent from another. perhaps try.ing to view it optimistically we ve traded a cultural and visual diversity for a more varied personal life or a more varied and interesting intellectual and technical life. at a time when it s difficult to tell one computer book from another manning cel.ebrates the inventiveness and initiative of the computer business with book covers based on the rich diversity of regional life of two centuries ago brought back to life by jefferys pictures. xxiv what is deep learning? this chapter covers high-level definitions of fundamental concepts timeline of the development of machine learning key factors behind deep learning s rising popularity and future potential in the past few years artificial intelligence (ai) has been a subject of intense media hype. machine learning deep learning and ai come up in countless articles often outside of technology-minded publications. we re promised a future of intelligent chatbots self-driving cars and virtual assistants a future sometimes painted in a grim light and other times as utopian where human jobs will be scarce and most economic activity will be handled by robots or ai agents. for a future or current practitioner of machine learning it s important to be able to recognize the signal amid the noise so that you can tell world-changing developments from overhyped press releases. our future is at stake and it s a future in which you have an active role to play after reading this book you ll be one of those who develop those ai systems. so let s tackle these questions what has deep learning achieved so far? how significant is it? where are we headed next? should you believe the hype? this chapter provides essential context around artificial intelligence machine learning and deep learning. 1 chapter 1 what is deep learning? 1.1 artificial intelligence machine learning and deep learning first we need to define clearly what we re talking about when we mention ai. what are artificial intelligence machine learning and deep learning (see figure 1.1)? how do they relate to each other? figure 1.1 artificial intelligence machine learning and deep learning 1.1.1 artificial intelligence artificial intelligence was born in the 1950s when a handful of pioneers from the nascent field of computer science started asking whether computers could be made to think a question whose ramifications we re still exploring today. while many of the underlying ideas had been brewing in the years and even decades prior artificial intelligence finally crystallized as a field of research in 1956 when john mccarthy then a young assistant professor of mathematics at dartmouth college organized a summer workshop under the following proposal the study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. an attempt will be made to find how to make machines use language form abstractions and concepts solve kinds of problems now reserved for humans and improve themselves. we think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer. at the end of the summer the workshop concluded without having fully solved the riddle it set out to investigate. nevertheless it was attended by many people who would move on to become pioneers in the field and it set in motion an intellectual revolution that is still ongoing to this day. concisely ai can be described as the effort to automate intellectual tasks normally per.formed by humans. as such ai is a general field that encompasses machine learning and deep learning but that also includes many more approaches that may not involve any learning. consider that until the 1980s most ai textbooks didn t mention learning at artificial intelligence machine learning and deep learning all! early chess programs for instance only involved hardcoded rules crafted by pro.grammers and didn t qualify as machine learning. in fact for a fairly long time most experts believed that human-level artificial intelligence could be achieved by having programmers handcraft a sufficiently large set of explicit rules for manipulating knowledge stored in explicit databases. this approach is known as symbolic ai. it was the dominant paradigm in ai from the 1950s to the late 1980s and it reached its peak popularity during the expert systems boom of the 1980s. although symbolic ai proved suitable to solve well-defined logical problems such as playing chess it turned out to be intractable to figure out explicit rules for solving more complex fuzzy problems such as image classification speech recogni.tion or natural language translation. a new approach arose to take symbolic ai s place machine learning. 1.1.2 machine learning in victorian england lady ada lovelace was a friend and collaborator of charles babbage the inventor of the analytical engine the first-known general-purpose mechanical computer. although visionary and far ahead of its time the analytical engine wasn t meant as a general-purpose computer when it was designed in the 1830s and 1840s because the concept of general-purpose computation was yet to be invented. it was merely meant as a way to use mechanical operations to automate cer.tain computations from the field of mathematical analysis hence the name analyti.cal engine. as such it was the intellectual descendant of earlier attempts at encoding mathematical operations in gear form such as the pascaline or leibniz s step reck.oner a refined version of the pascaline. designed by blaise pascal in 1642 (at age 19!) the pascaline was the world s first mechanical calculator it could add subtract mul.tiply or even divide digits. in 1843 ada lovelace remarked on the invention of the analytical engine the analytical engine has no pretensions whatever to originate anything. it can do whatever we know how to order it to perform. . . . its province is to assist us in making available what we re already acquainted with. even with 178 years of historical perspective lady lovelace s observation remains arresting. could a general-purpose computer originate anything or would it always be bound to dully execute processes we humans fully understand? could it ever be capable of any original thought? could it learn from experience? could it show creativity? her remark was later quoted by ai pioneer alan turing as lady lovelace s objec. tion in his landmark 1950 paper computing machinery and intelligence 1 which introduced the turing test as well as key concepts that would come to shape ai.2 turing 1 a.m. turing computing machinery and intelligence mind 59 no. 236 (1950) 433 460. 2 although the turing test has sometimes been interpreted as a literal test a goal the field of ai should set out to reach turing merely meant it as a conceptual device in a philosophical discussion about the nature of cognition. chapter 1 what is deep learning? was of the opinion highly provocative at the time that computers could in princi.ple be made to emulate all aspects of human intelligence. the usual way to make a computer do useful work is to have a human programmer write down rules a computer program to be followed to turn input data into appro.priate answers just like lady lovelace writing down step-by-step instructions for the analytical engine to perform. machine learning turns this around the machine looks at the input data and the corresponding answers and figures out what the rules should be (see figure 1.2). a machine learning system is trained rather than explicitly programmed. it s presented with many examples relevant to a task and it finds statisti.cal structure in these examples that eventually allows the system to come up with rules for automating the task. for instance if you wished to automate the task of tagging your vacation pictures you could present a machine learning system with many exam.ples of pictures already tagged by humans and the system would learn statistical rules for associating specific pictures to specific tags. rules answers data data rules figure 1.2 machine learning answers a new programming paradigm although machine learning only started to flourish in the 1990s it has quickly become the most popular and most successful subfield of ai a trend driven by the availability of faster hardware and larger datasets. machine learning is related to math.ematical statistics but it differs from statistics in several important ways in the same sense that medicine is related to chemistry but cannot be reduced to chemistry as medicine deals with its own distinct systems with their own distinct properties. unlike statistics machine learning tends to deal with large complex datasets (such as a data.set of millions of images each consisting of tens of thousands of pixels) for which clas.sical statistical analysis such as bayesian analysis would be impractical. as a result machine learning and especially deep learning exhibits comparatively little mathe.matical theory maybe too little and is fundamentally an engineering discipline. unlike theoretical physics or mathematics machine learning is a very hands-on field driven by empirical findings and deeply reliant on advances in software and hardware. 1.1.3 learning rules and representations from data to define deep learning and understand the difference between deep learning and other machine learning approaches first we need some idea of what machine learning algorithms do. we just stated that machine learning discovers rules for executing a artificial intelligence machine learning and deep learning data processing task given examples of what s expected. so to do machine learning we need three things input data points for instance if the task is speech recognition these data points could be sound files of people speaking. if the task is image tagging they could be pictures. examples of the expected output in a speech-recognition task these could be human-generated transcripts of sound files. in an image task expected outputs could be tags such as dog cat and so on. a way to measure whether the algorithm is doing a good job this is necessary in order to determine the distance between the algorithm s current output and its expected output. the measurement is used as a feedback signal to adjust the way the algorithm works. this adjustment step is what we call learning. a machine learning model transforms its input data into meaningful outputs a pro.cess that is learned from exposure to known examples of inputs and outputs. there.fore the central problem in machine learning and deep learning is to meaningfully transform data in other words to learn useful representations of the input data at hand representations that get us closer to the expected output. before we go any further what s a representation? at its core it s a different way to look at data to represent or encode data. for instance a color image can be encoded in the rgb format (red-green-blue) or in the hsv format (hue-saturation-value) these are two different representations of the same data. some tasks that may be diffi.cult with one representation can become easy with another. for example the task select all red pixels in the image is simpler in the rgb format whereas make the image less saturated is simpler in the hsv format. machine learning models are all about finding appropriate representations for their input data transformations of the data that make it more amenable to the task at hand. let s make this concrete. consider an x-axis a y-axis and y some points represented by their coordinates in the (x y) sys.tem as shown in figure 1.3. as you can see we have a few white points and a few black points. let s say we want to develop an algorithm that can take the coordinates (x y) of a point and output whether that point is likely to be black or to be white. in this case the inputs are the coordinates of our points. the expected outputs are the colors of our points. figure 1.3 some a way to measure whether our algorithm is doing a good sample data job could be for instance the percentage of points that are being correctly classified. what we need here is a new representation of our data that cleanly separates the white points from the black points. one transformation we could use among many other possibilities would be a coordinate change illustrated in figure 1.4. chapter 1 what is deep learning? 1 raw data 2 coordinate change 3 better representation in this new coordinate system the coordinates of our points can be said to be a new representation of our data. and it s a good one! with this representation the black/white classification problem can be expressed as a simple rule black points are such that x > 0 or white points are such that x  0 ) and so on. artificial intelligence machine learning and deep learning machine learning algorithms aren t usually creative in finding these transformations they re merely searching through a predefined set of operations called a hypothesis space. for instance the space of all possible coordinate changes would be our hypothesis space in the 2d coordinates classification example. so that s what machine learning is concisely searching for useful representations and rules over some input data within a predefined space of possibilities using guid.ance from a feedback signal. this simple idea allows for solving a remarkably broad range of intellectual tasks from speech recognition to autonomous driving. now that you understand what we mean by learning let s take a look at what makes deep learning special. 1.1.4 the deep in deep learning deep learning is a specific subfield of machine learning a new take on learning rep.resentations from data that puts an emphasis on learning successive layers of increas.ingly meaningful representations. the deep in deep learning isn t a reference to any kind of deeper understanding achieved by the approach rather it stands for this idea of successive layers of representations. how many layers contribute to a model of the data is called the depth of the model. other appropriate names for the field could have been layered representations learning or hierarchical representations learn.ing. modern deep learning often involves tens or even hundreds of successive layers of representations and they re all learned automatically from exposure to training data. meanwhile other approaches to machine learning tend to focus on learning only one or two layers of representations of the data (say taking a pixel histogram and then applying a classification rule) hence they re sometimes called shallow learning. in deep learning these layered representations are learned via models called neu.ral networks structured in literal layers stacked on top of each other. the term neural network refers to neurobiology but although some of the central concepts in deep learning were developed in part by drawing inspiration from our understanding of the brain (in particular the visual cortex) deep learning models are not models of the brain. there s no evidence that the brain implements anything like the learning mechanisms used in modern deep learning models. you may come across pop-science articles proclaiming that deep learning works like the brain or was modeled after the brain but that isn t the case. it would be confusing and counterproductive for new.comers to the field to think of deep learning as being in any way related to neurobiol.ogy you don t need that shroud of just like our minds mystique and mystery and you may as well forget anything you may have read about hypothetical links between deep learning and biology. for our purposes deep learning is a mathematical frame.work for learning representations from data. what do the representations learned by a deep learning algorithm look like? let s examine how a network several layers deep (see figure 1.5) transforms an image of a digit in order to recognize what digit it is. chapter 1 what is deep learning? layer 1 layer 2 layer 3 layer 4 original 0 input 1 2 3 4 final 5 output 6 7 8 figure 1.5 a deep neural network 9 for digit classification as you can see in figure 1.6 the network transforms the digit image into representa.tions that are increasingly different from the original image and increasingly informa.tive about the final result. you can think of a deep network as a multistage information-distillation process where information goes through successive filters and comes out increasingly purified (that is useful with regard to some task). layer 1 layer 2 layer 3 representations representations representations so that s what deep learning is technically a multistage way to learn data representa.tions. it s a simple idea but as it turns out very simple mechanisms sufficiently scaled can end up looking like magic. 1.1.5 understanding how deep learning works in three figures at this point you know that machine learning is about mapping inputs (such as images) to targets (such as the label cat ) which is done by observing many examples of input and targets. you also know that deep neural networks do this input-to-target mapping via a deep sequence of simple data transformations (layers) and that these artificial intelligence machine learning and deep learning data transformations are learned by exposure to examples. now let s look at how this learning happens concretely. the specification of what a layer does to its input data is stored in the layer s weights which in essence are a bunch of numbers. in technical terms we d say that the transformation implemented by a layer is parameterized by its weights (see figure 1.7). (weights are also sometimes called the parameters of a layer.) in this context learning means finding a set of values for the weights of all layers in a network such that the network will correctly map example inputs to their associated targets. but here s the thing a deep neural network can contain tens of millions of parameters. finding the correct values for all of them may seem like a daunting task especially given that mod.ifying the value of one parameter will affect the behavior of all the others! input x goal nding the right values for these weights to control something first you need to be able to observe it. to control the output of a neural network you need to be able to measure how far this output is from what you expected. this is the job of the loss function of the network also sometimes called the objective function or cost function. the loss function takes the predictions of the network and the true target (what you wanted the network to output) and computes a distance score capturing how well the network has done on this specific example (see figure 1.8). the fundamental trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little in a direction that will lower the loss score for the current example (see figure 1.9). this adjustment is the job of the optimizer which implements what s called the backpropagation algorithm the central algorithm in deep learning. the next chapter explains in more detail how backpropagation works. initially the weights of the network are assigned random values so the network merely implements a series of random transformations. naturally its output is far from what it should ideally be and the loss score is accordingly very high. but with every example the network processes the weights are adjusted a little in the correct direction and the loss score decreases. this is the training loop which repeated a suffi.cient number of times (typically tens of iterations over thousands of examples) yields weight values that minimize the loss function. a network with a minimal loss is one for which the outputs are as close as they can be to the targets a trained network. once again it s a simple mechanism that once scaled ends up looking like magic. 1.1.6 what deep learning has achieved so far although deep learning is a fairly old subfield of machine learning it only rose to prominence in the early 2010s. in the few years since it has achieved nothing short of a revolution in the field producing remarkable results on perceptual tasks and even natural language processing tasks problems involving skills that seem natural and intuitive to humans but have long been elusive for machines. in particular deep learning has enabled the following breakthroughs all in histor.ically difficult areas of machine learning near-human-level image classification near-human-level speech transcription near-human-level handwriting transcription artificial intelligence machine learning and deep learning dramatically improved machine translation dramatically improved text-to-speech conversion digital assistants such as google assistant and amazon alexa near-human-level autonomous driving improved ad targeting as used by google baidu or bing improved search results on the web ability to answer natural language questions superhuman go playing we re still exploring the full extent of what deep learning can do. we ve started apply.ing it with great success to a wide variety of problems that were thought to be impossi.ble to solve just a few years ago automatically transcribing the tens of thousands of ancient manuscripts held in the vatican s apostolic archive detecting and classifying plant diseases in fields using a simple smartphone assisting oncologists or radiologists with interpreting medical imaging data predicting natural disasters such as floods hurricanes or even earthquakes and so on. with every milestone we re getting closer to an age where deep learning assists us in every activity and every field of human endeavor science medicine manufacturing energy transportation software devel.opment agriculture and even artistic creation. 1.1.7 don t believe the short-term hype although deep learning has led to remarkable achievements in recent years expecta.tions for what the field will be able to achieve in the next decade tend to run much higher than what will likely be possible. although some world-changing applications like autonomous cars are already within reach many more are likely to remain elusive for a long time such as believable dialogue systems human-level machine translation across arbitrary languages and human-level natural language understanding. in par.ticular talk of human-level general intelligence shouldn t be taken too seriously. the risk with high expectations for the short term is that as technology fails to deliver research investment will dry up slowing progress for a long time. this has happened before. twice in the past ai went through a cycle of intense optimism followed by disappointment and skepticism with a dearth of funding as a result. it started with symbolic ai in the 1960s. in those early days projections about ai were flying high. one of the best-known pioneers and proponents of the symbolic ai approach was marvin minsky who claimed in 1967 within a generation . . . the problem of creating artificial intelligence will substantially be solved. three years later in 1970 he made a more precisely quantified prediction in from three to eight years we will have a machine with the general intelligence of an average human being. in 2021 such an achievement still appears to be far in the future so far that we have no way to predict how long it will take but in the 1960s and early 1970s several experts believed it to be right around the corner (as do many people today). a few years later as these high expectations failed to materialize researchers and government funds turned away from the field marking the start of the first ai winter (a reference to a nuclear win.ter because this was shortly after the height of the cold war). it wouldn t be the last one. in the 1980s a new take on symbolic ai expert systems started gathering steam among large companies. a few initial success stories triggered a wave of investment with corporations around the world starting their own in-house ai departments to develop expert systems. around 1985 companies were spending over $1 billion each year on the technology but by the early 1990s these systems had proven expensive to maintain difficult to scale and limited in scope and interest died down. thus began the second ai winter. we may be currently witnessing the third cycle of ai hype and disappointment and we re still in the phase of intense optimism. it s best to moderate our expectations for the short term and make sure people less familiar with the technical side of the field have a clear idea of what deep learning can and can t deliver. 1.1.8 the promise of ai although we may have unrealistic short-term expectations for ai the long-term pic.ture is looking bright. we re only getting started in applying deep learning to many important problems for which it could prove transformative from medical diagnoses to digital assistants. ai research has been moving forward amazingly quickly in the past ten years in large part due to a level of funding never before seen in the short history of ai but so far relatively little of this progress has made its way into the prod.ucts and processes that form our world. most of the research findings of deep learning aren t yet applied or at least are not applied to the full range of problems they could solve across all industries. your doctor doesn t yet use ai and neither does your accountant. you probably don t use ai technologies very often in your day-to-day life. of course you can ask your smartphone simple questions and get reasonable answers you can get fairly useful product recommendations on amazon.com and you can search for birthday on google photos and instantly find those pictures of your daughter s birthday party from last month. that s a far cry from where such technolo.gies used to stand. but such tools are still only accessories to our daily lives. ai has yet to transition to being central to the way we work think and live. right now it may seem hard to believe that ai could have a large impact on our world because it isn t yet widely deployed much as back in 1995 it would have been difficult to believe in the future impact of the internet. back then most people didn t see how the internet was relevant to them and how it was going to change their lives. the same is true for deep learning and ai today. but make no mistake ai is coming. in a not-so-distant future ai will be your assistant even your friend it will answer your questions help educate your kids and watch over your health. it will deliver your gro.ceries to your door and drive you from point a to point b. it will be your interface to an increasingly complex and information-intensive world. and even more important ai will help humanity as a whole move forward by assisting human scientists in new breakthrough discoveries across all scientific fields from genomics to mathematics. before deep learning a brief history of machine learning on the way we may face a few setbacks and maybe even a new ai winter in much the same way the internet industry was overhyped in 1998 99 and suffered from a crash that dried up investment throughout the early 2000s. but we ll get there eventu.ally. ai will end up being applied to nearly every process that makes up our society and our daily lives much like the internet is today. don t believe the short-term hype but do believe in the long-term vision. it may take a while for ai to be deployed to its true potential a potential the full extent of which no one has yet dared to dream but ai is coming and it will transform our world in a fantastic way. 1.2 before deep learning a brief history of machine learning deep learning has reached a level of public attention and industry investment never before seen in the history of ai but it isn t the first successful form of machine learn.ing. it s safe to say that most of the machine learning algorithms used in the industry today aren t deep learning algorithms. deep learning isn t always the right tool for the job sometimes there isn t enough data for deep learning to be applicable and some.times the problem is better solved by a different algorithm. if deep learning is your first contact with machine learning you may find yourself in a situation where all you have is the deep learning hammer and every machine learning problem starts to look like a nail. the only way not to fall into this trap is to be familiar with other approaches and practice them when appropriate. a detailed discussion of classical machine learning approaches is outside of the scope of this book but i ll briefly go over them and describe the historical context in which they were developed. this will allow us to place deep learning in the broader context of machine learning and better understand where deep learning comes from and why it matters. 1.2.1 probabilistic modeling probabilistic modeling is the application of the principles of statistics to data analysis. it is one of the earliest forms of machine learning and it s still widely used to this day. one of the best-known algorithms in this category is the naive bayes algorithm. naive bayes is a type of machine learning classifier based on applying bayes theo.rem while assuming that the features in the input data are all independent (a strong or naive assumption which is where the name comes from). this form of data analy.sis predates computers and was applied by hand decades before its first computer implementation (most likely dating back to the 1950s). bayes theorem and the foun.dations of statistics date back to the eighteenth century and these are all you need to start using naive bayes classifiers. a closely related model is logistic regression (logreg for short) which is sometimes considered to be the hello world of modern machine learning. don t be misled by its name logreg is a classification algorithm rather than a regression algorithm. much like naive bayes logreg predates computing by a long time yet it s still useful to this day thanks to its simple and versatile nature. it s often the first thing a data scientist will try on a dataset to get a feel for the classification task at hand. 1.2.2 early neural networks early iterations of neural networks have been completely supplanted by the modern variants covered in these pages but it s helpful to be aware of how deep learning orig.inated. although the core ideas of neural networks were investigated in toy forms as early as the 1950s the approach took decades to get started. for a long time the miss.ing piece was an efficient way to train large neural networks. this changed in the mid.1980s when multiple people independently rediscovered the backpropagation algo.rithm a way to train chains of parametric operations using gradient-descent optimi.zation (we ll precisely define these concepts later in the book) and started applying it to neural networks. the first successful practical application of neural nets came in 1989 from bell labs when yann lecun combined the earlier ideas of convolutional neural networks and backpropagation and applied them to the problem of classifying handwritten digits. the resulting network dubbed lenet was used by the united states postal ser.vice in the 1990s to automate the reading of zip codes on mail envelopes. 1.2.3 kernel methods as neural networks started to gain some respect among researchers in the 1990s thanks to this first success a new approach to machine learning rose to fame and quickly sent neural nets back to oblivion kernel methods. kernel methods are a group of classification algorithms the best known of which is the support vector machine (svm). the modern formulation of an svm was developed by vladimir vapnik and corinna cortes in the early 1990s at bell labs and published in 19953 although an older linear formulation was published by vapnik and alexey chervonenkis as early as 1963.4 svm is a classification algorithm that works by finding deci. sion boundaries separating two classes (see figure 1.10). svms proceed to find these boundaries in two steps 1 the data is mapped to a new high-dimensional represen.tation where the decision boundary can be expressed as a hyperplane (if the data was two-dimensional as in fig.ure 1.10 a hyperplane would be a straight line). 2 a good decision boundary (a separation hyperplane) is computed by trying to maximize the distance between the figure 1.10 hyperplane and the closest data points from each class a a decision boundary 3 vladimir vapnik and corinna cortes support-vector networks machine learning 20 no. 3 (1995) 273 297. 4 vladimir vapnik and alexey chervonenkis a note on one class of perceptrons automation and remote con.trol 25 (1964). before deep learning a brief history of machine learning step called maximizing the margin. this allows the boundary to generalize well to new samples outside of the training dataset. the technique of mapping data to a high-dimensional representation where a classifi.cation problem becomes simpler may look good on paper but in practice it s often computationally intractable. that s where the kernel trick comes in (the key idea that kernel methods are named after). here s the gist of it to find good decision hyper.planes in the new representation space you don t have to explicitly compute the coor.dinates of your points in the new space you just need to compute the distance between pairs of points in that space which can be done efficiently using a kernel function. a kernel function is a computationally tractable operation that maps any two points in your initial space to the distance between these points in your target repre.sentation space completely bypassing the explicit computation of the new representa.tion. kernel functions are typically crafted by hand rather than learned from data in the case of an svm only the separation hyperplane is learned. at the time they were developed svms exhibited state-of-the-art performance on simple classification problems and were one of the few machine learning methods backed by extensive theory and amenable to serious mathematical analysis making them well understood and easily interpretable. because of these useful properties svms became extremely popular in the field for a long time. but svms proved hard to scale to large datasets and didn t provide good results for perceptual problems such as image classification. because an svm is a shallow method applying an svm to perceptual problems requires first extracting useful rep.resentations manually (a step called feature engineering) which is difficult and brittle. for instance if you want to use an svm to classify handwritten digits you can t start from the raw pixels you should first find by hand useful representations that make the problem more tractable like the pixel histograms i mentioned earlier. 1.2.4 decision trees random forests and gradient boosting machines decision trees are flowchart-like structures that let you classify input data points or pre.dict output values given inputs (see figure 1.11). they re easy to visualize and inter.pret. decision trees learned from data began to receive significant research interest in the 2000s and by 2010 they were often preferred to kernel methods. category category category category coefficient 2 in the data greater than 3.5? in particular the random forest algorithm introduced a robust practical take on decision-tree learning that involves building a large number of specialized decision trees and then ensembling their outputs. random forests are applicable to a wide range of problems you could say that they re almost always the second-best algorithm for any shallow machine learning task. when the popular machine learning competition web.site kaggle (http//kaggle.com) got started in 2010 random forests quickly became a favorite on the platform until 2014 when gradient boosting machines took over. a gra.dient boosting machine much like a random forest is a machine learning technique based on ensembling weak prediction models generally decision trees. it uses gradient boosting a way to improve any machine learning model by iteratively training new models that specialize in addressing the weak points of the previous models. applied to decision trees the use of the gradient boosting technique results in models that strictly outperform random forests most of the time while having similar properties. it may be one of the best if not the best algorithm for dealing with nonperceptual data today. alongside deep learning it s one of the most commonly used techniques in kaggle competitions. 1.2.5 back to neural networks around 2010 although neural networks were almost completely shunned by the sci.entific community at large a number of people still working on neural networks started to make important breakthroughs the groups of geoffrey hinton at the uni.versity of toronto yoshua bengio at the university of montreal yann lecun at new york university and idsia in switzerland. in 2011 dan ciresan from idsia began to win academic image-classification com.petitions with gpu-trained deep neural networks the first practical success of mod.ern deep learning. but the watershed moment came in 2012 with the entry of hinton s group in the yearly large-scale image-classification challenge imagenet (imagenet large scale visual recognition challenge or ilsvrc for short). the imagenet challenge was notoriously difficult at the time consisting of classifying high-resolution color images into 1000 different categories after training on 1.4 million images. in 2011 the top-five accuracy of the winning model based on classical approaches to computer vision was only 74.3%.5 then in 2012 a team led by alex krizhevsky and advised by geoffrey hinton was able to achieve a top-five accuracy of 83.6% a significant breakthrough. the competition has been dominated by deep convolutional neural networks every year since. by 2015 the winner reached an accu.racy of 96.4% and the classification task on imagenet was considered to be a com.pletely solved problem. since 2012 deep convolutional neural networks (convnets) have become the go-to algorithm for all computer vision tasks more generally they work on all perceptual 5 top-five accuracy measures how often the model selects the correct answer as part of its top five guesses (out of 1000 possible answers in the case of imagenet). before deep learning a brief history of machine learning tasks. at any major computer vision conference after 2015 it was nearly impossible to find presentations that didn t involve convnets in some form. at the same time deep learning has also found applications in many other types of problems such as natural language processing. it has completely replaced svms and decision trees in a wide range of applications. for instance for several years the european organization for nuclear research cern used decision tree based methods for analyzing particle data from the atlas detector at the large hadron collider (lhc) but cern even.tually switched to keras-based deep neural networks due to their higher performance and ease of training on large datasets. 1.2.6 what makes deep learning different the primary reason deep learning took off so quickly is that it offered better perfor.mance for many problems. but that s not the only reason. deep learning also makes problem-solving much easier because it completely automates what used to be the most crucial step in a machine learning workflow feature engineering. previous machine learning techniques shallow learning only involved transform.ing the input data into one or two successive representation spaces usually via simple transformations such as high-dimensional non-linear projections (svms) or decision trees. but the refined representations required by complex problems generally can t be attained by such techniques. as such humans had to go to great lengths to make the initial input data more amenable to processing by these methods they had to manually engineer good layers of representations for their data. this is called feature engineering. deep learning on the other hand completely automates this step with deep learning you learn all features in one pass rather than having to engineer them yourself. this has greatly simplified machine learning workflows often replacing sophis.ticated multistage pipelines with a single simple end-to-end deep learning model. you may ask if the crux of the issue is to have multiple successive layers of repre.sentations could shallow methods be applied repeatedly to emulate the effects of deep learning? in practice successive applications of shallow-learning methods pro.duce fast-diminishing returns because the optimal first representation layer in a three-layer model isn t the optimal first layer in a one-layer or two-layer model. what is transformative about deep learning is that it allows a model to learn all layers of repre.sentation jointly at the same time rather than in succession (greedily as it s called). with joint feature learning whenever the model adjusts one of its internal features all other features that depend on it automatically adapt to the change without requiring human intervention. everything is supervised by a single feedback signal every change in the model serves the end goal. this is much more powerful than greedily stacking shallow models because it allows for complex abstract representations to be learned by breaking them down into long series of intermediate spaces (layers) each space is only a simple transformation away from the previous one. these are the two essential characteristics of how deep learning learns from data the incremental layer-by-layer way in which increasingly complex representations are developed and the fact that these intermediate incremental representations are learned jointly each layer being updated to follow both the representational needs of the layer above and the needs of the layer below. together these two properties have made deep learning vastly more successful than previous approaches to machine learning. 1.2.7 the modern machine learning landscape a great way to get a sense of the current landscape of machine learning algorithms and tools is to look at machine learning competitions on kaggle. due to its highly competitive environment (some contests have thousands of entrants and million-dollar prizes) and to the wide variety of machine learning problems covered kaggle offers a realistic way to assess what works and what doesn t. so what kind of algorithm is reliably winning competitions? what tools do top entrants use? in early 2019 kaggle ran a survey asking teams that ended in the top five of any competition since 2017 which primary software tool they had used in the competition (see figure 1.12). it turns out that top teams tend to use either deep learning methods (most often via the keras library) or gradient boosted trees (most often via the lightgbm or xgboost libraries). number of competitions deep classic before deep learning a brief history of machine learning it s not just competition champions either. kaggle also runs a yearly survey among machine learning and data science professionals worldwide. with tens of thousands of respondents this survey is one of our most reliable sources about the state of the industry. figure 1.13 shows the percentage of usage of different machine learning software frameworks. scikit-learn tensorflow keras xgboost pytorch lightgbm caret catboost prophet fast.ai tidymodels h2o3 mxnet other none jax from 2016 to 2020 the entire machine learning and data science industry has been dominated by these two approaches deep learning and gradient boosted trees. specif.ically gradient boosted trees is used for problems where structured data is available whereas deep learning is used for perceptual problems such as image classification. users of gradient boosted trees tend to use scikit-learn xgboost or lightgbm. meanwhile most practitioners of deep learning use keras often in combination with its parent framework tensorflow. the common point of these tools is they re all python libraries python is by far the most widely used language for machine learning and data science. these are the two techniques you should be the most familiar with in order to be successful in applied machine learning today gradient boosted trees for shallow-learning problems and deep learning for perceptual problems. in technical terms this means you ll need to be familiar with scikit-learn xgboost and keras the three libraries that currently dominate kaggle competitions. with this book in hand you re already one big step closer. 1.3 why deep learning? why now? the two key ideas of deep learning for computer vision convolutional neural networks and backpropagation were already well understood by 1990. the long short-term memory (lstm) algorithm which is fundamental to deep learning for timeseries was developed in 1997 and has barely changed since. so why did deep learning only take off after 2012? what changed in these two decades? in general three technical forces are driving advances in machine learning hardware datasets and benchmarks algorithmic advances because the field is guided by experimental findings rather than by theory algorith.mic advances only become possible when appropriate data and hardware are available to try new ideas (or to scale up old ideas as is often the case). machine learning isn t mathematics or physics where major advances can be done with a pen and a piece of paper. it s an engineering science. the real bottlenecks throughout the 1990s and 2000s were data and hardware. but here s what happened during that time the internet took off and high-performance graphics chips were developed for the needs of the gaming market. 1.3.1 hardware between 1990 and 2010 off-the-shelf cpus became faster by a factor of approximately 5000. as a result nowadays it s possible to run small deep learning models on your laptop whereas this would have been intractable 25 years ago. but typical deep learning models used in computer vision or speech recognition require orders of magnitude more computational power than your laptop can deliver. throughout the 2000s companies like nvidia and amd invested billions of dollars in developing fast massively parallel chips (graphical processing units or gpus) to power the graphics of increasingly photorealistic video games cheap single-purpose supercomputers designed to render complex 3d scenes on your screen in real time. this investment came to benefit the scientific community when in 2007 nvidia launched cuda (https//developer.nvidia.com/about-cuda) a programming interface why deep learning? why now? for its line of gpus. a small number of gpus started replacing massive clusters of cpus in various highly parallelizable applications beginning with physics modeling. deep neural networks consisting mostly of many small matrix multiplications are also highly parallelizable and around 2011 some researchers began to write cuda implementations of neural nets dan ciresan6 and alex krizhevsky7 were among the first. what happened is that the gaming market subsidized supercomputing for the next generation of artificial intelligence applications. sometimes big things begin as games. today the nvidia titan rtx a gpu that cost $2500 at the end of 2019 can deliver a peak of 16 teraflops in single precision (16 trillion float32 operations per second). that s about 500 times more computing power than the world s fastest super.computer from 1990 the intel touchstone delta. on a titan rtx it takes only a few hours to train an imagenet model of the sort that would have won the ilsvrc com.petition around 2012 or 2013. meanwhile large companies train deep learning mod.els on clusters of hundreds of gpus. what s more the deep learning industry has been moving beyond gpus and is investing in increasingly specialized efficient chips for deep learning. in 2016 at its annual i/o convention google revealed its tensor processing unit (tpu) project a new chip design developed from the ground up to run deep neural networks signifi.cantly faster and far more energy efficient than top-of-the-line gpus. today in 2020 the third iteration of the tpu card represents 420 teraflops of computing power. that s 10000 times more than the intel touchstone delta from 1990. these tpu cards are designed to be assembled into large-scale configurations called pods. one pod (1024 tpu cards) peaks at 100 petaflops. for scale that s about 10% of the peak computing power of the current largest supercomputer the ibm summit at oak ridge national lab which consists of 27000 nvidia gpus and peaks at around 1.1 exaflops. 1.3.2 data ai is sometimes heralded as the new industrial revolution. if deep learning is the steam engine of this revolution then data is its coal the raw material that powers our intelli.gent machines without which nothing would be possible. when it comes to data in addition to the exponential progress in storage hardware over the past 20 years (follow.ing moore s law) the game changer has been the rise of the internet making it feasible to collect and distribute very large datasets for machine learning. today large companies work with image datasets video datasets and natural language datasets that couldn t have been collected without the internet. user-generated image tags on flickr for 6 see flexible high performance convolutional neural networks for image classification proceedings of the 22nd international joint conference on artificial intelligence (2011) www.ijcai.org/proceedings/11/papers/ 210.pdf. 7 see imagenet classification with deep convolutional neural networks advances in neural information pro.cessing systems 25 (2012) http//mng.bz/2286. instance have been a treasure trove of data for computer vision. so are youtube videos. and wikipedia is a key dataset for natural language processing. if there s one dataset that has been a catalyst for the rise of deep learning it s the imagenet dataset consisting of 1.4 million images that have been hand annotated with 1000 image categories (one category per image). but what makes imagenet spe.cial isn t just its large size but also the yearly competition associated with it.8 as kaggle has been demonstrating since 2010 public competitions are an excel.lent way to motivate researchers and engineers to push the envelope. having common benchmarks that researchers compete to beat has greatly helped the rise of deep learning by highlighting its success against classical machine learning approaches. 1.3.3 algorithms in addition to hardware and data until the late 2000s we were missing a reliable way to train very deep neural networks. as a result neural networks were still fairly shal.low using only one or two layers of representations thus they weren t able to shine against more-refined shallow methods such as svms and random forests. the key issue was that of gradient propagation through deep stacks of layers. the feedback signal used to train neural networks would fade away as the number of layers increased. this changed around 2009 2010 with the advent of several simple but important algorithmic improvements that allowed for better gradient propagation better activation functions for neural layers better weight-initialization schemes starting with layer-wise pretraining which was then quickly abandoned better optimization schemes such as rmsprop and adam only when these improvements began to allow for training models with 10 or more layers did deep learning start to shine. finally in 2014 2015 and 2016 even more advanced ways to improve gradient propagation were discovered such as batch normalization residual connections and depthwise separable convolutions. today we can train models that are arbitrarily deep from scratch. this has unlocked the use of extremely large models which hold considerable representa.tional power that is to say which encode very rich hypothesis spaces. this extreme scalability is one of the defining characteristics of modern deep learning. large-scale model architectures which feature tens of layers and tens of millions of parameters have brought about critical advances both in computer vision (for instance architec.tures such as resnet inception or xception) and natural language processing (for instance large transformer-based architectures such as bert gpt-3 or xlnet). 8 the imagenet large scale visual recognition challenge (ilsvrc) www.image-net.org/challenges/lsvrc. why deep learning? why now? 1.3.4 a new wave of investment as deep learning became the new state of the art for computer vision in 2012 2013 and eventually for all perceptual tasks industry leaders took note. what followed was a gradual wave of industry investment far beyond anything previously seen in the his.tory of ai (see figure 1.14). total estimated investments in al start-ups 2011 17 and rst semester 2018 by start-up location us china eu israel canada japan other india usd billion 18 16 14 12 10 8 6 4 2 0 2011 2012 2013 2014 2015 2016 2017 in 2011 right before deep learning took the spotlight the total venture capital invest.ment in ai worldwide was less than a billion dollars which went almost entirely to practical applications of shallow machine learning approaches. in 2015 it had risen to over $5 billion and in 2017 to a staggering $16 billion. hundreds of startups launched in these few years trying to capitalize on the deep learning hype. mean.while large tech companies such as google amazon and microsoft have invested in internal research departments in amounts that would most likely dwarf the flow of venture-capital money. machine learning in particular deep learning has become central to the prod.uct strategy of these tech giants. in late 2015 google ceo sundar pichai stated machine learning is a core transformative way by which we re rethinking how we re doing everything. we re thoughtfully applying it across all our products be it search ads youtube or play. and we re in early days but you ll see us in a systematic way apply machine learning in all these areas. 9 as a result of this wave of investment the number of people working on deep learning went from a few hundred to tens of thousands in less than 10 years and research progress has reached a frenetic pace. 1.3.5 the democratization of deep learning one of the key factors driving this inflow of new faces in deep learning has been the democratization of the toolsets used in the field. in the early days doing deep learn.ing required significant c++ and cuda expertise which few people possessed. nowadays basic python scripting skills suffice to do advanced deep learning research. this has been driven most notably by the development of the now-defunct theano library and then the tensorflow library two symbolic tensor-manipulation frameworks for python that support autodifferentiation greatly simplifying the imple.mentation of new models and by the rise of user-friendly libraries such as keras which makes deep learning as easy as manipulating lego bricks. after its release in early 2015 keras quickly became the go-to deep learning solution for large numbers of new startups graduate students and researchers pivoting into the field. 1.3.6 will it last? is there anything special about deep neural networks that makes them the right approach for companies to be investing in and for researchers to flock to? or is deep learning just a fad that may not last? will we still be using deep neural networks in 20 years? deep learning has several properties that justify its status as an ai revolution and it s here to stay. we may not be using neural networks two decades from now but what.ever we use will directly inherit from modern deep learning and its core concepts. these important properties can be broadly sorted into three categories simplicity deep learning removes the need for feature engineering replacing complex brittle engineering-heavy pipelines with simple end-to-end trainable models that are typically built using only five or six different tensor operations. scalability deep learning is highly amenable to parallelization on gpus or tpus so it can take full advantage of moore s law. in addition deep learning models are trained by iterating over small batches of data allowing them to be trained on datasets of arbitrary size. (the only bottleneck is the amount of parallel computa.tional power available which thanks to moore s law is a fast-moving barrier.) versatility and reusability unlike many prior machine learning approaches deep learning models can be trained on additional data without restarting from 9 sundar pichai alphabet earnings call oct. 22 2015. why deep learning? why now? scratch making them viable for continuous online learning an important property for very large production models. furthermore trained deep learning models are repurposable and thus reusable for instance it s possible to take a deep learning model trained for image classification and drop it into a video-processing pipeline. this allows us to reinvest previous work into increasingly complex and powerful models. this also makes deep learning applicable to fairly small datasets. deep learning has only been in the spotlight for a few years and we may not yet have established the full scope of what it can do. with every passing year we learn about new use cases and engineering improvements that lift previous limitations. following a scientific revolution progress generally follows a sigmoid curve it starts with a period of fast progress which gradually stabilizes as researchers hit hard limitations and then further improvements become incremental. when i was writing the first edition of this book in 2016 i predicted that deep learning was still in the first half of that sigmoid with much more transformative progress to come in the following few years. this has proven true in practice as 2017 and 2018 have seen the rise of transformer-based deep learning models for natural language processing which have been a revolution in the field while deep learning also kept delivering steady progress in computer vision and speech recog.nition. today in 2021 deep learning seems to have entered the second half of that sigmoid. we should still expect significant progress in the years to come but we re probably out of the initial phase of explosive progress. today i m extremely excited about the deployment of deep learning technology to every problem it can solve the list is endless. deep learning is still a revolution in the making and it will take many years to realize its full potential. this chapter covers a first example of a neural network tensors and tensor operations how neural networks learn via backpropagation and gradient descent understanding deep learning requires familiarity with many simple mathematical concepts tensors tensor operations differentiation gradient descent and so on. our goal in this chapter will be to build up your intuition about these notions without get.ting overly technical. in particular we ll steer away from mathematical notation which can introduce unnecessary barriers for those without any mathematics back.ground and isn t necessary to explain things well. the most precise unambiguous description of a mathematical operation is its executable code. to provide sufficient context for introducing tensors and gradient descent we ll begin the chapter with a practical example of a neural network. then we ll go over every new concept that s been introduced point by point. keep in mind that these concepts will be essential for you to understand the practical examples in the fol.lowing chapters! 26 a first look at a neural network after reading this chapter you ll have an intuitive understanding of the mathemat. ical theory behind deep learning and you ll be ready to start diving into keras and tensorflow in chapter 3. 2.1 a first look at a neural network let s look at a concrete example of a neural network that uses the python library keras to learn to classify handwritten digits. unless you already have experience with keras or similar libraries you won t understand everything about this first example right away. that s fine. in the next chapter we ll review each element in the example and explain them in detail. so don t worry if some steps seem arbitrary or look like magic to you! we ve got to start somewhere. the problem we re trying to solve here is to classify grayscale images of handwrit.ten digits (28 28 pixels) into their 10 categories (0 through 9). we ll use the mnist dataset a classic in the machine learning community which has been around almost as long as the field itself and has been intensively studied. it s a set of 60000 training images plus 10000 test images assembled by the national institute of standards and technology (the nist in mnist) in the 1980s. you can think of solving mnist as the hello world of deep learning it s what you do to verify that your algorithms are working as expected. as you become a machine learning practitioner you ll see mnist come up over and over again in scientific papers blog posts and so on. you can see some mnist samples in figure 2.1. note in machine learning a category in a classification problem is called a class. data points are called samples. the class associated with a specific sample is called a label. you don t need to try to reproduce this example on your machine just now. if you wish to you ll first need to set up a deep learning workspace which is covered in chapter 3. the mnist dataset comes preloaded in keras in the form of a set of four numpy arrays. from tensorflow.keras.datasets import mnist (train_images train_labels) (test_images test_labels) = mnist.load_data() train_images and train_labels form the training set the data that the model will learn from. the model will then be tested on the test set test_images and test_labels. the images are encoded as numpy arrays and the labels are an array of digits rang.ing from 0 to 9. the images and labels have a one-to-one correspondence. let s look at the training data >>> train_images.shape (60000 28 28) >>> len(train_labels) 60000 >>> train_labels array([5 0 4 ... 5 6 8] dtype=uint8) and here s the test data >>> test_images.shape (10000 28 28) >>> len(test_labels) 10000 >>> test_labels array([7 2 1 ... 4 5 6] dtype=uint8) the workflow will be as follows first we ll feed the neural network the training data train_images and train_labels. the network will then learn to associate images and labels. finally we ll ask the network to produce predictions for test_images and we ll verify whether these predictions match the labels from test_labels. let s build the network again remember that you aren t expected to understand everything about this example yet. from tensorflow import keras from tensorflow.keras import layers model = keras.sequential([ layers.dense(512 activation="relu") layers.dense(10 activation="softmax") ]) the core building block of neural networks is the layer. you can think of a layer as a fil.ter for data some data goes in and it comes out in a more useful form. specifically layers extract representations out of the data fed into them hopefully representations that are more meaningful for the problem at hand. most of deep learning consists of chaining together simple layers that will implement a form of progressive data distilla.tion. a deep learning model is like a sieve for data processing made of a succession of increasingly refined data filters the layers. here our model consists of a sequence of two dense layers which are densely con.nected (also called fully connected) neural layers. the second (and last) layer is a 10-way softmax classification layer which means it will return an array of 10 probability scores (summing to 1). each score will be the probability that the current digit image belongs to one of our 10 digit classes. a first look at a neural network to make the model ready for training we need to pick three more things as part of the compilation step an optimizer the mechanism through which the model will update itself based on the training data it sees so as to improve its performance. a loss function how the model will be able to measure its performance on the training data and thus how it will be able to steer itself in the right direction. metrics to monitor during training and testing here we ll only care about accu.racy (the fraction of the images that were correctly classified). the exact purpose of the loss function and the optimizer will be made clear through.out the next two chapters. model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) before training we ll preprocess the data by reshaping it into the shape the model expects and scaling it so that all values are in the [0 1] interval. previously our train.ing images were stored in an array of shape (6000028 28) of type uint8 with values in the [0 255] interval. we ll transform it into a float32 array of shape (60000 28* 28) with values between 0 and 1. train_images = train_images.reshape((60000 28 * 28)) train_images = train_images.astype("float32") / 255 test_images = test_images.reshape((10000 28 * 28)) test_images = test_images.astype("float32") / 255 we re now ready to train the model which in keras is done via a call to the model s fit() method we fit the model to its training data. >>> model.fit(train_images train_labels epochs=5 batch_size=128) epoch 1/5 60000/60000 [===========================] - 5s - loss 0.2524 -acc 0.9273 epoch 2/5 51328/60000 [=====================>.....] - eta 1s - loss 0.1035 - acc 0.9692 two quantities are displayed during training the loss of the model over the training data and the accuracy of the model over the training data. we quickly reach an accu.racy of 0.989 (98.9%) on the training data. now that we have a trained model we can use it to predict class probabilities for new digits images that weren t part of the training data like those from the test set. >>> test_digits = test_images >>> predictions = model.predict(test_digits) >>> predictions array([1.0726176e-10 1.6918376e-10 6.1314843e-08 8.4106023e-06 2.9967067e-11 3.0331331e-09 8.3651971e-14 9.9999106e-01 2.6657624e-08 3.8127661e-07] dtype=float32) each number of index i in that array corresponds to the probability that digit image test_digits belongs to class i. this first test digit has the highest probability score (0.99999106 almost 1) at index 7 so according to our model it must be a 7 >>> predictions.argmax() 7 >>> predictions 0.99999106 we can check that the test label agrees >>> test_labels 7 on average how good is our model at classifying such never-before-seen digits? let s check by computing average accuracy over the entire test set. >>> test_loss test_acc = model.evaluate(test_images test_labels) >>> print(f"test_acc {test_acc}") test_acc 0.9785 the test-set accuracy turns out to be 97.8% that s quite a bit lower than the training-set accuracy (98.9%). this gap between training accuracy and test accuracy is an example of overfitting the fact that machine learning models tend to perform worse on new data than on their training data. overfitting is a central topic in chapter 3. this concludes our first example you just saw how you can build and train a neural network to classify handwritten digits in less than 15 lines of python code. in this chapter and the next we ll go into detail about every moving piece we just pre.viewed and clarify what s going on behind the scenes. you ll learn about tensors the data-storing objects going into the model tensor operations which layers are made of and gradient descent which allows your model to learn from its training examples. data representations for neural networks 2.2 data representations for neural networks in the previous example we started from data stored in multidimensional numpy arrays also called tensors. in general all current machine learning systems use tensors as their basic data structure. tensors are fundamental to the field so fundamental that tensorflow was named after them. so what s a tensor? at its core a tensor is a container for data usually numerical data. so it s a con.tainer for numbers. you may be already familiar with matrices which are rank-2 ten.sors tensors are a generalization of matrices to an arbitrary number of dimensions (note that in the context of tensors a dimension is often called an axis). 2.2.1 scalars (rank-0 tensors) a tensor that contains only one number is called a scalar (or scalar tensor or rank-0 tensor or 0d tensor). in numpy a float32 or float64 number is a scalar tensor (or scalar array). you can display the number of axes of a numpy tensor via the ndim attri.bute a scalar tensor has 0 axes (ndim == 0). the number of axes of a tensor is also called its rank. here s a numpy scalar >>> import numpy as np >>> x = np.array(12) >>> x array(12) >>> x.ndim 0 2.2.2 vectors (rank-1 tensors) an array of numbers is called a vector or rank-1 tensor or 1d tensor. a rank-1 tensor is said to have exactly one axis. following is a numpy vector >>> x = np.array([12 3 6 14 7]) >>> x array([12 3 6 14 7]) >>> x.ndim 1 this vector has five entries and so is called a 5-dimensional vector. don t confuse a 5d vector with a 5d tensor! a 5d vector has only one axis and has five dimensions along its axis whereas a 5d tensor has five axes (and may have any number of dimensions along each axis). dimensionality can denote either the number of entries along a spe.cific axis (as in the case of our 5d vector) or the number of axes in a tensor (such as a 5d tensor) which can be confusing at times. in the latter case it s technically more correct to talk about a tensor of rank 5 (the rank of a tensor being the number of axes) but the ambiguous notation 5d tensor is common regardless. 2.2.3 matrices (rank-2 tensors) an array of vectors is a matrix or rank-2 tensor or 2d tensor. a matrix has two axes (often referred to as rows and columns). you can visually interpret a matrix as a rectan.gular grid of numbers. this is a numpy matrix >>> x = np.array([[5 78 2 34 0] [6 79 3 35 1] [7 80 4 36 2]]) >>> x.ndim 2 the entries from the first axis are called the rows and the entries from the second axis are called the columns. in the previous example [578 2340] is the first row of x and [5 6 7] is the first column. 2.2.4 rank-3 and higher-rank tensors if you pack such matrices in a new array you obtain a rank-3 tensor (or 3d tensor) which you can visually interpret as a cube of numbers. following is a numpy rank-3 tensor >>> x = np.array([[[5 78 2 34 0] [6 79 3 35 1] [7 80 4 36 2]] [[5 78 2 34 0] [6 79 3 35 1] [7 80 4 36 2]] [[5 78 2 34 0] [6 79 3 35 1] [7 80 4 36 2]]]) >>> x.ndim 3 by packing rank-3 tensors in an array you can create a rank-4 tensor and so on. in deep learning you ll generally manipulate tensors with ranks 0 to 4 although you may go up to 5 if you process video data. 2.2.5 key attributes a tensor is defined by three key attributes number of axes (rank) for instance a rank-3 tensor has three axes and a matrix has two axes. this is also called the tensor s ndim in python libraries such as numpy or tensorflow. shape this is a tuple of integers that describes how many dimensions the ten.sor has along each axis. for instance the previous matrix example has shape (35) and the rank-3 tensor example has shape (335). a vector has a shape with a single element such as (5) whereas a scalar has an empty shape (). data representations for neural networks data type (usually called dtype in python libraries) this is the type of the data contained in the tensor for instance a tensor s type could be float16 float32 float64 uint8 and so on. in tensorflow you are also likely to come across string tensors. to make this more concrete let s look back at the data we processed in the mnist example. first we load the mnist dataset from tensorflow.keras.datasets import mnist (train_images train_labels) (test_images test_labels) = mnist.load_data() next we display the number of axes of the tensor train_images the ndim attribute >>> train_images.ndim 3 here s its shape >>> train_images.shape (60000 28 28) and this is its data type the dtype attribute >>> train_images.dtype uint8 so what we have here is a rank-3 tensor of 8-bit integers. more precisely it s an array of 60000 matrices of 28 28 integers. each such matrix is a grayscale image with coeffi.cients between 0 and 255. let s display the fourth digit in this rank-3 tensor using the matplotlib library (a well-known python data visualization library which comes preinstalled in colab) see figure 2.2. import matplotlib.pyplot as plt digit = train_images plt.imshow(digit cmap=plt.cm.binary) plt.show() naturally the corresponding label is the integer 9 >>> train_labels 9 2.2.6 manipulating tensors in numpy in the previous example we selected a specific digit alongside the first axis using the syntax train_images[i]. selecting specific elements in a tensor is called tensor slicing. let s look at the tensor-slicing operations you can do on numpy arrays. the following example selects digits #10 to #100 (#100 isn t included) and puts them in an array of shape (90 2828) >>> my_slice = train_images >>> my_slice.shape (90 28 28) it s equivalent to this more detailed notation which specifies a start index and stop index for the slice along each tensor axis. note that is equivalent to selecting the entire axis equivalent to the >>> my_slice = train_images[10100 ] previous example >>> my_slice.shape (90 28 28) >>> my_slice = train_images[10100 028 028] also equivalent to the >>> my_slice.shape previous example (90 28 28) in general you may select slices between any two indices along each tensor axis. for instance in order to select 14 14 pixels in the bottom-right corner of all images you would do this my_slice = train_images[ 14 14] it s also possible to use negative indices. much like negative indices in python lists they indicate a position relative to the end of the current axis. in order to crop the images to patches of 14 14 pixels centered in the middle you d do this my_slice = train_images[ 7-7 7-7] data representations for neural networks 2.2.7 the notion of data batches in general the first axis (axis 0 because indexing starts at 0) in all data tensors you ll come across in deep learning will be the samples axis (sometimes called the samples dimension). in the mnist example samples are images of digits. in addition deep learning models don t process an entire dataset at once rather they break the data into small batches. concretely here s one batch of our mnist digits with a batch size of 128 batch = train_images and here s the next batch batch = train_images and the nth batch n = 3 batch = train_images[128 * n128 * (n + 1)] when considering such a batch tensor the first axis (axis 0) is called the batch axis or batch dimension. this is a term you ll frequently encounter when using keras and other deep learning libraries. 2.2.8 real-world examples of data tensors let s make data tensors more concrete with a few examples similar to what you ll encounter later. the data you ll manipulate will almost always fall into one of the fol.lowing categories vector data rank-2 tensors of shape (samples features) where each sample is a vector of numerical attributes ( features ) timeseries data or sequence data rank-3 tensors of shape (samples timesteps features) where each sample is a sequence (of length timesteps) of feature vectors images rank-4 tensors of shape (samples height width channels) where each sample is a 2d grid of pixels and each pixel is represented by a vector of values ( channels ) video rank-5 tensors of shape (samplesframes height width channels) where each sample is a sequence (of length frames) of images 2.2.9 vector data this is one of the most common cases. in such a dataset each single data point can be encoded as a vector and thus a batch of data will be encoded as a rank-2 tensor (that is an array of vectors) where the first axis is the samples axis and the second axis is the features axis. let s take a look at two examples an actuarial dataset of people where we consider each person s age gender and income. each person can be characterized as a vector of 3 values and thus an entire dataset of 100000 people can be stored in a rank-2 tensor of shape (100000 3). a dataset of text documents where we represent each document by the counts of how many times each word appears in it (out of a dictionary of 20000 com.mon words). each document can be encoded as a vector of 20000 values (one count per word in the dictionary) and thus an entire dataset of 500 documents can be stored in a tensor of shape (500 20000). 2.2.10 timeseries data or sequence data whenever time matters in your data (or the notion of sequence order) it makes sense to store it in a rank-3 tensor with an explicit time axis. each sample can be encoded as a sequence of vectors (a rank-2 tensor) and thus a batch of data will be encoded as a rank-3 tensor (see figure 2.3). samples figure 2.3 a rank-3 timeseries data tensor the time axis is always the second axis (axis of index 1) by convention. let s look at a few examples a dataset of stock prices. every minute we store the current price of the stock the highest price in the past minute and the lowest price in the past minute. thus every minute is encoded as a 3d vector an entire day of trading is encoded as a matrix of shape (3903) (there are 390 minutes in a trading day) and 250 days worth of data can be stored in a rank-3 tensor of shape (250 3903). here each sample would be one day s worth of data. a dataset of tweets where we encode each tweet as a sequence of 280 characters out of an alphabet of 128 unique characters. in this setting each character can be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry at the index corresponding to the character). then each tweet can be encoded as a rank-2 tensor of shape (280 128) and a dataset of 1 million tweets can be stored in a tensor of shape (1000000280 128). data representations for neural networks 2.2.11 image data images typically have three dimensions height width and color depth. although grayscale images (like our mnist digits) have only a single color channel and could thus be stored in rank-2 tensors by convention image tensors are always rank-3 with a one-dimensional color channel for grayscale images. a batch of 128 grayscale images of size 256 256 could thus be stored in a tensor of shape (128 256 256 1) and a batch of 128 color images could be stored in a tensor of shape (128 256 256 3) (see figure 2.4). color channels height figure 2.4 a rank-4 image data tensor there are two conventions for shapes of image tensors the channels-last convention (which is standard in tensorflow) and the channels-first convention (which is increas.ingly falling out of favor). the channels-last convention places the color-depth axis at the end (samples height width color_depth). meanwhile the channels-first convention places the color depth axis right after the batch axis (samples color_depth height width). with the channels-first convention the previous examples would become (128 1 256256) and (128 3256 256). the keras api provides support for both formats. 2.2.12 video data video data is one of the few types of real-world data for which you ll need rank-5 ten.sors. a video can be understood as a sequence of frames each frame being a color image. because each frame can be stored in a rank-3 tensor (height width color_ depth) a sequence of frames can be stored in a rank-4 tensor (frames height width color_depth) and thus a batch of different videos can be stored in a rank-5 tensor of shape (samples frames heightwidth color_depth). for instance a 60-second 144 256 youtube video clip sampled at 4 frames per second would have 240 frames. a batch of four such video clips would be stored in a tensor of shape (4 240 144 256 3). that s a total of 106168320 values! if the dtype of the tensor was float32 each value would be stored in 32 bits so the tensor would represent 405 mb. heavy! videos you encounter in real life are much lighter because they aren t stored in float32 and they re typically compressed by a large fac.tor (such as in the mpeg format). 2.3 the gears of neural networks tensor operations much as any computer program can be ultimately reduced to a small set of binary operations on binary inputs (and or nor and so on) all transformations learned by deep neural networks can be reduced to a handful of tensor operations (or tensor func.tions) applied to tensors of numeric data. for instance it s possible to add tensors multiply tensors and so on. in our initial example we built our model by stacking dense layers on top of each other. a keras layer instance looks like this keras.layers.dense(512 activation="relu") this layer can be interpreted as a function which takes as input a matrix and returns another matrix a new representation for the input tensor. specifically the function is as follows (where w is a matrix and b is a vector both attributes of the layer) output = relu(dot(input w) + b) let s unpack this. we have three tensor operations here a dot product (dot) between the input tensor and a tensor named w an addition (+) between the resulting matrix and a vector b a relu operation relu(x) is max(x 0) relu stands for rectified linear unit note although this section deals entirely with linear algebra expressions you won t find any mathematical notation here. i ve found that mathematical concepts can be more readily mastered by programmers with no mathemati.cal background if they re expressed as short python snippets instead of math.ematical equations. so we ll use numpy and tensorflow code throughout. 2.3.1 element-wise operations the relu operation and addition are element-wise operations operations that are applied independently to each entry in the tensors being considered. this means these operations are highly amenable to massively parallel implementations (vectorized implementations a term that comes from the vector processor supercomputer architec.ture from the 1970 90 period). if you want to write a naive python implementation of an element-wise operation you use a for loop as in this naive implementation of an element-wise relu operation x is a rank-2 def naive_relu(x) numpy tensor. assert len(x.shape) == 2 the gears of neural networks tensor operations x = x.copy() avoid overwriting for i in range(x.shape) the input tensor. for j in range(x.shape) x[i j] = max(x[i j] 0) return x you could do the same for addition x and y are rank-2 def naive_add(x y) numpy tensors. assert len(x.shape) == 2 assert x.shape == y.shape x = x.copy() avoid overwriting for i in range(x.shape) the input tensor. for j in range(x.shape) x[i j] += y[i j] return x on the same principle you can do element-wise multiplication subtraction and so on. in practice when dealing with numpy arrays these operations are available as well-optimized built-in numpy functions which themselves delegate the heavy lifting to a basic linear algebra subprograms (blas) implementation. blas are low-level highly parallel efficient tensor-manipulation routines that are typically implemented in fortran or c. so in numpy you can do the following element-wise operation and it will be blaz.ing fast import numpy as np element-wise addition z = x + y element-wise relu z = np.maximum(z 0.) let s actually time the difference import time x = np.random.random((20 100)) y = np.random.random((20 100)) t0 = time.time() for _ in range(1000) z = x + y z = np.maximum(z 0.) print("took {0.2f} s".format(time.time() - t0)) this takes 0.02 s. meanwhile the naive version takes a stunning 2.45 s t0 = time.time() for _ in range(1000) z = naive_add(x y) z = naive_relu(z) print("took {0.2f} s".format(time.time() - t0)) likewise when running tensorflow code on a gpu element-wise operations are exe.cuted via fully vectorized cuda implementations that can best utilize the highly par.allel gpu chip architecture. 2.3.2 broadcasting our earlier naive implementation of naive_add only supports the addition of rank-2 tensors with identical shapes. but in the dense layer introduced earlier we added a rank-2 tensor with a vector. what happens with addition when the shapes of the two tensors being added differ? when possible and if there s no ambiguity the smaller tensor will be broadcast to match the shape of the larger tensor. broadcasting consists of two steps 1 axes (called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor. 2 the smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor. let s look at a concrete example. consider x with shape (32 10) and y with shape (10) x is a random matrix with shape (32 10). import numpy as np x = np.random.random((32 10)) y is a random vector with shape (10). y = np.random.random((10)) first we add an empty first axis to y whose shape becomes (110) the shape of y y = np.expand_dims(y axis=0) is now (1 10). then we repeat y 32 times alongside this new axis so that we end up with a tensor y with shape (3210) where y[i] ==y for i in range(0 32) repeat y 32 times along axis 0 to y = np.concatenate([y] * 32 axis=0) obtain y which has shape (32 10). at this point we can proceed to add x and y because they have the same shape. in terms of implementation no new rank-2 tensor is created because that would be terribly inefficient. the repetition operation is entirely virtual it happens at the algorithmic level rather than at the memory level. but thinking of the vector being repeated 10 times alongside a new axis is a helpful mental model. here s what a naive implementation would look like x is a rank-2 def naive_add_matrix_and_vector(x y) numpy tensor. assert len(x.shape) == 2 assert len(y.shape) == 1 y is a numpy vector. assert x.shape == y.shape x = x.copy() avoid overwriting for i in range(x.shape) the input tensor. the gears of neural networks tensor operations for j in range(x.shape) x[i j] += y[j] return x with broadcasting you can generally perform element-wise operations that take two inputs tensors if one tensor has shape (ab nn+1 m) and the other has shape (n n+1 m). the broadcasting will then automatically happen for axes a through n-1. the following example applies the element-wise maximum operation to two tensors of different shapes via broadcasting x is a random tensor with import numpy as np shape (64 3 32 10). x = np.random.random((64 3 32 10)) y = np.random.random((32 10)) y is a random z = np.maximum(x y) tensor with shape (32 10). the output z has shape (64 3 32 10) like x. 2.3.3 tensor product the tensor product or dot product (not to be confused with an element-wise product the * operator) is one of the most common most useful tensor operations. in numpy a tensor product is done using the np.dot function (because the math. ematical notation for tensor product is usually a dot) x = np.random.random((32)) y = np.random.random((32)) z = np.dot(x y) in mathematical notation you d note the operation with a dot ( ) z=x y mathematically what does the dot operation do? let s start with the dot product of two vectors x and y. it s computed as follows def naive_vector_dot(x y) assert len(x.shape) == 1 x and y are assert len(y.shape) == 1 numpy vectors. assert x.shape == y.shape z = 0. for i in range(x.shape) z += x[i] * y[i] return z you ll have noticed that the dot product between two vectors is a scalar and that only vectors with the same number of elements are compatible for a dot product. you can also take the dot product between a matrix x and a vector y which returns a vector where the coefficients are the dot products between y and the rows of x. you implement it as follows def naive_matrix_vector_dot(x y) assert len(x.shape) == 2 assert len(y.shape) == 1 assert x.shape == y.shape z = np.zeros(x.shape) for i in range(x.shape) for j in range(x.shape) z[i] += x[i j] * y[j] return z x is a numpy matrix. y is a numpy vector. the first dimension of x must be the same as the 0th dimension of y! this operation returns a vector of 0s with the same shape as y. you could also reuse the code we wrote previously which highlights the relationship between a matrix-vector product and a vector product def naive_matrix_vector_dot(x y) z = np.zeros(x.shape) for i in range(x.shape) z[i] = naive_vector_dot(x[i ] y) return z note that as soon as one of the two tensors has an ndim greater than 1 dot is no lon.ger symmetric which is to say that dot(x y) isn t the same as dot(y x). of course a dot product generalizes to tensors with an arbitrary number of axes. the most common applications may be the dot product between two matrices. you can take the dot product of two matrices x and y (dot(x y)) if and only if x.shape == y.shape. the result is a matrix with shape (x.shape y.shape) where the coefficients are the vector products between the rows of x and the columns of y. here s the naive implementation def naive_matrix_dot(x y) assert len(x.shape) == 2 x and y are numpy matrices. assert len(y.shape) == 2 assert x.shape == y.shape the first dimension of x must be the z = np.zeros((x.shape y.shape)) same as the 0th dimension of y! for i in range(x.shape) this iterates over the rows of x . . . for j in range(y.shape) operation row_x = x[i ] returns a . . . and over the columns of y. matrix of 0s column_y = y[ j] with a specific z[i j] = naive_vector_dot(row_x column_y) shape. return z to understand dot-product shape compatibility it helps to visualize the input and out.put tensors by aligning them as shown in figure 2.5. in the figure x y and z are pictured as rectangles (literal boxes of coefficients). because the rows of x and the columns of y must have the same size it follows that the width of x must match the height of y. if you go on to develop new machine learning algorithms you ll likely be drawing such diagrams often. the gears of neural networks tensor operations more generally you can take the dot product between higher-dimensional tensors following the same rules for shape compatibility as outlined earlier for the 2d case (abc d) (d) (abc) (abc d) (de) (abce) and so on. 2.3.4 tensor reshaping a third type of tensor operation that s essential to understand is tensor reshaping. although it wasn t used in the dense layers in our first neural network example we used it when we preprocessed the digits data before feeding it into our model train_images = train_images.reshape((60000 28 * 28)) reshaping a tensor means rearranging its rows and columns to match a target shape. naturally the reshaped tensor has the same total number of coefficients as the initial tensor. reshaping is best understood via simple examples >>> x = np.array([[0. 1.] [2. 3.] [4. 5.]]) >>> x.shape (3 2) >>> x = x.reshape((6 1)) >>> x array([[ 0.] [ 1.] [ 2.] [ 3.] [ 4.] [ 5.]]) >>> x = x.reshape((2 3)) >>> x array([[ 0. 1. 2.] [ 3. 4. 5.]]) a special case of reshaping that s commonly encountered is transposition. transposing a matrix means exchanging its rows and its columns so that x[i ] becomes x[ i] >>> x = np.zeros((300 20)) creates an all. >>> x = np.transpose(x) zeros matrix of >>> x.shape shape (300 20) (20 300) 2.3.5 geometric interpretation of tensor operations because the contents of the tensors manipulated by tensor operations can be inter.preted as coordinates of points in some geometric space all tensor operations have a geometric interpretation. for instance let s consider addition. we ll start with the fol.lowing vector a = [0.5 1] it s a point in a 2d space (see figure 2.6). it s common to picture a vector as an arrow linking the origin to the point as shown in figure 2.7. let s consider a new point b =[1 0.25] which we ll add to the previous one. this is done geometrically by chaining together the vector arrows with the resulting location being the vector representing the sum of the previous two vectors (see figure 2.8). as you can see adding a vector b to a vector a represents the action of copying point a in a new location whose distance and direction from the original point a is determined by the vector b. if you apply the same vector addition to a group of points in the plane (an object ) you would be creating a copy of the entire object in a new location (see the gears of neural networks tensor operations figure 2.9). tensor addition thus represents the action of translating an object (moving the object without distorting it) by a certain amount in a certain direction. + x horizontal factor y vertical factor figure 2.9 2d translation as a vertical factor horizontal factor vector addition in general elementary geometric operations such as translation rotation scaling skewing and so on can be expressed as tensor operations. here are a few examples translation as you just saw adding a vector to a point will move the point by a fixed amount in a fixed direction. applied to a set of points (such as a 2d object) this is called a translation (see figure 2.9). rotation a counterclockwise rotation of a 2d vector by an angle theta (see fig.ure 2.10) can be achieved via a dot product with a 2 2 matrix r=[[cos(theta) -sin(theta)] [sin(theta) cos(theta)]]. cos(theta) sin(theta) x sin(theta) cos(theta) y figure 2.10 2d rotation theta (counterclockwise) as a dot product scaling a vertical and horizontal scaling of the image (see figure 2.11) can be achieved via a dot product with a 2 2 matrix s = [[horizontal_factor 0] [0 vertical_factor]] (note that such a matrix is called a diagonal matrix because it only has non-zero coefficients in its diagonal going from the top left to the bottom right). 10 x 0 0.5 y figure 2.11 2d scaling as a dot product linear transform a dot product with an arbitrary matrix implements a linear transform. note that scaling and rotation listed previously are by definition lin.ear transforms. affine transform an affine transform (see figure 2.12) is the combination of a linear transform (achieved via a dot product with some matrix) and a transla.tion (achieved via a vector addition). as you have probably recognized that s exactly the y=w x+b computation implemented by the dense layer! a dense layer without an activation function is an affine layer. w x + b figure 2.12 affine transform in the plane dense layer with relu activation an important observation about affine trans.forms is that if you apply many of them repeatedly you still end up with an affine transform (so you could just have applied that one affine transform in the first place). let s try it with two affine2(affine1(x)) = w2 (w1 x + b1) +b2=(w2 w1) x+(w2 b1+b2). that s an affine transform where the linear part is the matrix w2 w1 and the translation part is the vector w2 b1+b2. as a consequence a multilayer neural network made entirely of dense layers without the gears of neural networks tensor operations activations would be equivalent to a single dense layer. this deep neural net.work would just be a linear model in disguise! this is why we need activation functions like relu (seen in action in figure 2.13). thanks to activation func.tions a chain of dense layers can be made to implement very complex non-linear geometric transformations resulting in very rich hypothesis spaces for your deep neural networks. we ll cover this idea in more detail in the next chapter. relu(w x + b) figure 2.13 affine transform followed by relu activation 2.3.6 a geometric interpretation of deep learning you just learned that neural networks consist entirely of chains of tensor operations and that these tensor operations are just simple geometric transformations of the input data. it follows that you can interpret a neural network as a very complex geo.metric transformation in a high-dimensional space implemented via a series of sim.ple steps. in 3d the following mental image may prove useful. imagine two sheets of colored paper one red and one blue. put one on top of the other. now crumple them together into a small ball. that crumpled paper ball is your input data and each sheet of paper is a class of data in a classification problem. what a neural network is meant to do is figure out a transformation of the paper ball that would uncrumple it so as to make the two classes cleanly separable again (see figure 2.14). with deep learning this would be implemented as a series of simple transformations of the 3d space such as those you could apply on the paper ball with your fingers one movement at a time. uncrumpling paper balls is what machine learning is about finding neat representa.tions for complex highly folded data manifolds in high-dimensional spaces (a mani.fold is a continuous surface like our crumpled sheet of paper). at this point you should have a pretty good intuition as to why deep learning excels at this it takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones which is pretty much the strategy a human would follow to uncrumple a paper ball. each layer in a deep network applies a trans.formation that disentangles the data a little and a deep stack of layers makes tractable an extremely complicated disentanglement process. 2.4 the engine of neural networks gradient-based optimization as you saw in the previous section each neural layer from our first model example transforms its input data as follows output = relu(dot(input w) + b) in this expression w and b are tensors that are attributes of the layer. they re called the weights or trainable parameters of the layer (the kernel and bias attributes respec.tively). these weights contain the information learned by the model from exposure to training data. initially these weight matrices are filled with small random values (a step called random initialization). of course there s no reason to expect that relu(dot(inputw) + b) when w and b are random will yield any useful representations. the resulting representations are meaningless but they re a starting point. what comes next is to gradually adjust these weights based on a feedback signal. this gradual adjustment also called training is the learning that machine learning is all about. this happens within what s called a training loop which works as follows. repeat these steps in a loop until the loss seems sufficiently low 1 draw a batch of training samples x and corresponding targets y_true. 2 run the model on x (a step called the forward pass) to obtain predictions y_pred. 3 compute the loss of the model on the batch a measure of the mismatch between y_pred and y_true. 4 update all weights of the model in a way that slightly reduces the loss on this batch. you ll eventually end up with a model that has a very low loss on its training data a low mismatch between predictions y_pred and expected targets y_true. the model has learned to map its inputs to correct targets. from afar it may look like magic but when you reduce it to elementary steps it turns out to be simple. step 1 sounds easy enough just i/o code. steps 2 and 3 are merely the application of a handful of tensor operations so you could implement these steps purely from what you learned in the previous section. the difficult part is step 4 updating the model s weights. given an individual weight coefficient in the model how can you compute whether the coefficient should be increased or decreased and by how much? one naive solution would be to freeze all weights in the model except the one sca.lar coefficient being considered and try different values for this coefficient. let s say the engine of neural networks gradient-based optimization the initial value of the coefficient is 0.3. after the forward pass on a batch of data the loss of the model on the batch is 0.5. if you change the coefficient s value to 0.35 and rerun the forward pass the loss increases to 0.6. but if you lower the coefficient to 0.25 the loss falls to 0.4. in this case it seems that updating the coefficient by 0.05 would contribute to minimizing the loss. this would have to be repeated for all coeffi.cients in the model. but such an approach would be horribly inefficient because you d need to com.pute two forward passes (which are expensive) for every individual coefficient (of which there are many usually thousands and sometimes up to millions). thankfully there s a much better approach gradient descent. gradient descent is the optimization technique that powers modern neural net.works. here s the gist of it. all of the functions used in our models (such as dot or +) transform their input in a smooth and continuous way if you look at z=x+y for instance a small change in y only results in a small change in z and if you know the direction of the change in y you can infer the direction of the change in z. mathemat.ically you d say these functions are differentiable. if you chain together such functions the bigger function you obtain is still differentiable. in particular this applies to the function that maps the model s coefficients to the loss of the model on a batch of data a small change in the model s coefficients results in a small predictable change in the loss value. this enables you to use a mathematical operator called the gradient to describe how the loss varies as you move the model s coefficients in different direc.tions. if you compute this gradient you can use it to move the coefficients (all at once in a single update rather than one at a time) in a direction that decreases the loss. if you already know what differentiable means and what a gradient is you can skip to section 2.4.3. otherwise the following two sections will help you understand these concepts. 2.4.1 what s a derivative? consider a continuous smooth function f(x) = y mapping a number x to a new number y. we can use the function in figure 2.15 as an example. because the function is continuous a small change in x can only result in a small change in y that s the intuition behind continuity. let s say you increase x by a small factor epsilon_x this results in a small epsilon_y change to y as shown in figure 2.16. figure 2.16 with a continuous function a small change in x results in a small change in y. in addition because the function is smooth (its curve doesn t have any abrupt angles) when epsilon_x is small enough around a certain point p it s possible to approxi.mate f as a linear function of slope a so that epsilon_y becomes a *epsilon_x f(x + epsilon_x) = y + a * epsilon_x obviously this linear approximation is valid only when x is close enough to p. the slope a is called the derivative of f in p. if a is negative it means a small increase in x around p will result in a decrease of f(x) (as shown in figure 2.17) and if a is positive a small increase in x will result in an increase of f(x). further the abso.lute value of a (the magnitude of the derivative) tells you how quickly this increase or decrease will happen. x figure 2.17 derivative of f in p for every differentiable function f(x) (differentiable means can be derived for exam.ple smooth continuous functions can be derived) there exists a derivative function f'(x) that maps values of x to the slope of the local linear approximation of f in those points. for instance the derivative of cos(x) is -sin(x) the derivative of f(x) = a*x is f'(x) =a and so on. being able to derive functions is a very powerful tool when it comes to optimization the task of finding values of x that minimize the value of f(x). if you re trying to update x by a factor epsilon_x in order to minimize f(x) and you know the deriva.tive of f then your job is done the derivative completely describes how f(x) evolves as you change x. if you want to reduce the value of f(x) you just need to move x a lit.tle in the opposite direction from the derivative. the engine of neural networks gradient-based optimization 2.4.2 derivative of a tensor operation the gradient the function we were just looking at turned a scalar value x into another scalar value y you could plot it as a curve in a 2d plane. now imagine a function that turns a tuple of scalars (xy) into a scalar value z that would be a vector operation. you could plot it as a 2d surface in a 3d space (indexed by coordinates x y z). likewise you can imagine functions that take matrices as inputs functions that take rank-3 tensors as inputs etc. the concept of derivation can be applied to any such function as long as the sur.faces they describe are continuous and smooth. the derivative of a tensor operation (or tensor function) is called a gradient. gradients are just the generalization of the concept of derivatives to functions that take tensors as inputs. remember how for a scalar function the derivative represents the local slope of the curve of the function? in the same way the gradient of a tensor function represents the curvature of the multidi.mensional surface described by the function. it characterizes how the output of the function varies when its input parameters vary. let s look at an example grounded in machine learning. consider an input vector x (a sample in a dataset) a matrix w (the weights of a model) a target y_true (what the model should learn to associate to x) a loss function loss (meant to measure the gap between the model s current predictions and y_true) you can use w to compute a target candidate y_pred and then compute the loss or mismatch between the target candidate y_pred and the target y_true we use the model weights w to make a prediction for x. y_pred = dot(w x) loss_value = loss(y_pred y_true) we estimate how far off the prediction was. now we d like to use gradients to figure out how to update w so as to make loss_value smaller. how do we do that? given fixed inputs x and y_true the preceding operations can be interpreted as a function mapping values of w (the model s weights) to loss values f describes the curve (or high-dimensional loss_value = f(w) surface) formed by loss values when w varies. let s say the current value of w is w0. then the derivative of f at the point w0 is a ten.sor grad(loss_value w0) with the same shape as w where each coefficient grad(loss_value w0)[i j] indicates the direction and magnitude of the change in loss_value you observe when modifying w0[i j]. that tensor grad(loss_value w0) is the gradient of the function f(w) = loss_value in w0 also called gradient of loss_value with respect to w around w0. partial derivatives the tensor operation grad(f(w) w) (which takes as input a matrix w) can be expressed as a combination of scalar functions grad_ij(f(w) w_ij) each of which would return the derivative of loss_value = f(w) with respect to the coeffi.cient w[i j] of w assuming all other coefficients are constant. grad_ij is called the partial derivative of f with respect to w[i j]. concretely what does grad(loss_valuew0) represent? you saw earlier that the deriva.tive of a function f(x) of a single coefficient can be interpreted as the slope of the curve of f. likewise grad(loss_value w0) can be interpreted as the tensor describing the direction of steepest ascent of loss_value = f(w) around w0 as well as the slope of this ascent. each partial derivative describes the slope of f in a specific direction. for this reason in much the same way that for a function f(x) you can reduce the value of f(x) by moving x a little in the opposite direction from the derivative with a function f(w) of a tensor you can reduce loss_value=f(w) by moving w in the opposite direction from the gradient for example w1 = w0 -step * grad(f(w0) w0) (where step is a small scaling factor). that means going against the direction of steep.est ascent of f which intuitively should put you lower on the curve. note that the scaling factor step is needed because grad(loss_value w0) only approximates the curva.ture when you re close to w0 so you don t want to get too far from w0. 2.4.3 stochastic gradient descent given a differentiable function it s theoretically possible to find its minimum analyti.cally it s known that a function s minimum is a point where the derivative is 0 so all you have to do is find all the points where the derivative goes to 0 and check for which of these points the function has the lowest value. applied to a neural network that means finding analytically the combination of weight values that yields the smallest possible loss function. this can be done by solv.ing the equation grad(f(w) w)=0 for w. this is a polynomial equation of n variables where n is the number of coefficients in the model. although it would be possible to solve such an equation for n=2 or n=3 doing so is intractable for real neural net.works where the number of parameters is never less than a few thousand and can often be several tens of millions. instead you can use the four-step algorithm outlined at the beginning of this section modify the parameters little by little based on the current loss value for a random batch of data. because you re dealing with a differentiable function you can compute its gradient which gives you an efficient way to implement step 4. if you update the weights in the opposite direction from the gradient the loss will be a little less every time 1 draw a batch of training samples x and corresponding targets y_true. 2 run the model on x to obtain predictions y_pred (this is called the forward pass). the engine of neural networks gradient-based optimization 3 compute the loss of the model on the batch a measure of the mismatch between y_pred and y_true. 4 compute the gradient of the loss with regard to the model s parameters (this is called the backward pass). 5 move the parameters a little in the opposite direction from the gradient for example w -=learning_rate * gradient thus reducing the loss on the batch a bit. the learning rate (learning_rate here) would be a scalar factor modulat.ing the speed of the gradient descent process. easy enough! what we just described is called mini-batch stochastic gradient descent (mini-batch sgd). the term stochastic refers to the fact that each batch of data is drawn at random (stochastic is a scientific synonym of random). figure 2.18 illustrates what happens in 1d when the model has only one parameter and you have only one training sample. value curve (one learnable parameter) as you can see intuitively it s important to pick a reasonable value for the learning_ rate factor. if it s too small the descent down the curve will take many iterations and it could get stuck in a local minimum. if learning_rate is too large your updates may end up taking you to completely random locations on the curve. note that a variant of the mini-batch sgd algorithm would be to draw a single sam.ple and target at each iteration rather than drawing a batch of data. this would be true sgd (as opposed to mini-batch sgd). alternatively going to the opposite extreme you could run every step on all data available which is called batch gradient descent. each update would then be more accurate but far more expensive. the efficient com.promise between these two extremes is to use mini-batches of reasonable size. although figure 2.18 illustrates gradient descent in a 1d parameter space in prac.tice you ll use gradient descent in highly dimensional spaces every weight coefficient in a neural network is a free dimension in the space and there may be tens of thou.sands or even millions of them. to help you build intuition about loss surfaces you can also visualize gradient descent along a 2d loss surface as shown in figure 2.19. but you can t possibly visualize what the actual process of training a neural network looks like you can t represent a 1000000-dimensional space in a way that makes sense to humans. as such it s good to keep in mind that the intuitions you develop through these low-dimensional representations may not always be accurate in practice. this has historically been a source of issues in the world of deep learning research. additionally there exist multiple variants of sgd that differ by taking into account previous weight updates when computing the next weight update rather than just looking at the current value of the gradients. there is for instance sgd with momen.tum as well as adagrad rmsprop and several others. such variants are known as opti.mization methods or optimizers. in particular the concept of momentum which is used in many of these variants deserves your attention. momentum addresses two issues with sgd convergence speed and local minima. consider figure 2.20 which shows the curve of a loss as a function of a model parameter. as you can see around a certain parameter value there is a local minimum around that point moving left would result in the loss increasing but so would moving right. the engine of neural networks gradient-based optimization if the parameter under consideration were being optimized via sgd with a small learning rate the optimization process could get stuck at the local minimum instead of making its way to the global minimum. you can avoid such issues by using momentum which draws inspiration from physics. a useful mental image here is to think of the optimization process as a small ball rolling down the loss curve. if it has enough momentum the ball won t get stuck in a ravine and will end up at the global minimum. momentum is imple.mented by moving the ball at each step based not only on the current slope value (current acceleration) but also on the current velocity (resulting from past accelera.tion). in practice this means updating the parameter w based not only on the cur.rent gradient value but also on the previous parameter update such as in this naive implementation constant momentum factor past_velocity = 0. momentum = 0.1 optimization loop while loss > 0.01 w loss gradient = get_current_parameters() velocity = past_velocity * momentum - learning_rate * gradient w = w + momentum * velocity - learning_rate * gradient past_velocity = velocity update_parameter(w) 2.4.4 chaining derivatives the backpropagation algorithm in the preceding algorithm we casually assumed that because a function is differentia.ble we can easily compute its gradient. but is that true? how can we compute the gra.dient of complex expressions in practice? in the two-layer model we started the chapter with how can we get the gradient of the loss with regard to the weights? that s where the backpropagation algorithm comes in. the chain rule backpropagation is a way to use the derivatives of simple operations (such as addition relu or tensor product) to easily compute the gradient of arbitrarily complex combi.nations of these atomic operations. crucially a neural network consists of many tensor operations chained together each of which has a simple known derivative. for instance the model defined in listing 2.2 can be expressed as a function parameter.ized by the variables w1 b1 w2 and b2 (belonging to the first and second dense layers respectively) involving the atomic operations dot relu softmax and + as well as our loss function loss which are all easily differentiable loss_value = loss(y_true softmax(dot(relu(dot(inputs w1) + b1) w2) + b2)) calculus tells us that such a chain of functions can be derived using the following identity called the chain rule. consider two functions f and g as well as the composed function fg such that fg(x) == f(g(x)) def fg(x) x1 = g(x) y = f(x1) return y then the chain rule states that grad(y x) == grad(y x1) * grad(x1 x). this enables you to compute the derivative of fg as long as you know the derivatives of f and g. the chain rule is named as it is because when you add more intermediate func.tions it starts looking like a chain def fghj(x) x1 = j(x) x2 = h(x1) x3 = g(x2) y = f(x3) return y grad(y x) == (grad(y x3) * grad(x3 x2) * grad(x2 x1) * grad(x1 x)) applying the chain rule to the computation of the gradient values of a neural network gives rise to an algorithm called backpropagation. let s see how that works concretely. automatic differentiation with computation graphs a useful way to think about backpropagation is in terms of computation graphs. a computation graph is the data structure at the heart of tensorflow and the deep learning revolution in general. it s a directed acyclic graph of operations in our case tensor operations. for instance figure 2.21 shows the graph representation of our first model. computation graphs have been an extremely successful abstraction in computer science because they enable us to treat computation as data a comput.able expression is encoded as a machine-readable data structure that can be used as the input or out.put of another program. for instance you could imagine a program that receives a computation graph and returns a new computation graph that implements a large-scale distributed version of the same computation this would mean that you could distribute any computation without having to write the distribution logic yourself. or imagine a pro- graph representation of our gram that receives a computation graph and can two-layer model the engine of neural networks gradient-based optimization automatically generate the derivative of the expression it represents. it s much easier to do these things if your computation is expressed as an explicit graph data structure rather than say lines of ascii characters in a .py file. to explain backpropagation clearly let s look at a really basic example of a com.putation graph (see figure 2.22). we ll consider a simplified version of figure 2.21 where we only have one linear layer and where all variables are scalar. we ll take two scalar variables w and b a scalar input x and apply some operations to them to com.bine them into an output y. finally we ll apply an absolute value error-loss function loss_val = abs(y_true -y). since we want to update w and b in a way that will min.imize loss_val we are interested in computing grad(loss_val b) and grad(loss _val w). figure 2.22 a basic example of a computation graph let s set concrete values for the input nodes in the graph that is to say the input x the target y_true w and b. we ll propagate these values to all nodes in the graph from top to bottom until we reach loss_val. this is the forward pass (see figure 2.23). now let s reverse the graph for each edge in the graph going from a to b we will create an opposite edge from b to a and ask how much does b vary when a varies? that is to say what is grad(b a)? we ll annotate each inverted edge with this value. this backward graph represents the backward pass (see figure 2.24). loss_val = 3 figure 2.23 running a forward pass grad(loss_val x2) = 1 figure 2.24 running a backward pass the engine of neural networks gradient-based optimization we have the following grad(loss_valx2) =1 because as x2 varies by an amount epsilon loss_val = abs(4 -x2) varies by the same amount. grad(x2 x1) = 1 because as x1 varies by an amount epsilon x2 = x1 + b = x1 + 1 varies by the same amount. grad(x2 b) = 1 because as b varies by an amount epsilon x2 = x1 + b = 6 + b varies by the same amount. grad(x1 w) =2 because as w varies by an amount epsilon x1=x*w=2*w var.ies by 2* epsilon. what the chain rule says about this backward graph is that you can obtain the deriva.tive of a node with respect to another node by multiplying the derivatives for each edge along the path linking the two nodes. for instance grad(loss_val w) = grad(loss_val x2) *grad(x2x1) *grad(x1 w) (see figure 2.25). grad(x1 w) = 2 grad(x2 b) = 1 grad(loss_val x2) = 1 by applying the chain rule to our graph we obtain what we were looking for grad(loss_valw) =1 *1*2 =2 grad(loss_valb) =1 *1=1 note if there are multiple paths linking the two nodes of interest a and b in the backward graph we would obtain grad(b a) by summing the contribu. tions of all the paths. and with that you just saw backpropagation in action! backpropagation is simply the application of the chain rule to a computation graph. there s nothing more to it. backpropagation starts with the final loss value and works backward from the top lay.ers to the bottom layers computing the contribution that each parameter had in the loss value. that s where the name backpropagation comes from we back propa.gate the loss contributions of different nodes in a computation graph. nowadays people implement neural networks in modern frameworks that are capable of automatic differentiation such as tensorflow. automatic differentiation is implemented with the kind of computation graph you ve just seen. automatic differ.entiation makes it possible to retrieve the gradients of arbitrary compositions of differ.entiable tensor operations without doing any extra work besides writing down the forward pass. when i wrote my first neural networks in c in the 2000s i had to write my gradients by hand. now thanks to modern automatic differentiation tools you ll never have to implement backpropagation yourself. consider yourself lucky! the gradient tape in tensorflow the api through which you can leverage tensorflow s powerful automatic differenti.ation capabilities is the gradienttape. it s a python scope that will record the tensor operations that run inside it in the form of a computation graph (sometimes called a tape ). this graph can then be used to retrieve the gradient of any output with respect to any variable or set of variables (instances of the tf.variable class). a tf.variable is a specific kind of tensor meant to hold mutable state for instance the weights of a neural network are always tf.variable instances. instantiate a scalar variable open a gradienttape scope. with an initial value of 0. import tensorflow as tf inside the scope apply x = tf.variable(0.) some tensor operations with tf.gradienttape() as tape to our variable. y = 2 * x + 3 grad_of_y_wrt_x = tape.gradient(y x) use the tape to retrieve the gradient of the output y with respect to our variable x. the gradienttape works with tensor operations instantiate a variable with shape x = tf.variable(tf.random.uniform((2 2))) (2 2) and an initial value of all zeros. with tf.gradienttape() as tape y = 2 * x + 3 grad_of_y_wrt_x = tape.gradient(y x) grad_of_y_wrt_x is a tensor of shape (2 2) (like x) describing the curvature of y = 2 * a + 3 around x = [[0 0] [0 0]]. looking back at our first example it also works with lists of variables w = tf.variable(tf.random.uniform((2 2))) b = tf.variable(tf.zeros((2))) x = tf.random.uniform((2 2)) with tf.gradienttape() as tape y = tf.matmul(x w) + b grad_of_y_wrt_w_and_b = tape.gradient(y [w b]) you will learn about the gradient tape in the next chapter. 2.5 looking back at our first example you re nearing the end of this chapter and you should now have a general under.standing of what s going on behind the scenes in a neural network. what was a mag.ical black box at the start of the chapter has turned into a clearer picture as illustrated in figure 2.26 the model composed of layers that are chained together maps the input data to predictions. the loss function then compares these predic.tions to the targets producing a loss value a measure of how well the model s pre.dictions match what was expected. the optimizer uses this loss value to update the model s weights. input x matmul is how you say dot product in tensorflow. grad_of_y_wrt_w_and_b is a list of two tensors with the same shapes as w and b respectively. let s go back to the first example in this chapter and review each piece of it in the light of what you ve learned since. this was the input data (train_images train_labels) (test_images test_labels) = mnist.load_data() train_images = train_images.reshape((60000 28 * 28)) train_images = train_images.astype("float32") / 255 test_images = test_images.reshape((10000 28 * 28)) test_images = test_images.astype("float32") / 255 now you understand that the input images are stored in numpy tensors which are here formatted as float32 tensors of shape (60000784) (training data) and (10000 784) (test data) respectively. this was our model model = keras.sequential([ layers.dense(512 activation="relu") layers.dense(10 activation="softmax") ]) now you understand that this model consists of a chain of two dense layers that each layer applies a few simple tensor operations to the input data and that these opera.tions involve weight tensors. weight tensors which are attributes of the layers are where the knowledge of the model persists. this was the model-compilation step model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) now you understand that sparse_categorical_crossentropy is the loss function that s used as a feedback signal for learning the weight tensors and which the train.ing phase will attempt to minimize. you also know that this reduction of the loss happens via mini-batch stochastic gradient descent. the exact rules governing a spe.cific use of gradient descent are defined by the rmsprop optimizer passed as the first argument. finally this was the training loop model.fit(train_images train_labels epochs=5 batch_size=128) now you understand what happens when you call fit the model will start to iterate on the training data in mini-batches of 128 samples 5 times over (each iteration over all the training data is called an epoch). for each batch the model will compute the gradient of the loss with regard to the weights (using the backpropagation algorithm which derives from the chain rule in calculus) and move the weights in the direction that will reduce the value of the loss for this batch. after these 5 epochs the model will have performed 2345 gradient updates (469 per epoch) and the loss of the model will be sufficiently low that the model will be capable of classifying handwritten digits with high accuracy. at this point you already know most of what there is to know about neural net.works. let s prove it by reimplementing a simplified version of that first example from scratch in tensorflow step by step. looking back at our first example 2.5.1 reimplementing our first example from scratch in tensorflow what better demonstrates full unambiguous understanding than implementing every.thing from scratch? of course what from scratch means here is relative we won t reimplement basic tensor operations and we won t implement backpropagation. but we ll go to such a low level that we will barely use any keras functionality at all. don t worry if you don t understand every little detail in this example just yet. the next chapter will dive in more detail into the tensorflow api. for now just try to fol.low the gist of what s going on the intent of this example is to help crystalize your understanding of the mathematics of deep learning using a concrete implementation. let s go! a simple dense class you ve learned earlier that the dense layer implements the following input transfor.mation where w and b are model parameters and activation is an element-wise function (usually relu but it would be softmax for the last layer) output = activation(dot(w input) + b) let s implement a simple python class naivedense that creates two tensorflow variables w and b and exposes a __call__() method that applies the preceding transformation. import tensorflow as tf class naivedense def __init__(self input_size output_size activation) self.activation = activation w_shape = (input_size output_size) create a matrix w of shape (input_size output_size) initialized with random values. w_initial_value = tf.random.uniform(w_shape minval=0 maxval=1e-1) self.w = tf.variable(w_initial_value) create a vector b of shape (output_size) initialized with zeros. b_shape = (output_size b_initial_value = tf.zeros(b_shape) self.b = tf.variable(b_initial_value) apply the forward pass. def __call__(self inputs) return self.activation(tf.matmul(inputs self.w) + self.b) convenience method for @property retrieving the layer s weights def weights(self) return [self.w self.b] a simple sequential class now let s create a naivesequential class to chain these layers. it wraps a list of layers and exposes a __call__() method that simply calls the underlying layers on the inputs in order. it also features a weights property to easily keep track of the layers parameters. class naivesequential def __init__(self layers) self.layers = layers def __call__(self inputs) x = inputs for layer in self.layers x = layer(x) return x @property def weights(self) weights = [] for layer in self.layers weights += layer.weights return weights using this naivedense class and this naivesequential class we can create a mock keras model model = naivesequential([ naivedense(input_size=28 * 28 output_size=512 activation=tf.nn.relu) naivedense(input_size=512 output_size=10 activation=tf.nn.softmax) ]) assert len(model.weights) == 4 a batch generator next we need a way to iterate over the mnist data in mini-batches. this is easy import math class batchgenerator def __init__(self images labels batch_size=128) assert len(images) == len(labels) self.index = 0 self.images = images self.labels = labels self.batch_size = batch_size self.num_batches = math.ceil(len(images) / batch_size) def next(self) images = self.images[self.index self.index + self.batch_size] labels = self.labels[self.index self.index + self.batch_size] self.index += self.batch_size return images labels 2.5.2 running one training step the most difficult part of the process is the training step updating the weights of the model after running it on one batch of data. we need to 1 compute the predictions of the model for the images in the batch. 2 compute the loss value for these predictions given the actual labels. looking back at our first example run the forward pass (compute the model s predictions under a gradienttape scope). 2.5.3 3 compute the gradient of the loss with regard to the model s weights. 4 move the weights by a small amount in the direction opposite to the gradient. to compute the gradient we will use the tensorflow gradienttape object we intro.duced in section 2.4.4 def one_training_step(model images_batch labels_batch) with tf.gradienttape() as tape predictions = model(images_batch) per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy( labels_batch predictions) average_loss = tf.reduce_mean(per_sample_losses) gradients = tape.gradient(average_loss model.weights) update_weights(gradients model.weights) return average_loss compute the gradient of the loss with regard to the weights. the output gradients update the weights using the gradients is a list where each entry corresponds to (we will define this function shortly). a weight from the model.weights list. as you already know the purpose of the weight update step (represented by the pre.ceding update_weights function) is to move the weights by a bit in a direction that will reduce the loss on this batch. the magnitude of the move is determined by the learning rate typically a small quantity. the simplest way to implement this update_weights function is to subtract gradient *learning_rate from each weight learning_rate = 1e-3 assign_sub is the def update_weights(gradients weights) equivalent of -= for for g w in zip(gradients weights) tensorflow variables. w.assign_sub(g * learning_rate) in practice you would almost never implement a weight update step like this by hand. instead you would use an optimizer instance from keras like this from tensorflow.keras import optimizers optimizer = optimizers.sgd(learning_rate=1e-3) def update_weights(gradients weights) optimizer.apply_gradients(zip(gradients weights)) now that our per-batch training step is ready we can move on to implementing an entire epoch of training. the full training loop an epoch of training simply consists of repeating the training step for each batch in the training data and the full training loop is simply the repetition of one epoch def fit(model images labels epochs batch_size=128) for epoch_counter in range(epochs) print(f"epoch {epoch_counter}") batch_generator = batchgenerator(images labels) for batch_counter in range(batch_generator.num_batches) images_batch labels_batch = batch_generator.next() loss = one_training_step(model images_batch labels_batch) if batch_counter % 100 == 0 print(f"loss at batch {batch_counter} {loss.2f}") let s test drive it from tensorflow.keras.datasets import mnist (train_images train_labels) (test_images test_labels) = mnist.load_data() train_images = train_images.reshape((60000 28 * 28)) train_images = train_images.astype("float32") / 255 test_images = test_images.reshape((10000 28 * 28)) test_images = test_images.astype("float32") / 255 fit(model train_images train_labels epochs=10 batch_size=128) 2.5.4 evaluating the model we can evaluate the model by taking the argmax of its predictions over the test images and comparing it to the expected labels predictions = model(test_images) predictions = predictions.numpy() calling .numpy() on a predicted_labels = np.argmax(predictions axis=1) tensorflow tensor converts matches = predicted_labels == test_labels it to a numpy tensor. print(f"accuracy {matches.mean().2f}") all done! as you can see it s quite a bit of work to do by hand what you can do in a few lines of keras code. but because you ve gone through these steps you should now have a crystal clear understanding of what goes on inside a neural network when you call fit(). having this low-level mental model of what your code is doing behind the scenes will make you better able to leverage the high-level features of the keras api. summary tensors form the foundation of modern machine learning systems. they come in various flavors of dtype rank and shape. you can manipulate numerical tensors via tensor operations (such as addition tensor product or element-wise multiplication) which can be interpreted as encoding geometric transformations. in general everything in deep learning is amenable to a geometric interpretation. deep learning models consist of chains of simple tensor operations parameter.ized by weights which are themselves tensors. the weights of a model are where its knowledge is stored. learning means finding a set of values for the model s weights that minimizes a loss function for a given set of training data samples and their corresponding targets. learning happens by drawing random batches of data samples and their tar.gets and computing the gradient of the model parameters with respect to the loss on the batch. the model parameters are then moved a bit (the magnitude of the move is defined by the learning rate) in the opposite direction from the gradient. this is called mini-batch stochastic gradient descent. the entire learning process is made possible by the fact that all tensor operations in neural networks are differentiable and thus it s possible to apply the chain rule of derivation to find the gradient function mapping the current parameters and current batch of data to a gradient value. this is called backpropagation. two key concepts you ll see frequently in future chapters are loss and optimizers. these are the two things you need to define before you begin feeding data into a model. the loss is the quantity you ll attempt to minimize during training so it should represent a measure of success for the task you re trying to solve. the optimizer specifies the exact way in which the gradient of the loss will be used to update parameters for instance it could be the rmsprop optimizer sgd with momentum and so on. this chapter covers a closer look at tensorflow keras and their relationship setting up a deep learning workspace an overview of how core deep learning concepts translate to keras and tensorflow this chapter is meant to give you everything you need to start doing deep learning in practice. i ll give you a quick presentation of keras (https//keras.io) and tensor-flow (https//tensorflow.org) the python-based deep learning tools that we ll use throughout the book. you ll find out how to set up a deep learning workspace with tensorflow keras and gpu support. finally building on top of the first contact you had with keras and tensorflow in chapter 2 we ll review the core components of neural networks and how they translate to the keras and tensorflow apis. by the end of this chapter you ll be ready to move on to practical real-world applications which will start with chapter 4. 68 3.1 what s tensorflow? tensorflow is a python-based free open source machine learning platform devel.oped primarily by google. much like numpy the primary purpose of tensorflow is to enable engineers and researchers to manipulate mathematical expressions over numerical tensors. but tensorflow goes far beyond the scope of numpy in the follow.ing ways it can automatically compute the gradient of any differentiable expression (as you saw in chapter 2) making it highly suitable for machine learning. it can run not only on cpus but also on gpus and tpus highly parallel hard.ware accelerators. computation defined in tensorflow can be easily distributed across many machines. tensorflow programs can be exported to other runtimes such as c++ java.script (for browser-based applications) or tensorflow lite (for applications running on mobile devices or embedded devices) etc. this makes tensorflow applications easy to deploy in practical settings. it s important to keep in mind that tensorflow is much more than a single library. it s really a platform home to a vast ecosystem of components some developed by google and some developed by third parties. for instance there s tf-agents for reinforce.ment-learning research tfx for industry-strength machine learning workflow man.agement tensorflow serving for production deployment and there s the tensorflow hub repository of pretrained models. together these components cover a very wide range of use cases from cutting-edge research to large-scale production applications. tensorflow scales fairly well for instance scientists from oak ridge national lab have used it to train a 1.1 exaflops extreme weather forecasting model on the 27000 gpus of the ibm summit supercomputer. likewise google has used tensor-flow to develop very compute-intensive deep learning applications such as the chess-playing and go-playing agent alphazero. for your own models if you have the bud.get you can realistically hope to scale to around 10 petaflops on a small tpu pod or a large cluster of gpus rented on google cloud or aws. that would still be around 1% of the peak compute power of the top supercomputer in 2019! 3.2 what s keras? keras is a deep learning api for python built on top of tensorflow that provides a con.venient way to define and train any kind of deep learning model. keras was initially developed for research with the aim of enabling fast deep learning experimentation. through tensorflow keras can run on top of different types of hardware (see fig.ure 3.1) gpu tpu or plain cpu and can be seamlessly scaled to thousands of machines. keras is known for prioritizing the developer experience. it s an api for human beings not machines. it follows best practices for reducing cognitive load it offers keras tensorflow cpu gpu tpu figure 3.1 keras and tensorflow tensorflow is a low-level tensor computing platform and keras is a high-level deep learning api deep learning development layers models optimizers losses metrics... tensor manipulation infrastructure tensors variables automatic di erentiation distribution... hardware execution consistent and simple workflows it minimizes the number of actions required for com.mon use cases and it provides clear and actionable feedback upon user error. this makes keras easy to learn for a beginner and highly productive to use for an expert. keras has well over a million users as of late 2021 ranging from academic research.ers engineers and data scientists at both startups and large companies to graduate students and hobbyists. keras is used at google netflix uber cern nasa yelp instacart square and hundreds of startups working on a wide range of problems across every industry. your youtube recommendations originate from keras models. the waymo self-driving cars are developed with keras models. keras is also a popular framework on kaggle the machine learning competition website where most deep learning competitions have been won using keras. because keras has a large and diverse user base it doesn t force you to follow a sin.gle true way of building and training models. rather it enables a wide range of dif.ferent workflows from the very high level to the very low level corresponding to different user profiles. for instance you have an array of ways to build models and an array of ways to train them each representing a certain trade-off between usability and flexibility. in chapter 5 we ll review in detail a good fraction of this spectrum of work.flows. you could be using keras like you would use scikit-learn just calling fit() and letting the framework do its thing or you could be using it like numpy taking full control of every little detail. this means that everything you re learning now as you re getting started will still be relevant once you ve become an expert. you can get started easily and then gradu.ally dive into workflows where you re writing more and more logic from scratch. you won t have to switch to an entirely different framework as you go from student to researcher or from data scientist to deep learning engineer. this philosophy is not unlike that of python itself! some languages only offer one way to write programs for instance object-oriented programming or functional pro.gramming. meanwhile python is a multiparadigm language it offers an array of possi.ble usage patterns that all work nicely together. this makes python suitable to a wide range of very different use cases system administration data science machine learning setting up a deep learning workspace engineering web development . . . or just learning how to program. likewise you can think of keras as the python of deep learning a user-friendly deep learning language that offers a variety of workflows to different user profiles. 3.3 keras and tensorflow a brief history keras predates tensorflow by eight months. it was released in march 2015 and tensorflow was released in november 2015. you may ask if keras is built on top of tensorflow how it could exist before tensorflow was released? keras was originally built on top of theano another tensor-manipulation library that provided automatic differentiation and gpu support the earliest of its kind. theano developed at the montr al institute for learning algorithms (mila) at the universit de montr al was in many ways a precursor of tensorflow. it pioneered the idea of using static com.putation graphs for automatic differentiation and for compiling code to both cpu and gpu. in late 2015 after the release of tensorflow keras was refactored to a multiback.end architecture it became possible to use keras with either theano or tensorflow and switching between the two was as easy as changing an environment variable. by september 2016 tensorflow had reached a level of technical maturity where it became possible to make it the default backend option for keras. in 2017 two new addi.tional backend options were added to keras cntk (developed by microsoft) and mxnet (developed by amazon). nowadays both theano and cntk are out of devel.opment and mxnet is not widely used outside of amazon. keras is back to being a single-backend api on top of tensorflow. keras and tensorflow have had a symbiotic relationship for many years. through.out 2016 and 2017 keras became well known as the user-friendly way to develop ten.sorflow applications funneling new users into the tensorflow ecosystem. by late 2017 a majority of tensorflow users were using it through keras or in combination with keras. in 2018 the tensorflow leadership picked keras as tensorflow s official high-level api. as a result the keras api is front and center in tensorflow 2.0 released in september 2019 an extensive redesign of tensorflow and keras that takes into account over four years of user feedback and technical progress. by this point you must be eager to start running keras and tensorflow code in practice. let s get you started. 3.4 setting up a deep learning workspace before you can get started developing deep learning applications you need to set up your development environment. it s highly recommended although not strictly nec.essary that you run deep learning code on a modern nvidia gpu rather than your computer s cpu. some applications in particular image processing with convolu.tional networks will be excruciatingly slow on cpu even a fast multicore cpu. and even for applications that can realistically be run on cpu you ll generally see the speed increase by a factor of 5 or 10 by using a recent gpu. to do deep learning on a gpu you have three options buy and install a physical nvidia gpu on your workstation. use gpu instances on google cloud or aws ec2. use the free gpu runtime from colaboratory a hosted notebook service offered by google (for details about what a notebook is see the next section). colaboratory is the easiest way to get started as it requires no hardware purchase and no software installation just open a tab in your browser and start coding. it s the option we recommend for running the code examples in this book. however the free version of colaboratory is only suitable for small workloads. if you want to scale up you ll have to use the first or second option. if you don t already have a gpu that you can use for deep learning (a recent high-end nvidia gpu) then running deep learning experiments in the cloud is a simple low-cost way for you to move to larger workloads without having to buy any additional hardware. if you re developing using jupyter notebooks the experience of running in the cloud is no different from running locally. but if you re a heavy user of deep learning this setup isn t sustainable in the long term or even for more than a few months. cloud instances aren t cheap you d pay $2.48 per hour for a v100 gpu on google cloud in mid-2021. meanwhile a solid consumer-class gpu will cost you somewhere between $1500 and $2500 a price that has been fairly stable over time even as the specs of these gpus keep improving. if you re a heavy user of deep learning consider setting up a local workstation with one or more gpus. additionally whether you re running locally or in the cloud it s better to be using a unix workstation. although it s technically possible to run keras on windows directly we don t recommend it. if you re a windows user and you want to do deep learning on your own workstation the simplest solution to get everything running is to set up an ubuntu dual boot on your machine or to leverage windows subsystem for linux (wsl) a compatibility layer that enables you to run linux applications from windows. it may seem like a hassle but it will save you a lot of time and trouble in the long run. 3.4.1 jupyter notebooks the preferred way to run deep learning experiments jupyter notebooks are a great way to run deep learning experiments in particular the many code examples in this book. they re widely used in the data science and machine learning communities. a notebook is a file generated by the jupyter notebook app (https//jupyter.org) that you can edit in your browser. it mixes the ability to exe.cute python code with rich text-editing capabilities for annotating what you re doing. a notebook also allows you to break up long experiments into smaller pieces that can be executed independently which makes development interactive and means you don t have to rerun all of your previous code if something goes wrong late in an experiment. setting up a deep learning workspace i recommend using jupyter notebooks to get started with keras although that isn t a requirement you can also run standalone python scripts or run code from within an ide such as pycharm. all the code examples in this book are available as open source notebooks you can download them from github at github.com/fchollet/deep.learning-with-python-notebooks. 3.4.2 using colaboratory colaboratory (or colab for short) is a free jupyter notebook service that requires no installation and runs entirely in the cloud. effectively it s a web page that lets you write and execute keras scripts right away. it gives you access to a free (but limited) gpu runtime and even a tpu runtime so you don t have to buy your own gpu. colaboratory is what we recommend for running the code examples in this book. first steps with colaboratory to get started with colab go to https//colab.research.google.com and click the new notebook button. you ll see the standard notebook interface shown in figure 3.2. you ll notice two buttons in the toolbar + code and + text. they re for creating exe.cutable python code cells and annotation text cells respectively. after entering code in a code cell pressing shift-enter will execute it (see figure 3.3). in a text cell you can use markdown syntax (see figure 3.4). pressing shift-enter on a text cell will render it. text cells are useful for giving a readable structure to your notebooks use them to annotate your code with section titles and long explanation paragraphs or to embed figures. notebooks are meant to be a multimedia experience! installing packages with pip the default colab environment already comes with tensorflow and keras installed so you can start using it right away without any installation steps required. but if you ever need to install something with pip you can do so by using the following syntax in a code cell (note that the line starts with ! to indicate that it is a shell command rather than python code) !pip install package_name first steps with tensorflow using the gpu runtime to use the gpu runtime with colab select runtime > change runtime type in the menu and select gpu for the hardware accelerator (see figure 3.5). tensorflow and keras will automatically execute on gpu if a gpu is available so there s nothing more you need to do after you ve selected the gpu runtime. you ll notice that there s also a tpu runtime option in that hardware accelerator dropdown menu. unlike the gpu runtime using the tpu runtime with tensorflow and keras does require a bit of manual setup in your code. we ll cover this in chap.ter 13. for the time being we recommend that you stick to the gpu runtime to follow along with the code examples in the book. you now have a way to start running keras code in practice. next let s see how the key ideas you learned about in chapter 2 translate to keras and tensorflow code. 3.5 first steps with tensorflow as you saw in the previous chapters training a neural network revolves around the fol.lowing concepts first low-level tensor manipulation the infrastructure that underlies all mod.ern machine learning. this translates to tensorflow apis tensors including special tensors that store the network s state (variables) tensor operations such as addition relu matmul backpropagation a way to compute the gradient of mathematical expressions (handled in tensorflow via the gradienttape object) second high-level deep learning concepts. this translates to keras apis layers which are combined into a model a loss function which defines the feedback signal used for learning an optimizer which determines how learning proceeds metrics to evaluate model performance such as accuracy a training loop that performs mini-batch stochastic gradient descent in the previous chapter you already had a first light contact with some of the corre.sponding tensorflow and keras apis you ve briefly used tensorflow s variable class the matmul operation and the gradienttape. you ve instantiated keras dense layers packed them into a sequential model and trained that model with the fit() method. now let s take a deeper dive into how all of these different concepts can be approached in practice using tensorflow and keras. 3.5.1 constant tensors and variables to do anything in tensorflow we re going to need some tensors. tensors need to be created with some initial value. for instance you could create all-ones or all-zeros ten.sors (see listing 3.1) or tensors of values drawn from a random distribution (see list.ing 3.2). >>> import tensorflow as tf >>> x = tf.ones(shape=(2 1)) >>> print(x) tf.tensor( [[1.] [1.]] shape=(2 1) dtype=float32) >>> x = tf.zeros(shape=(2 1)) >>> print(x) tf.tensor( [[0.] [0.]] shape=(2 1) dtype=float32) equivalent to np.ones(shape=(2 1)) equivalent to np.zeros(shape=(2 1)) >>> x = tf.random.normal(shape=(3 1) mean=0. stddev=1.) >>> print(x) tensor of random values drawn from a normal distribution tf.tensor( with mean 0 and standard deviation 1. equivalent to [[-0.14208166] np.random.normal(size=(3 1) loc=0. scale=1.). [-0.95319825] [ 1.1096532 ]] shape=(3 1) dtype=float32) >>> x = tf.random.uniform(shape=(3 1) minval=0. maxval=1.) >>> print(x) tensor of random values drawn from a uniform distribution between 0 tf.tensor( and 1. equivalent to np.random.uniform(size=(3 1) low=0. high=1.). first steps with tensorflow [[0.33779848] [0.06692922] [0.7749394 ]] shape=(3 1) dtype=float32) a significant difference between numpy arrays and tensorflow tensors is that tensor-flow tensors aren t assignable they re constant. for instance in numpy you can do the following. import numpy as np x = np.ones(shape=(2 2)) x[0 0] = 0. try to do the same thing in tensorflow and you will get an error eagertensor object does not support item assignment. this will fail as a x = tf.ones(shape=(2 2)) tensor isn t assignable. x[0 0] = 0. to train a model we ll need to update its state which is a set of tensors. if tensors aren t assignable how do we do it? that s where variables come in. tf.variable is the class meant to manage modifiable state in tensorflow. you ve already briefly seen it in action in the training loop implementation at the end of chapter 2. to create a variable you need to provide some initial value such as a random tensor. >>> v = tf.variable(initial_value=tf.random.normal(shape=(3 1))) >>> print(v) array([[-0.75133973] [-0.4872893 ] [ 1.6626885 ]] dtype=float32)> the state of a variable can be modified via its assign method as follows. >>> v.assign(tf.ones((3 1))) array([[1.] [1.] [1.]] dtype=float32)> it also works for a subset of the coefficients. >>> v[0 0].assign(3.) array([[3.] [1.] [1.]] dtype=float32)> similarly assign_add() and assign_sub() are efficient equivalents of += and -= as shown next. >>> v.assign_add(tf.ones((3 1))) array([[2.] [2.] [2.]] dtype=float32)> 3.5.2 tensor operations doing math in tensorflow just like numpy tensorflow offers a large collection of tensor operations to express mathematical formulas. here are a few examples. a = tf.ones((2 2)) take the square. b = tf.square(a) take the square root. c = tf.sqrt(a) d = b + c add two tensors (element-wise). e = tf.matmul(a b) e *= d take the product of two tensors (as discussed in chapter 2). multiply two tensors (element-wise). importantly each of the preceding operations gets executed on the fly at any point you can print what the current result is just like in numpy. we call this eager execution. 3.5.3 a second look at the gradienttape api so far tensorflow seems to look a lot like numpy. but here s something numpy can t do retrieve the gradient of any differentiable expression with respect to any of its inputs. just open a gradienttape scope apply some computation to one or several input tensors and retrieve the gradient of the result with respect to the inputs. input_var = tf.variable(initial_value=3.) with tf.gradienttape() as tape result = tf.square(input_var) gradient = tape.gradient(result input_var) first steps with tensorflow this is most commonly used to retrieve the gradients of the loss of a model with respect to its weights gradients = tape.gradient(loss weights). you saw this in action in chapter 2. so far you ve only seen the case where the input tensors in tape.gradient() were tensorflow variables. it s actually possible for these inputs to be any arbitrary tensor. however only trainable variables are tracked by default. with a constant tensor you d have to manually mark it as being tracked by calling tape.watch() on it. input_const = tf.constant(3.) with tf.gradienttape() as tape tape.watch(input_const) result = tf.square(input_const) gradient = tape.gradient(result input_const) why is this necessary? because it would be too expensive to preemptively store the information required to compute the gradient of anything with respect to anything. to avoid wasting resources the tape needs to know what to watch. trainable variables are watched by default because computing the gradient of a loss with regard to a list of trainable variables is the most common use of the gradient tape. the gradient tape is a powerful utility even capable of computing second-order gra.dients that is to say the gradient of a gradient. for instance the gradient of the posi.tion of an object with regard to time is the speed of that object and the second-order gradient is its acceleration. if you measure the position of a falling apple along a vertical axis over time and find that it verifies position(time) = 4.9 * time ** 2 what is its acceleration? let s use two nested gradient tapes to find out. time = tf.variable(0.) with tf.gradienttape() as outer_tape with tf.gradienttape() as inner_tape position = 4.9 * time ** 2 speed = inner_tape.gradient(position time) acceleration = outer_tape.gradient(speed time) we use the outer tape to compute the gradient of the gradient from the inner tape. naturally the answer is 4.9 * 2 = 9.8. 3.5.4 an end-to-end example a linear classifier in pure tensorflow you know about tensors variables and tensor operations and you know how to com.pute gradients. that s enough to build any machine learning model based on gradi.ent descent. and you re only at chapter 3! in a machine learning job interview you may be asked to implement a linear classi.fier from scratch in tensorflow a very simple task that serves as a filter between candi.dates who have some minimal machine learning background and those who don t. let s get you past that filter and use your newfound knowledge of tensorflow to implement such a linear classifier. first let s come up with some nicely linearly separable synthetic data to work with two classes of points in a 2d plane. we ll generate each class of points by drawing their coordinates from a random distribution with a specific covariance matrix and a spe.cific mean. intuitively the covariance matrix describes the shape of the point cloud and the mean describes its position in the plane (see figure 3.6). we ll reuse the same covariance matrix for both point clouds but we ll use two different mean values the point clouds will have the same shape but different positions. num_samples_per_class = 1000 negative_samples = np.random.multivariate_normal( mean=[0 3] cov=[[1 0.5][0.5 1]] size=num_samples_per_class) positive_samples = np.random.multivariate_normal( mean=[3 0] cov=[[1 0.5][0.5 1]] size=num_samples_per_class) generate the first class of points 1000 random 2d points. cov=[[1 0.5][0.5 1]] corresponds to an oval-like point cloud oriented from bottom left to top right. generate the other class of points with a different mean and the same covariance matrix. in the preceding code negative_samples and positive_samples are both arrays with shape (1000 2). let s stack them into a single array with shape (2000 2). inputs = np.vstack((negative_samples positive_samples)).astype(np.float32) let s generate the corresponding target labels an array of zeros and ones of shape (2000 1) where targets[i 0] is 0 if inputs[i] belongs to class 0 (and inversely). targets = np.vstack((np.zeros((num_samples_per_class 1) dtype="float32") np.ones((num_samples_per_class 1) dtype="float32"))) next let s plot our data with matplotlib. import matplotlib.pyplot as plt plt.scatter(inputs[ 0] inputs[ 1] c=targets[ 0]) plt.show() first steps with tensorflow now let s create a linear classifier that can learn to separate these two blobs. a linear classifier is an affine transformation (prediction =w input+b) trained to minimize the square of the difference between predictions and the targets. as you ll see it s actually a much simpler example than the end-to-end example of a toy two-layer neural network you saw at the end of chapter 2. however this time you should be able to understand everything about the code line by line. let s create our variables w and b initialized with random values and with zeros respectively. the inputs will be 2d points. input_dim = 2 output_dim = 1 the output predictions will be a single score per sample (close to 0 if the sample is predicted to be in class 0 and close to 1 if the sample is predicted to be in class 1). w= tf.variable(initial_value=tf.random.uniform(shape=(input_dimoutput_dim))) b= tf.variable(initial_value=tf.zeros(shape=(output_dim))) here s our forward pass function. def model(inputs) return tf.matmul(inputs w) + b because our linear classifier operates on 2d inputs w is really just two scalar coeffi.cients w1 and w2 w=[[w1] [w2]]. meanwhile b is a single scalar coefficient. as such for a given input point [xy] its prediction value is prediction =[[w1][w2]] [x y] +b =w1 *x +w2 *y +b. the following listing shows our loss function. per_sample_losses will be a tensor with the same shape as targets and predictions containing per-sample loss scores. def square_loss(targets predictions) per_sample_losses = tf.square(targets - predictions) return tf.reduce_mean(per_sample_losses) we need to average these per-sample loss scores into a single scalar loss value this is what reduce_mean does. next is the training step which receives some training data and updates the weights w and b so as to minimize the loss on the data. retrieve the gradient learning_rate = 0.1 of the loss with regard to weights. def training_step(inputs targets) with tf.gradienttape() as tape forward pass inside a predictions = model(inputs) gradient tape scope loss = square_loss(predictions targets) grad_loss_wrt_w grad_loss_wrt_b = tape.gradient(loss [w b]) w.assign_sub(grad_loss_wrt_w * learning_rate) update the weights. b.assign_sub(grad_loss_wrt_b * learning_rate) return loss for simplicity we ll do batch training instead of mini-batch training we ll run each training step (gradient computation and weight update) for all the data rather than iterate over the data in small batches. on one hand this means that each training step will take much longer to run since we ll compute the forward pass and the gradients for 2000 samples at once. on the other hand each gradient update will be much more effective at reducing the loss on the training data since it will encompass information from all training samples instead of say only 128 random samples. as a result we will need many fewer steps of training and we should use a larger learning rate than we would typically use for mini-batch training (we ll use learning_rate =0.1 defined in listing 3.20). for step in range(40) loss = training_step(inputs targets) print(f"loss at step {step} {loss.4f}") after 40 steps the training loss seems to have stabilized around 0.025. let s plot how our linear model classifies the training data points. because our targets are zeros and ones a given input point will be classified as 0 if its prediction value is below 0.5 and as 1 if it is above 0.5 (see figure 3.7) predictions = model(inputs) plt.scatter(inputs[ 0] inputs[ 1] c=predictions[ 0] > 0.5) plt.show() first steps with tensorflow recall that the prediction value for a given point [x y] is simply prediction == [[w1] [w2]] [x y] + b == w1 * x + w2 * y + b. thus class 0 is defined as w1 * x + w2 * y + b  0.5. you ll notice that what you re looking at is really the equation of a line in the 2d plane w1*x+w2*y+b=0.5. above the line is class 1 and below the line is class 0. you may be used to seeing line equations in the format y=a*x+b in the same format our line becomes y=-w1/w2 * x+(0.5 -b) /w2. let s plot this line (shown in figure 3.8) generate 100 regularly spaced this is our line s numbers between 1 and 4 which equation. we will use to plot our line. plot our line ("-r" x = np.linspace(-1 4 100) means plot it as y = - w / w * x + (0.5 - b) / w a red line ). plt.plot(x y "-r") plt.scatter(inputs[ 0] inputs[ 1] c=predictions[ 0] > 0.5) plot our model s predictions on the same plot. this is really what a linear classifier is all about finding the parameters of a line (or in higher-dimensional spaces a hyperplane) neatly separating two classes of data. 3.6 anatomy of a neural network understanding core keras apis at this point you know the basics of tensorflow and you can use it to implement a toy model from scratch such as the batch linear classifier in the previous section or the toy neural network at the end of chapter 2. that s a solid foundation to build upon. it s now time to move on to a more productive more robust path to deep learn.ing the keras api. 3.6.1 layers the building blocks of deep learning the fundamental data structure in neural networks is the layer to which you were introduced in chapter 2. a layer is a data processing module that takes as input one or more tensors and that outputs one or more tensors. some layers are stateless but more frequently layers have a state the layer s weights one or several tensors learned with stochastic gradient descent which together contain the network s knowledge. different types of layers are appropriate for different tensor formats and different types of data processing. for instance simple vector data stored in rank-2 tensors of shape (samples features) is often processed by densely connected layers also called fully connected or dense layers (the dense class in keras). sequence data stored in rank-3 tensors of shape (samples timesteps features) is typically processed by recurrent layers such as an lstm layer or 1d convolution layers (conv1d). image data stored in rank-4 tensors is usually processed by 2d convolution layers (conv2d). you can think of layers as the lego bricks of deep learning a metaphor that is made explicit by keras. building deep learning models in keras is done by clipping together compatible layers to form useful data-transformation pipelines. the base layer class in keras a simple api should have a single abstraction around which everything is centered. in keras that s the layer class. everything in keras is either a layer or something that closely interacts with a layer. a layer is an object that encapsulates some state (weights) and some computation (a forward pass). the weights are typically defined in a build() (although they could also be created in the constructor __init__()) and the computation is defined in the call() method. in the previous chapter we implemented a naivedense class that contained two weights w and b and applied the computation output = activation(dot(input w) + b). this is what the same layer would look like in keras. from tensorflow import keras all keras layers inherit from the base layer class. class simpledense(keras.layers.layer) anatomy of a neural network understanding core keras apis we define the forward pass computation in the call() method. def __init__(self units activation=none) super().__init__() self.units = units weight creation self.activation = activation takes place in the build() method. def build(self input_shape) input_dim = input_shape[-1] self.w = self.add_weight(shape=(input_dim self.units) initializer="random_normal") self.b = self.add_weight(shape=(self.units) initializer="zeros") add_weight() is a shortcut method for creating weights. def call(self inputs) it is also possible to create y = tf.matmul(inputs self.w) + self.b standalone variables and assign if self.activation is not none them as layer attributes like self.w = y = self.activation(y) tf.variable(tf.random.uniform(w_shape)). return y in the next section we ll cover in detail the purpose of these build() and call() methods. don t worry if you don t understand everything just yet! once instantiated a layer like this can be used just like a function taking as input a tensorflow tensor >>> my_dense = simpledense(units=32 activation=tf.nn.relu) instantiate our >>> input_tensor = tf.ones(shape=(2 784)) layer defined create >>> output_tensor = my_dense(input_tensor) previously. some test >>> print(output_tensor.shape) call the layer on inputs. (2 32)) the inputs just like a function. you re probably wondering why did we have to implement call() and build() since we ended up using our layer by plainly calling it that is to say by using its __call__() method? it s because we want to be able to create the state just in time. let s see how that works. automatic shape inference building layers on the fly just like with lego bricks you can only clip together layers that are compatible. the notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. consider the following example a dense layer with from tensorflow.keras import layers 32 output units layer = layers.dense(32 activation="relu") this layer will return a tensor where the first dimension has been transformed to be 32. it can only be connected to a downstream layer that expects 32-dimensional vec.tors as its input. when using keras you don t have to worry about size compatibility most of the time because the layers you add to your models are dynamically built to match the shape of the incoming layer. for instance suppose you write the following from tensorflow.keras import models from tensorflow.keras import layers model = models.sequential([ layers.dense(32 activation="relu") layers.dense(32) ]) the layers didn t receive any information about the shape of their inputs instead they automatically inferred their input shape as being the shape of the first inputs they see. in the toy version of the dense layer we implemented in chapter 2 (which we named naivedense) we had to pass the layer s input size explicitly to the constructor in order to be able to create its weights. that s not ideal because it would lead to mod.els that look like this where each new layer needs to be made aware of the shape of the layer before it model = naivesequential([ naivedense(input_size=784 output_size=32 activation="relu") naivedense(input_size=32 output_size=64 activation="relu") naivedense(input_size=64 output_size=32 activation="relu") naivedense(input_size=32 output_size=10 activation="softmax") ]) it would be even worse if the rules used by a layer to produce its output shape are complex. for instance what if our layer returned outputs of shape (batch input_ size * 2if input_size %2 == 0else input_size * 3)? if we were to reimplement our naivedense layer as a keras layer capable of auto.matic shape inference it would look like the previous simpledense layer (see listing 3.22) with its build() and call() methods. in simpledense we no longer create weights in the constructor like in the naive-dense example instead we create them in a dedicated state-creation method build() which receives as an argument the first input shape seen by the layer. the build() method is called automatically the first time the layer is called (via its __call__() method). in fact that s why we defined the computation in a separate call() method rather than in the __call__() method directly. the __call__() method of the base layer schematically looks like this def __call__(self inputs) if not self.built self.build(inputs.shape) self.built = true return self.call(inputs) with automatic shape inference our previous example becomes simple and neat model = keras.sequential([ simpledense(32 activation="relu") simpledense(64 activation="relu") anatomy of a neural network understanding core keras apis simpledense(32 activation="relu") simpledense(10 activation="softmax") ]) note that automatic shape inference is not the only thing that the layer class s __call__() method handles. it takes care of many more things in particular routing between eager and graph execution (a concept you ll learn about in chapter 7) and input masking (which we ll cover in chapter 11). for now just remember when implementing your own layers put the forward pass in the call() method. 3.6.2 from layers to models a deep learning model is a graph of layers. in keras that s the model class. until now you ve only seen sequential models (a subclass of model) which are simple stacks of layers mapping a single input to a single output. but as you move forward you ll be exposed to a much broader variety of network topologies. these are some common ones two-branch networks multihead networks residual connections network topology can get quite involved. for instance figure 3.9 shows the topology of the graph of layers of a transformer a common architecture designed to process text data. there are generally two ways of building such models in keras you could directly subclass the model class or you could use the functional api which lets you do more with less code. we ll cover both approaches in chapter 7. the topology of a model defines a hypothesis space. you may remember that in chap.ter 1 we described machine learning as searching for useful representations of some input data within a predefined space of possibilities using guidance from a feedback sig.nal. by choosing a network topology you constrain your space of possibilities (hypoth.esis space) to a specific series of tensor operations mapping input data to output data. what you ll then be searching for is a good set of values for the weight tensors involved in these tensor operations. to learn from data you have to make assumptions about it. these assumptions define what can be learned. as such the structure of your hypothesis space the architecture of your model is extremely important. it encodes the assumptions you make about your problem the prior knowledge that the model starts with. for instance if you re working on a two-class classification problem with a model made of a single dense layer with no activation (a pure affine transformation) you are assum.ing that your two classes are linearly separable. picking the right network architecture is more an art than a science and although there are some best practices and principles you can rely on only practice can help you become a proper neural-network architect. the next few chapters will both teach you explicit principles for building neural networks and help you develop intuition as to what works or doesn t work for specific problems. you ll build a solid intuition about what type of model architectures work for different kinds of problems how to build these networks in practice how to pick the right learning configuration and how to tweak a model until it yields the results you want to see. 3.6.3 the compile step configuring the learning process once the model architecture is defined you still have to choose three more things loss function (objective function) the quantity that will be minimized during training. it represents a measure of success for the task at hand. anatomy of a neural network understanding core keras apis optimizer determines how the network will be updated based on the loss func.tion. it implements a specific variant of stochastic gradient descent (sgd). metrics the measures of success you want to monitor during training and vali.dation such as classification accuracy. unlike the loss training will not optimize directly for these metrics. as such metrics don t need to be differentiable. once you ve picked your loss optimizer and metrics you can use the built-in compile() and fit() methods to start training your model. alternatively you could also write your own custom training loops we ll cover how to do this in chapter 7. it s a lot more work! for now let s take a look at compile() and fit(). the compile() method configures the training process you ve already been intro.duced to it in your very first neural network example in chapter 2. it takes the argu.ments optimizer loss and metrics (a list) define a linear classifier. specify the optimizer by name rmsprop model = keras.sequential([keras.layers.dense(1)]) (it s case-insensitive). model.compile(optimizer="rmsprop" loss="mean_squared_error" specify the loss metrics=["accuracy"]) by name mean squared error. specify a list of metrics in this case only accuracy. in the preceding call to compile() we passed the optimizer loss and metrics as strings (such as "rmsprop"). these strings are actually shortcuts that get converted to python objects. for instance "rmsprop" becomes keras.optimizers.rmsprop(). importantly it s also possible to specify these arguments as object instances like this model.compile(optimizer=keras.optimizers.rmsprop() loss=keras.losses.meansquarederror() metrics=[keras.metrics.binaryaccuracy()]) this is useful if you want to pass your own custom losses or metrics or if you want to further configure the objects you re using for instance by passing a learning_rate argument to the optimizer model.compile(optimizer=keras.optimizers.rmsprop(learning_rate=1e-4) loss=my_custom_loss metrics=[my_custom_metric_1 my_custom_metric_2]) in chapter 7 we ll cover how to create custom losses and metrics. in general you won t have to create your own losses metrics or optimizers from scratch because keras offers a wide range of built-in options that is likely to include what you need optimizers sgd (with or without momentum) rmsprop adam adagrad etc. losses categoricalcrossentropy sparsecategoricalcrossentropy binarycrossentropy meansquarederror kldivergence cosinesimilarity etc. metrics categoricalaccuracy sparsecategoricalaccuracy binaryaccuracy auc precision recall etc. throughout this book you ll see concrete applications of many of these options. 3.6.4 picking a loss function choosing the right loss function for the right problem is extremely important your network will take any shortcut it can to minimize the loss so if the objective doesn t fully correlate with success for the task at hand your network will end up doing things you may not have wanted. imagine a stupid omnipotent ai trained via sgd with this poorly chosen objective function maximizing the average well-being of all humans alive. to make its job easier this ai might choose to kill all humans except a few and focus on the well-being of the remaining ones because average well-being isn t affected by how many humans are left. that might not be what you intended! just remember that all neural networks you build will be just as ruthless in lowering their loss function so choose the objective wisely or you ll have to face unintended side effects. fortunately when it comes to common problems such as classification regression and sequence prediction there are simple guidelines you can follow to choose the correct loss. for instance you ll use binary crossentropy for a two-class classification problem categorical crossentropy for a many-class classification problem and so on. only when you re working on truly new research problems will you have to develop your own loss functions. in the next few chapters we ll detail explicitly which loss functions to choose for a wide range of common tasks. anatomy of a neural network understanding core keras apis 3.6.5 understanding the fit() method after compile() comes fit(). the fit() method implements the training loop itself. these are its key arguments the data (inputs and targets) to train on. it will typically be passed either in the form of numpy arrays or a tensorflow dataset object. you ll learn more about the dataset api in the next chapters. the number of epochs to train for how many times the training loop should iter.ate over the data passed. the batch size to use within each epoch of mini-batch gradient descent the number of training examples considered to compute the gradients for one weight update step. the input examples history = model.fit( as a numpy array inputs targets the corresponding epochs=5 training targets as batch_size=128 a numpy array ) the training loop iterate over the data in the training loop will will iterate over the batches of 128 examples. data 5 times. the call to fit() returns a history object. this object contains a history field which is a dict mapping keys such as "loss" or specific metric names to the list of their per-epoch values. >>> history.history {"binary_accuracy" [0.855 0.9565 0.9555 0.95 0.951] "loss" [0.6573270302042366 0.07434618508815766 0.07687718723714351 0.07412414988875389 0.07617757616937161]} 3.6.6 monitoring loss and metrics on validation data the goal of machine learning is not to obtain models that perform well on the train.ing data which is easy all you have to do is follow the gradient. the goal is to obtain models that perform well in general and particularly on data points that the model has never encountered before. just because a model performs well on its training data doesn t mean it will perform well on data it has never seen! for instance it s possible that your model could end up merely memorizing a mapping between your training samples and their targets which would be useless for the task of predicting targets for data the model has never seen before. we ll go over this point in much more detail in chapter 5. to keep an eye on how the model does on new data it s standard practice to reserve a subset of the training data as validation data you won t be training the model on this data but you will use it to compute a loss value and metrics value. you do this by using the validation_data argument in fit(). like the training data the valida.tion data could be passed as numpy arrays or as a tensorflow dataset object. model = keras.sequential([keras.layers.dense(1)]) model.compile(optimizer=keras.optimizers.rmsprop(learning_rate=0.1) loss=keras.losses.meansquarederror() metrics=[keras.metrics.binaryaccuracy()]) indices_permutation = np.random.permutation(len(inputs)) shuffled_inputs = inputs[indices_permutation] shuffled_targets = targets[indices_permutation] to avoid having samples from only one class in the validation data shuffle the inputs and targets using a random indices permutation. num_validation_samples = int(0.3 * len(inputs)) val_inputs = shuffled_inputs[num_validation_samples] val_targets = shuffled_targets[num_validation_samples] training_inputs = shuffled_inputs[num_validation_samples] training_targets = shuffled_targets[num_validation_samples] model.fit( training_inputs training data used to update training_targets the weights of the model epochs=5 reserve 30% of the training inputs and targets for validation (we ll exclude these samples from training and reserve them to compute the validation loss and metrics). batch_size=16 validation_data=(val_inputs val_targets) validation data used only to monitor the validation ) loss and metrics the value of the loss on the validation data is called the validation loss to distin.guish it from the training loss. note that it s essential to keep the training data and validation data strictly separate the purpose of validation is to monitor whether what the model is learning is actually useful on new data. if any of the validation data has been seen by the model during training your validation loss and metrics will be flawed. note that if you want to compute the validation loss and metrics after the training is complete you can call the evaluate() method loss_and_metrics = model.evaluate(val_inputs val_targets batch_size=128) evaluate() will iterate in batches (of size batch_size) over the data passed and return a list of scalars where the first entry is the validation loss and the following entries are the validation metrics. if the model has no metrics only the validation loss is returned (rather than a list). 3.6.7 inference using a model after training once you ve trained your model you re going to want to use it to make predictions on new data. this is called inference. to do this a naive approach would simply be to __call__() the model takes a numpy array or predictions = model(new_inputs) tensorflow tensor and returns a tensorflow tensor however this will process all inputs in new_inputs at once which may not be feasible if you re looking at a lot of data (in particular it may require more memory than your gpu has). a better way to do inference is to use the predict() method. it will iterate over the data in small batches and return a numpy array of predictions. and unlike __call__() it can also process tensorflow dataset objects. takes a numpy array or predictions = model.predict(new_inputs batch_size=128) a dataset and returns a numpy array for instance if we use predict() on some of our validation data with the linear model we trained earlier we get scalar scores that correspond to the model s predic.tion for each input sample >>> predictions = model.predict(val_inputs batch_size=128) >>> print(predictions) [[0.3590725 ] [0.82706255] [0.74428225] [0.682058 ] [0.7312616 ] [0.6059811 ] [0.78046083] [0.025846 ] [0.16594526] [0.72068727]] for now this is all you need to know about keras models. you are ready to move on to solving real-world machine learning problems with keras in the next chapter. summary tensorflow is an industry-strength numerical computing framework that can run on cpu gpu or tpu. it can automatically compute the gradient of any differentiable expression it can be distributed to many devices and it can export programs to various external runtimes even javascript. keras is the standard api for doing deep learning with tensorflow. it s what we ll use throughout this book. key tensorflow objects include tensors variables tensor operations and the gradient tape. the central class of keras is the layer. a layer encapsulates some weights and some computation. layers are assembled into models. before you start training a model you need to pick an optimizer a loss and some metrics which you specify via the model.compile() method. to train a model you can use the fit() method which runs mini-batch gradi.ent descent for you. you can also use it to monitor your loss and metrics on val.idation data a set of inputs that the model doesn t see during training. once your model is trained you use the model.predict() method to generate predictions on new inputs. getting started with neural networks classification and regression this chapter covers your first examples of real-world machine learning workflows handling classification problems over vector data handling continuous regression problems over vector data this chapter is designed to get you started using neural networks to solve real prob.lems. you ll consolidate the knowledge you gained from chapters 2 and 3 and you ll apply what you ve learned to three new tasks covering the three most com.mon use cases of neural networks binary classification multiclass classification and scalar regression classifying movie reviews as positive or negative (binary classification) classifying news wires by topic (multiclass classification) estimating the price of a house given real-estate data (scalar regression) these examples will be your first contact with end-to-end machine learning work.flows you ll get introduced to data preprocessing basic model architecture princi.ples and model evaluation. 95 96 chapter 4 getting started with neural networks classification and regression classification and regression glossary classification and regression involve many specialized terms. you ve come across some of them in earlier examples and you ll see more of them in future chapters. they have precise machine learning specific definitions and you should be famil.iar with them sample or input one data point that goes into your model. prediction or output what comes out of your model. target the truth. what your model should ideally have predicted according to an external source of data. prediction error or loss value a measure of the distance between your model s prediction and the target. classes a set of possible labels to choose from in a classification problem. for example when classifying cat and dog pictures dog and cat are the two classes. label a specific instance of a class annotation in a classification problem. for instance if picture #1234 is annotated as containing the class dog then dog is a label of picture #1234. ground-truth or annotations all targets for a dataset typically collected by humans. binary classification a classification task where each input sample should be categorized into two exclusive categories. multiclass classification a classification task where each input sample should be categorized into more than two categories for instance classifying handwritten digits. multilabel classification a classification task where each input sample can be assigned multiple labels. for instance a given image may contain both a cat and a dog and should be annotated both with the cat label and the dog label. the number of labels per image is usually variable. scalar regression a task where the target is a continuous scalar value. pre.dicting house prices is a good example the different target prices form a con.tinuous space. vector regression a task where the target is a set of continuous values for example a continuous vector. if you re doing regression against multiple val.ues (such as the coordinates of a bounding box in an image) then you re doing vector regression. mini-batch or batch a small set of samples (typically between 8 and 128) that are processed simultaneously by the model. the number of samples is often a power of 2 to facilitate memory allocation on gpu. when training a mini-batch is used to compute a single gradient-descent update applied to the weights of the model. by the end of this chapter you ll be able to use neural networks to handle simple clas.sification and regression tasks over vector data. you ll then be ready to start building a more principled theory-driven understanding of machine learning in chapter 5. classifying movie reviews a binary classification example 4.1 classifying movie reviews a binary classification example two-class classification or binary classification is one of the most common kinds of machine learning problems. in this example you ll learn to classify movie reviews as positive or negative based on the text content of the reviews. 4.1.1 the imdb dataset you ll work with the imdb dataset a set of 50000 highly polarized reviews from the internet movie database. they re split into 25000 reviews for training and 25000 reviews for testing each set consisting of 50% negative and 50% positive reviews. just like the mnist dataset the imdb dataset comes packaged with keras. it has already been preprocessed the reviews (sequences of words) have been turned into sequences of integers where each integer stands for a specific word in a dictionary. this enables us to focus on model building training and evaluation. in chapter 11 you ll learn how to process raw text input from scratch. the following code will load the dataset (when you run it the first time about 80 mb of data will be downloaded to your machine). from tensorflow.keras.datasets import imdb (train_data train_labels) (test_data test_labels) = imdb.load_data( num_words=10000) the argument num_words=10000 means you ll only keep the top 10000 most fre.quently occurring words in the training data. rare words will be discarded. this allows us to work with vector data of manageable size. if we didn t set this limit we d be work.ing with 88585 unique words in the training data which is unnecessarily large. many of these words only occur in a single sample and thus can t be meaningfully used for classification. the variables train_data and test_data are lists of reviews each review is a list of word indices (encoding a sequence of words). train_labels and test_labels are lists of 0s and 1s where 0 stands for negative and 1 stands for positive >>> train_data [1 14 22 16 ... 178 32] >>> train_labels 1 because we re restricting ourselves to the top 10000 most frequent words no word index will exceed 10000 >>> max([max(sequence) for sequence in train_data]) 9999 for kicks here s how you can quickly decode one of these reviews back to english words. 98 chapter 4 getting started with neural networks classification and regression word_index is a dictionary mapping word_index = imdb.get_word_index() words to an integer index. reverse_word_index = dict( [(value key) for (key value) in word_index.items()]) reverses it decoded_review = " ".join( mapping [reverse_word_index.get(i -3 "?") for i in train_data]) integer indices to words decodes the review. note that the indices are offset by 3 because 0 1 and 2 are reserved indices for padding start of sequence and unknown. 4.1.2 preparing the data you can t directly feed lists of integers into a neural network. they all have different lengths but a neural network expects to process contiguous batches of data. you have to turn your lists into tensors. there are two ways to do that pad your lists so that they all have the same length turn them into an integer tensor of shape (samples max_length) and start your model with a layer capa.ble of handling such integer tensors (the embedding layer which we ll cover in detail later in the book). multi-hot encode your lists to turn them into vectors of 0s and 1s. this would mean for instance turning the sequence [8 5] into a 10000-dimensional vec.tor that would be all 0s except for indices 8 and 5 which would be 1s. then you could use a dense layer capable of handling floating-point vector data as the first layer in your model. let s go with the latter solution to vectorize the data which you ll do manually for maximum clarity. creates an all-zero matrix import numpy as np of shape (len(sequences) def vectorize_sequences(sequences dimension=10000) dimension) results = np.zeros((len(sequences) dimension)) for i sequence in enumerate(sequences) sets specific indices for j in sequence of results[i] to 1s results[i j] = 1. return results x_train = vectorize_sequences(train_data) vectorized x_test = vectorize_sequences(test_data) training data vectorized test data here s what the samples look like now >>> x_train array([ 0. 1. 1. ... 0. 0. 0.]) classifying movie reviews a binary classification example you should also vectorize your labels which is straightforward y_train = np.asarray(train_labels).astype("float32") y_test = np.asarray(test_labels).astype("float32") now the data is ready to be fed into a neural network. 4.1.3 building your model the input data is vectors and the labels are scalars (1s and 0s) this is one of the simplest problem setups you ll ever encounter. a type of model that performs well on such a prob.lem is a plain stack of densely connected (dense) layers with relu activations. there are two key architecture decisions to be made about such a stack of dense layers how many layers to use how many units to choose for each layer input (vectorized text) in chapter 5 you ll learn formal principles to guide you in making these choices. for the time being you ll have to trust me with the following architecture choices two intermediate layers with 16 units each a third layer that will output the scalar predic.tion regarding the sentiment of the current review figure 4.1 shows what the model looks like. and the following listing shows the keras implementation similar to the mnist example you saw previously. figure 4.1 the three-layer model from tensorflow import keras from tensorflow.keras import layers model = keras.sequential([ layers.dense(16 activation="relu") layers.dense(16 activation="relu") layers.dense(1 activation="sigmoid") ]) the first argument being passed to each dense layer is the number of units in the layer the dimensionality of representation space of the layer. you remember from chapters 2 and 3 that each such dense layer with a relu activation implements the fol.lowing chain of tensor operations output = relu(dot(input w) + b) 100 chapter 4 getting started with neural networks classification and regression having 16 units means the weight matrix w will have shape (input_dimension 16) the dot product with w will project the input data onto a 16-dimensional representa.tion space (and then you ll add the bias vector b and apply the relu operation). you can intuitively understand the dimensionality of your representation space as how much freedom you re allowing the model to have when learning internal representa.tions. having more units (a higher-dimensional representation space) allows your model to learn more-complex representations but it makes the model more computa.tionally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data). the intermediate layers use relu as their activation function and the final layer uses a sigmoid activation so as to output a probability (a score between 0 and 1 indicat.ing how likely the sample is to have the target 1 how likely the review is to be posi.tive). a relu (rectified linear unit) is a function meant to zero out negative values (see figure 4.2) whereas a sigmoid squashes arbitrary values into the  interval (see fig.ure 4.3) outputting something that can be interpreted as a probability. finally you need to choose a loss function and an optimizer. because you re facing a binary classification problem and the output of your model is a probability (you end your model with a single-unit layer with a sigmoid activation) it s best to use the binary_crossentropy loss. it isn t the only viable choice for instance you could use mean_squared_error. but crossentropy is usually the best choice when you re dealing classifying movie reviews a binary classification example what are activation functions and why are they necessary? without an activation function like relu (also called a non-linearity) the dense layer would consist of two linear operations a dot product and an addition output = dot(input w) + b the layer could only learn linear transformations (affine transformations) of the input data the hypothesis space of the layer would be the set of all possible linear trans.formations of the input data into a 16-dimensional space. such a hypothesis space is too restricted and wouldn t benefit from multiple layers of representations because a deep stack of linear layers would still implement a linear operation adding more layers wouldn t extend the hypothesis space (as you saw in chapter 2). in order to get access to a much richer hypothesis space that will benefit from deep representations you need a non-linearity or activation function. relu is the most popular activation function in deep learning but there are many other candidates which all come with similarly strange names prelu elu and so on. with models that output probabilities. crossentropy is a quantity from the field of infor. mation theory that measures the distance between probability distributions or in this case between the ground-truth distribution and your predictions. as for the choice of the optimizer we ll go with rmsprop which is a usually a good default choice for virtually any problem. 102 chapter 4 getting started with neural networks classification and regression here s the step where we configure the model with the rmsprop optimizer and the binary_crossentropy loss function. note that we ll also monitor accuracy during training. model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) 4.1.4 validating your approach as you learned in chapter 3 a deep learning model should never be evaluated on its training data it s standard practice to use a validation set to monitor the accuracy of the model during training. here we ll create a validation set by setting apart 10000 samples from the original training data. x_val = x_train partial_x_train = x_train y_val = y_train partial_y_train = y_train we will now train the model for 20 epochs (20 iterations over all samples in the train.ing data) in mini-batches of 512 samples. at the same time we will monitor loss and accuracy on the 10000 samples that we set apart. we do so by passing the validation data as the validation_data argument. history = model.fit(partial_x_train partial_y_train epochs=20 batch_size=512 validation_data=(x_val y_val)) on cpu this will take less than 2 seconds per epoch training is over in 20 seconds. at the end of every epoch there is a slight pause as the model computes its loss and accuracy on the 10000 samples of the validation data. note that the call to model.fit() returns a history object as you saw in chapter 3. this object has a member history which is a dictionary containing data about every.thing that happened during training. let s look at it >>> history_dict = history.history >>> history_dict.keys() [u"accuracy" u"loss" u"val_accuracy" u"val_loss"] classifying movie reviews a binary classification example the dictionary contains four entries one per metric that was being monitored during training and during validation. in the following two listings let s use matplotlib to plot the training and validation loss side by side (see figure 4.4) as well as the training and validation accuracy (see figure 4.5). note that your own results may vary slightly due to a different random initialization of your model. 104 chapter 4 getting started with neural networks classification and regression import matplotlib.pyplot as plt history_dict = history.history loss_values = history_dict["loss"] val_loss_values = history_dict["val_loss"] epochs = range(1 len(loss_values) + 1) plt.plot(epochs loss_values "bo" label="training loss") "bo" is for "blue dot." plt.plot(epochs val_loss_values "b" label="validation loss") plt.title("training and validation loss") plt.xlabel("epochs") "b" is for plt.ylabel("loss") "solid blue line." plt.legend() plt.show() plt.clf() clears the figure acc = history_dict["accuracy"] val_acc = history_dict["val_accuracy"] plt.plot(epochs acc "bo" label="training acc") plt.plot(epochs val_acc "b" label="validation acc") plt.title("training and validation accuracy") plt.xlabel("epochs") plt.ylabel("accuracy") plt.legend() plt.show() as you can see the training loss decreases with every epoch and the training accuracy increases with every epoch. that s what you would expect when running gradient-descent optimization the quantity you re trying to minimize should be less with every iteration. but that isn t the case for the validation loss and accuracy they seem to peak at the fourth epoch. this is an example of what we warned against earlier a model that performs better on the training data isn t necessarily a model that will do better on data it has never seen before. in precise terms what you re seeing is overfitting after the fourth epoch you re overoptimizing on the training data and you end up learning representations that are specific to the training data and don t gener.alize to data outside of the training set. in this case to prevent overfitting you could stop training after four epochs. in general you can use a range of techniques to mitigate overfitting which we ll cover in chapter 5. let s train a new model from scratch for four epochs and then evaluate it on the test data. model = keras.sequential([ layers.dense(16 activation="relu") layers.dense(16 activation="relu") classifying movie reviews a binary classification example layers.dense(1 activation="sigmoid") ]) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) model.fit(x_train y_train epochs=4 batch_size=512) results = model.evaluate(x_test y_test) the final results are as follows the first number 0.29 is the test loss and the second number >>> results 0.88 is the test accuracy. [0.2929924130630493 0.88327999999999995] this fairly naive approach achieves an accuracy of 88%. with state-of-the-art approaches you should be able to get close to 95%. 4.1.5 using a trained model to generate predictions on new data after having trained a model you ll want to use it in a practical setting. you can gener.ate the likelihood of reviews being positive by using the predict method as you ve learned in chapter 3 >>> model.predict(x_test) array([[ 0.98006207] [ 0.99758697] [ 0.99975556] ... [ 0.82167041] [ 0.02885115] [ 0.65371346]] dtype=float32) as you can see the model is confident for some samples (0.99 or more or 0.01 or less) but less confident for others (0.6 0.4). 4.1.6 further experiments the following experiments will help convince you that the architecture choices you ve made are all fairly reasonable although there s still room for improvement you used two representation layers before the final classification layer. try using one or three representation layers and see how doing so affects validation and test accuracy. try using layers with more units or fewer units 32 units 64 units and so on. try using the mse loss function instead of binary_crossentropy. try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu. 106 chapter 4 getting started with neural networks classification and regression 4.1.7 wrapping up here s what you should take away from this example you usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it as tensors into a neural network. sequences of words can be encoded as binary vectors but there are other encoding options too. stacks of dense layers with relu activations can solve a wide range of problems (including sentiment classification) and you ll likely use them frequently. in a binary classification problem (two output classes) your model should end with a dense layer with one unit and a sigmoid activation the output of your model should be a scalar between 0 and 1 encoding a probability. with such a scalar sigmoid output on a binary classification problem the loss function you should use is binary_crossentropy. the rmsprop optimizer is generally a good enough choice whatever your prob.lem. that s one less thing for you to worry about. as they get better on their training data neural networks eventually start over-fitting and end up obtaining increasingly worse results on data they ve never seen before. be sure to always monitor performance on data that is outside of the training set. 4.2 classifying newswires a multiclass classification example in the previous section you saw how to classify vector inputs into two mutually exclu.sive classes using a densely connected neural network. but what happens when you have more than two classes? in this section we ll build a model to classify reuters newswires into 46 mutually exclusive topics. because we have many classes this problem is an instance of multi-class classification and because each data point should be classified into only one cate.gory the problem is more specifically an instance of single-label multiclass classification. if each data point could belong to multiple categories (in this case topics) we d be facing a multilabel multiclass classification problem. 4.2.1 the reuters dataset you ll work with the reuters dataset a set of short newswires and their topics published by reuters in 1986. it s a simple widely used toy dataset for text classification. there are 46 different topics some topics are more represented than others but each topic has at least 10 examples in the training set. like imdb and mnist the reuters dataset comes packaged as part of keras. let s take a look. from tensorflow.keras.datasets import reuters (train_data train_labels) (test_data test_labels) = reuters.load_data( num_words=10000) classifying newswires a multiclass classification example as with the imdb dataset the argument num_words=10000 restricts the data to the 10000 most frequently occurring words found in the data. you have 8982 training examples and 2246 test examples >>> len(train_data) 8982 >>> len(test_data) 2246 as with the imdb reviews each example is a list of integers (word indices) >>> train_data [1 245 273 207 156 53 74 160 26 14 46 296 26 39 74 2979 3554 14 46 4689 4329 86 61 3499 4795 14 61 451 4329 17 12] here s how you can decode it back to words in case you re curious. word_index = reuters.get_word_index() reverse_word_index = dict( [(value key) for (key value) in word_index.items()]) decoded_newswire = " ".join( [reverse_word_index.get(i - 3 "?") for i in train_data]) note that the indices are offset by 3 because 0 1 and 2 are reserved indices for padding start of sequence and unknown. the label associated with an example is an integer between 0 and 45 a topic index >>> train_labels 3 4.2.2 preparing the data you can vectorize the data with the exact same code as in the previous example. vectorized training data x_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data) vectorized test data to vectorize the labels there are two possibilities you can cast the label list as an inte.ger tensor or you can use one-hot encoding. one-hot encoding is a widely used format for categorical data also called categorical encoding. in this case one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index. the following listing shows an example. def to_one_hot(labels dimension=46) results = np.zeros((len(labels) dimension)) 108 chapter 4 getting started with neural networks classification and regression for i label in enumerate(labels) results[i label] = 1. return results vectorized training labels y_train = to_one_hot(train_labels) y_test = to_one_hot(test_labels) vectorized test labels note that there is a built-in way to do this in keras from tensorflow.keras.utils import to_categorical y_train = to_categorical(train_labels) y_test = to_categorical(test_labels) 4.2.3 building your model this topic-classification problem looks similar to the previous movie-review classifica.tion problem in both cases we re trying to classify short snippets of text. but there is a new constraint here the number of output classes has gone from 2 to 46. the dimensionality of the output space is much larger. in a stack of dense layers like those we ve been using each layer can only access information present in the output of the previous layer. if one layer drops some information relevant to the classification problem this information can never be recovered by later layers each layer can potentially become an information bottle.neck. in the previous example we used 16-dimensional intermediate layers but a 16-dimensional space may be too limited to learn to separate 46 different classes such small layers may act as information bottlenecks permanently dropping rele.vant information. for this reason we ll use larger layers. let s go with 64 units. model = keras.sequential([ layers.dense(64 activation="relu") layers.dense(64 activation="relu") layers.dense(46 activation="softmax") ]) there are two other things you should note about this architecture. first we end the model with a dense layer of size 46. this means for each input sample the network will output a 46-dimensional vector. each entry in this vector (each dimension) will encode a different output class. second the last layer uses a softmax activation. you saw this pattern in the mnist example. it means the model will output a probability distribution over the 46 different output classes for every input sample the model will produce a 46-dimensional out.put vector where output[i] is the probability that the sample belongs to class i. the 46 scores will sum to 1. the best loss function to use in this case is categorical_crossentropy. it mea.sures the distance between two probability distributions here between the probability classifying newswires a multiclass classification example distribution output by the model and the true distribution of the labels. by minimiz.ing the distance between these two distributions you train the model to output some.thing as close as possible to the true labels. model.compile(optimizer="rmsprop" loss="categorical_crossentropy" metrics=["accuracy"]) 4.2.4 validating your approach let s set apart 1000 samples in the training data to use as a validation set. x_val = x_train partial_x_train = x_train y_val = y_train partial_y_train = y_train now let s train the model for 20 epochs. history = model.fit(partial_x_train partial_y_train epochs=20 batch_size=512 validation_data=(x_val y_val)) and finally let s display its loss and accuracy curves (see figures 4.6 and 4.7). 110 chapter 4 getting started with neural networks classification and regression loss = history.history["loss"] val_loss = history.history["val_loss"] epochs = range(1 len(loss) + 1) plt.plot(epochs loss "bo" label="training loss") plt.plot(epochs val_loss "b" label="validation loss") plt.title("training and validation loss") plt.xlabel("epochs") plt.ylabel("loss") plt.legend() plt.show() clears the figure plt.clf() acc = history.history["accuracy"] val_acc = history.history["val_accuracy"] plt.plot(epochs acc "bo" label="training accuracy") plt.plot(epochs val_acc "b" label="validation accuracy") plt.title("training and validation accuracy") plt.xlabel("epochs") plt.ylabel("accuracy") plt.legend() plt.show() the model begins to overfit after nine epochs. let s train a new model from scratch for nine epochs and then evaluate it on the test set. model = keras.sequential([ layers.dense(64 activation="relu") classifying newswires a multiclass classification example layers.dense(64 activation="relu") layers.dense(46 activation="softmax") ]) model.compile(optimizer="rmsprop" loss="categorical_crossentropy" metrics=["accuracy"]) model.fit(x_train y_train epochs=9 batch_size=512) results = model.evaluate(x_test y_test) here are the final results >>> results [0.9565213431445807 0.79697239536954589] this approach reaches an accuracy of ~80%. with a balanced binary classification problem the accuracy reached by a purely random classifier would be 50%. but in this case we have 46 classes and they may not be equally represented. what would be the accuracy of a random baseline? we could try quickly implementing one to check this empirically >>> import copy >>> test_labels_copy = copy.copy(test_labels) >>> np.random.shuffle(test_labels_copy) >>> hits_array = np.array(test_labels) == np.array(test_labels_copy) >>> hits_array.mean() 0.18655387355298308 as you can see a random classifier would score around 19% classification accuracy so the results of our model seem pretty good in that light. 4.2.5 generating predictions on new data calling the model s predict method on new samples returns a class probability distri.bution over all 46 topics for each sample. let s generate topic predictions for all of the test data predictions = model.predict(x_test) each entry in predictions is a vector of length 46 >>> predictions.shape (46) the coefficients in this vector sum to 1 as they form a probability distribution >>> np.sum(predictions) 1.0 112 chapter 4 getting started with neural networks classification and regression the largest entry is the predicted class the class with the highest probability >>> np.argmax(predictions) 4 4.2.6 a different way to handle the labels and the loss we mentioned earlier that another way to encode the labels would be to cast them as an integer tensor like this y_train = np.array(train_labels) y_test = np.array(test_labels) the only thing this approach would change is the choice of the loss function. the loss function used in listing 4.21 categorical_crossentropy expects the labels to follow a categorical encoding. with integer labels you should use sparse_categorical_ crossentropy model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) this new loss function is still mathematically the same as categorical_crossentropy it just has a different interface. 4.2.7 the importance of having sufficiently large intermediate layers we mentioned earlier that because the final outputs are 46-dimensional you should avoid intermediate layers with many fewer than 46 units. now let s see what happens when we introduce an information bottleneck by having intermediate layers that are significantly less than 46-dimensional for example 4-dimensional. model = keras.sequential([ layers.dense(64 activation="relu") layers.dense(4 activation="relu") layers.dense(46 activation="softmax") ]) model.compile(optimizer="rmsprop" loss="categorical_crossentropy" metrics=["accuracy"]) model.fit(partial_x_train partial_y_train epochs=20 batch_size=128 validation_data=(x_val y_val)) the model now peaks at ~71% validation accuracy an 8% absolute drop. this drop is mostly due to the fact that we re trying to compress a lot of information (enough predicting house prices a regression example information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. the model is able to cram most of the necessary information into these four-dimensional representations but not all of it. 4.2.8 further experiments like in the previous example i encourage you to try out the following experiments to train your intuition about the kind of configuration decisions you have to make with such models try using larger or smaller layers 32 units 128 units and so on. you used two intermediate layers before the final softmax classification layer. now try using a single intermediate layer or three intermediate layers. 4.2.9 wrapping up here s what you should take away from this example if you re trying to classify data points among n classes your model should end with a dense layer of size n. in a single-label multiclass classification problem your model should end with a softmax activation so that it will output a probability distribution over the n output classes. categorical crossentropy is almost always the loss function you should use for such problems. it minimizes the distance between the probability distributions output by the model and the true distribution of the targets. there are two ways to handle labels in multiclass classification encoding the labels via categorical encoding (also known as one-hot encod.ing) and using categorical_crossentropy as a loss function encoding the labels as integers and using the sparse_categorical_cross.entropy loss function if you need to classify data into a large number of categories you should avoid creating information bottlenecks in your model due to intermediate layers that are too small. 4.3 predicting house prices a regression example the two previous examples were considered classification problems where the goal was to predict a single discrete label of an input data point. another common type of machine learning problem is regression which consists of predicting a continuous value instead of a discrete label for instance predicting the temperature tomorrow given meteorological data or predicting the time that a software project will take to complete given its specifications. note don t confuse regression and the logistic regression algorithm. confusingly logistic regression isn t a regression algorithm it s a classification algorithm. 114 chapter 4 getting started with neural networks classification and regression 4.3.1 the boston housing price dataset in this section we ll attempt to predict the median price of homes in a given boston suburb in the mid-1970s given data points about the suburb at the time such as the crime rate the local property tax rate and so on. the dataset we ll use has an interest.ing difference from the two previous examples. it has relatively few data points only 506 split between 404 training samples and 102 test samples. and each feature in the input data (for example the crime rate) has a different scale. for instance some val.ues are proportions which take values between 0 and 1 others take values between 1 and 12 others between 0 and 100 and so on. from tensorflow.keras.datasets import boston_housing (train_data train_targets) (test_data test_targets) = ( boston_housing.load_data()) let s look at the data >>> train_data.shape (404 13) >>> test_data.shape (102 13) as you can see we have 404 training samples and 102 test samples each with 13 numerical features such as per capita crime rate average number of rooms per dwell.ing accessibility to highways and so on. the targets are the median values of owner-occupied homes in thousands of dollars >>> train_targets [15.2 42.3 50.... 19.4 19.4 29.1] the prices are typically between $10000 and $50000. if that sounds cheap remem.ber that this was the mid-1970s and these prices aren t adjusted for inflation. 4.3.2 preparing the data it would be problematic to feed into a neural network values that all take wildly differ.ent ranges. the model might be able to automatically adapt to such heterogeneous data but it would definitely make learning more difficult. a widespread best practice for dealing with such data is to do feature-wise normalization for each feature in the input data (a column in the input data matrix) we subtract the mean of the feature and divide by the standard deviation so that the feature is centered around 0 and has a unit standard deviation. this is easily done in numpy. mean = train_data.mean(axis=0) train_data -= mean predicting house prices a regression example std = train_data.std(axis=0) train_data /= std test_data -= mean test_data /= std note that the quantities used for normalizing the test data are computed using the training data. you should never use any quantity computed on the test data in your workflow even for something as simple as data normalization. 4.3.3 building your model because so few samples are available we ll use a very small model with two intermedi.ate layers each with 64 units. in general the less training data you have the worse overfitting will be and using a small model is one way to mitigate overfitting. def build_model() model = keras.sequential([ because we need to instantiate layers.dense(64 activation="relu") the same model multiple times layers.dense(64 activation="relu") we use a function to construct it. layers.dense(1) ]) model.compile(optimizer="rmsprop" loss="mse" metrics=["mae"]) return model the model ends with a single unit and no activation (it will be a linear layer). this is a typical setup for scalar regression (a regression where you re trying to predict a single continuous value). applying an activation function would constrain the range the out.put can take for instance if you applied a sigmoid activation function to the last layer the model could only learn to predict values between 0 and 1. here because the last layer is purely linear the model is free to learn to predict values in any range. note that we compile the model with the mse loss function mean squared error the square of the difference between the predictions and the targets. this is a widely used loss function for regression problems. we re also monitoring a new metric during training mean absolute error (mae). it s the absolute value of the difference between the predictions and the targets. for instance an mae of 0.5 on this problem would mean your predictions are off by $500 on average. 4.3.4 validating your approach using k-fold validation to evaluate our model while we keep adjusting its parameters (such as the number of epochs used for training) we could split the data into a training set and a validation set as we did in the previous examples. but because we have so few data points the validation set would end up being very small (for instance about 100 examples). as a consequence the validation scores might change a lot depending on which data points we chose for validation and which we chose for training the validation scores 116 chapter 4 getting started with neural networks classification and regression might have a high variance with regard to the validation split. this would prevent us from reliably evaluating our model. the best practice in such situations is to use k-fold cross-validation (see figure 4.8). data split into 3 partitions validation fold 1 score #1 validation final score fold 2 score #2 average validation fold 3 score #3 figure 4.8 k-fold cross-validation with k=3 it consists of splitting the available data into k partitions (typically k = 4 or 5) instanti.ating k identical models and training each one on k 1 partitions while evaluating on the remaining partition. the validation score for the model used is then the aver.age of the k validation scores obtained. in terms of code this is straightforward. k = 4 num_val_samples = len(train_data) // k num_epochs = 100 prepares the all_scores = [] validation data data for i in range(k) from partition #k print(f"processing fold #{i}") val_data = train_data[i * num_val_samples (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples (i + 1) * num_val_samples] partial_train_data = np.concatenate( prepares the training data [train_data[i * num_val_samples] data from all other partitions train_data[(i + 1) * num_val_samples]] axis=0) partial_train_targets = np.concatenate( [train_targets[i * num_val_samples] train_targets[(i + 1) * num_val_samples]] axis=0) model = build_model() model.fit(partial_train_data partial_train_targets builds the keras model (already compiled) trains the model (in silent mode verbose = 0) epochs=num_epochs batch_size=16 verbose=0) val_mse val_mae = model.evaluate(val_data val_targets verbose=0) all_scores.append(val_mae) evaluates the model on the validation data predicting house prices a regression example 117 running this with num_epochs = 100 yields the following results >>> all_scores [2.112449 3.0801501 2.6483836 2.4275346] >>> np.mean(all_scores) 2.5671294 the different runs do indeed show rather different validation scores from 2.1 to 3.1. the average (2.6) is a much more reliable metric than any single score that s the entire point of k-fold cross-validation. in this case we re off by $2600 on average which is significant considering that the prices range from $10000 to $50000. let s try training the model a bit longer 500 epochs. to keep a record of how well the model does at each epoch we ll modify the training loop to save the per-epoch validation score log for each fold. num_epochs = 500 prepares the all_mae_histories = [] validation data data for i in range(k) from partition #k print(f"processing fold #{i}") val_data = train_data[i * num_val_samples (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples (i + 1) * num_val_samples] partial_train_data = np.concatenate( prepares the training [train_data[i * num_val_samples] data data from all train_data[(i + 1) * num_val_samples]] other partitions axis=0) partial_train_targets = np.concatenate( [train_targets[i * num_val_samples] builds the keras train_targets[(i + 1) * num_val_samples]] model (already axis=0) compiled) model = build_model() history = model.fit(partial_train_data partial_train_targets validation_data=(val_data val_targets) epochs=num_epochs batch_size=16 verbose=0) mae_history = history.history["val_mae"] all_mae_histories.append(mae_history) we can then compute the average of the per-epoch mae scores for all folds. trains the model (in silent mode verbose=0) average_mae_history = [ np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)] let s plot this see figure 4.9. plt.plot(range(1 len(average_mae_history) + 1) average_mae_history) plt.xlabel("epochs") plt.ylabel("validation mae") plt.show() 118 chapter 4 getting started with neural networks classification and regression it may be a little difficult to read the plot due to a scaling issue the validation mae for the first few epochs is dramatically higher than the values that follow. let s omit the first 10 data points which are on a different scale than the rest of the curve. truncated_mae_history = average_mae_history plt.plot(range(1 len(truncated_mae_history) + 1) truncated_mae_history) plt.xlabel("epochs") plt.ylabel("validation mae") plt.show() as you can see in figure 4.10 validation mae stops improving significantly after 120 140 epochs (this number includes the 10 epochs we omitted). past that point we start overfitting. once you re finished tuning other parameters of the model (in addition to the number of epochs you could also adjust the size of the intermediate layers) you can train a final production model on all of the training data with the best parameters and then look at its performance on the test data. gets a fresh compiled model trains it on the model = build_model() entirety of the data model.fit(train_data train_targets epochs=130batch_size=16verbose=0) test_mse_score test_mae_score = model.evaluate(test_data test_targets) predicting house prices a regression example here s the final result >>> test_mae_score 2.4642276763916016 we re still off by a bit under $2500. it s an improvement! just like with the two previ.ous tasks you can try varying the number of layers in the model or the number of units per layer to see if you can squeeze out a lower test error. 4.3.5 generating predictions on new data when calling predict() on our binary classification model we retrieved a scalar score between 0 and 1 for each input sample. with our multiclass classification model we retrieved a probability distribution over all classes for each sample. now with this sca.lar regression model predict() returns the model s guess for the sample s price in thousands of dollars >>> predictions = model.predict(test_data) >>> predictions array([9.990133] dtype=float32) the first house in the test set is predicted to have a price of about $10000. 4.3.6 wrapping up here s what you should take away from this scalar regression example regression is done using different loss functions than we used for classification. mean squared error (mse) is a loss function commonly used for regression. 120 chapter 4 getting started with neural networks classification and regression similarly evaluation metrics to be used for regression differ from those used for classification naturally the concept of accuracy doesn t apply for regression. a common regression metric is mean absolute error (mae). when features in the input data have values in different ranges each feature should be scaled independently as a preprocessing step. when there is little data available using k-fold validation is a great way to reli.ably evaluate a model. when little training data is available it s preferable to use a small model with few intermediate layers (typically only one or two) in order to avoid severe overfitting. summary the three most common kinds of machine learning tasks on vector data are binary classification multiclass classification and scalar regression. the wrapping up sections earlier in the chapter summarize the important points you ve learned regarding each task. regression uses different loss functions and different evaluation metrics than classification. you ll usually need to preprocess raw data before feeding it into a neural network. when your data has features with different ranges scale each feature inde.pendently as part of preprocessing. as training progresses neural networks eventually begin to overfit and obtain worse results on never-before-seen data. if you don t have much training data use a small model with only one or two intermediate layers to avoid severe overfitting. if your data is divided into many categories you may cause information bottle.necks if you make the intermediate layers too small. when you re working with little data k-fold validation can help reliably evalu.ate your model. this chapter covers understanding the tension between generalization and optimization the fundamental issue in machine learning evaluation methods for machine learning models best practices to improve model fitting best practices to achieve better generalization after the three practical examples in chapter 4 you should be starting to feel famil.iar with how to approach classification and regression problems using neural net.works and you ve witnessed the central problem of machine learning overfitting. this chapter will formalize some of your new intuition about machine learning into a solid conceptual framework highlighting the importance of accurate model eval.uation and the balance between training and generalization. 5.1 generalization the goal of machine learning in the three examples presented in chapter 4 predicting movie reviews topic clas.sification and house-price regression we split the data into a training set a valida.tion set and a test set. the reason not to evaluate the models on the same data they 121 were trained on quickly became evident after just a few epochs performance on never-before-seen data started diverging from performance on the training data which always improves as training progresses. the models started to overfit. overfitting happens in every machine learning problem. the fundamental issue in machine learning is the tension between optimization and generalization. optimization refers to the process of adjusting a model to get the best performance possible on the training data (the learning in machine learning) whereas generalization refers to how well the trained model performs on data it has never seen before. the goal of the game is to get good generalization of course but you don t control generalization you can only fit the model to its training data. if you do that too well overfitting kicks in and generalization suffers. but what causes overfitting? how can we achieve good generalization? 5.1.1 underfitting and overfitting for the models you saw in the previous chapter performance on the held-out valida.tion data started improving as training went on and then inevitably peaked after a while. this pattern (illustrated in figure 5.1) is universal. you ll see it with any model type and any dataset. at the beginning of training optimization and generalization are correlated the lower the loss on training data the lower the loss on test data. while this is happening your model is said to be underfit there is still progress to be made the network hasn t yet modeled all relevant patterns in the training data. but after a certain number of iterations on the training data generalization stops improving validation metrics stall and then begin to degrade the model is starting to overfit. that is it s beginning to learn patterns that are specific to the training data but that are misleading or irrele.vant when it comes to new data. generalization the goal of machine learning overfitting is particularly likely to occur when your data is noisy if it involves uncertainty or if it includes rare features. let s look at concrete examples. noisy training data in real-world datasets it s fairly common for some inputs to be invalid. perhaps a mnist digit could be an all-black image for instance or something like figure 5.2. what are these? i don t know either. but they re all part of the mnist training set. what s even worse however is having perfectly valid inputs that end up mislabeled like those in figure 5.3. if a model goes out of its way to incorporate such outliers its generalization perfor.mance will degrade as shown in figure 5.4. for instance a 4 that looks very close to the mislabeled 4 in figure 5.3 may end up getting classified as a 9. ambiguous features not all data noise comes from inaccuracies even perfectly clean and neatly labeled data can be noisy when the problem involves uncertainty and ambiguity. in classifica.tion tasks it is often the case that some regions of the input feature space are associ.ated with multiple classes at the same time. let s say you re developing a model that takes an image of a banana and predicts whether the banana is unripe ripe or rotten. these categories have no objective boundaries so the same picture might be classified as either unripe or ripe by different human labelers. similarly many problems involve randomness. you could use atmospheric pressure data to predict whether it will rain tomorrow but the exact same measurements may be followed sometimes by rain and sometimes by a clear sky with some probability. a model could overfit to such probabilistic data by being too confident about ambiguous regions of the feature space like in figure 5.5. a more robust fit would ignore individual data points and look at the bigger picture. generalization the goal of machine learning rare features and spurious correlations if you ve only ever seen two orange tabby cats in your life and they both happened to be terribly antisocial you might infer that orange tabby cats are generally likely to be antisocial. that s overfitting if you had been exposed to a wider variety of cats includ.ing more orange ones you d have learned that cat color is not well correlated with character. likewise machine learning models trained on datasets that include rare feature values are highly susceptible to overfitting. in a sentiment classification task if the word cherimoya (a fruit native to the andes) only appears in one text in the train.ing data and this text happens to be negative in sentiment a poorly regularized model might put a very high weight on this word and always classify new texts that mention cherimoyas as negative whereas objectively there s nothing negative about the cherimoya.1 importantly a feature value doesn t need to occur only a couple of times to lead to spurious correlations. consider a word that occurs in 100 samples in your training data and that s associated with a positive sentiment 54% of the time and with a nega.tive sentiment 46% of the time. that difference may well be a complete statistical fluke yet your model is likely to learn to leverage that feature for its classification task. this is one of the most common sources of overfitting. here s a striking example. take mnist. create a new training set by concatenating 784 white noise dimensions to the existing 784 dimensions of the data so half of the data is now noise. for comparison also create an equivalent dataset by concatenating 784 all-zeros dimensions. our concatenation of meaningless features does not at all affect the information content of the data we re only adding something. human clas.sification accuracy wouldn t be affected by these transformations at all. from tensorflow.keras.datasets import mnist import numpy as np (train_images train_labels) _ = mnist.load_data() train_images = train_images.reshape((60000 28 * 28)) train_images = train_images.astype("float32") / 255 train_images_with_noise_channels = np.concatenate( [train_images np.random.random((len(train_images) 784))] axis=1) train_images_with_zeros_channels = np.concatenate( [train_images np.zeros((len(train_images) 784))] axis=1) now let s train the model from chapter 2 on both of these training sets. 1 mark twain even called it the most delicious fruit known to men. from tensorflow import keras from tensorflow.keras import layers def get_model() model = keras.sequential([ layers.dense(512 activation="relu") layers.dense(10 activation="softmax") ]) model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) return model model = get_model() history_noise = model.fit( train_images_with_noise_channels train_labels epochs=10 batch_size=128 validation_split=0.2) model = get_model() history_zeros = model.fit( train_images_with_zeros_channels train_labels epochs=10 batch_size=128 validation_split=0.2) let s compare how the validation accuracy of each model evolves over time. import matplotlib.pyplot as plt val_acc_noise = history_noise.history["val_accuracy"] val_acc_zeros = history_zeros.history["val_accuracy"] epochs = range(1 11) plt.plot(epochs val_acc_noise "b-" label="validation accuracy with noise channels") plt.plot(epochs val_acc_zeros "b--" label="validation accuracy with zeros channels") plt.title("effect of noise channels on validation accuracy") plt.xlabel("epochs") plt.ylabel("accuracy") plt.legend() despite the data holding the same information in both cases the validation accuracy of the model trained with noise channels ends up about one percentage point lower (see figure 5.6) purely through the influence of spurious correlations. the more noise channels you add the further accuracy will degrade. noisy features inevitably lead to overfitting. as such in cases where you aren t sure whether the features you have are informative or distracting it s common to do feature generalization the goal of machine learning selection before training. restricting the imdb data to the top 10000 most common words was a crude form of feature selection for instance. the typical way to do fea.ture selection is to compute some usefulness score for each feature available a measure of how informative the feature is with respect to the task such as the mutual information between the feature and the labels and only keep features that are above some threshold. doing this would filter out the white noise channels in the preceding example. 5.1.2 the nature of generalization in deep learning a remarkable fact about deep learning models is that they can be trained to fit any.thing as long as they have enough representational power. don t believe me? try shuffling the mnist labels and train a model on that. even though there is no relationship whatsoever between the inputs and the shuffled labels the training loss goes down just fine even with a relatively small model. natu.rally the validation loss does not improve at all over time since there is no possibility of generalization in this setting. (train_images train_labels) _ = mnist.load_data() train_images = train_images.reshape((60000 28 * 28)) train_images = train_images.astype("float32") / 255 random_train_labels = train_labels[] np.random.shuffle(random_train_labels) model = keras.sequential([ layers.dense(512 activation="relu") layers.dense(10 activation="softmax") ]) model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) model.fit(train_images random_train_labels epochs=100 batch_size=128 validation_split=0.2) in fact you don t even need to do this with mnist data you could just generate white noise inputs and random labels. you could fit a model on that too as long as it has enough parameters. it would just end up memorizing specific inputs much like a python dictionary. if this is the case then how come deep learning models generalize at all? shouldn t they just learn an ad hoc mapping between training inputs and targets like a fancy dict? what expectation can we have that this mapping will work for new inputs? as it turns out the nature of generalization in deep learning has rather little to do with deep learning models themselves and much to do with the structure of informa.tion in the real world. let s take a look at what s really going on here. the manifold hypothesis the input to an mnist classifier (before preprocessing) is a 28 28 array of integers between 0 and 255. the total number of possible input values is thus 256 to the power of 784 much greater than the number of atoms in the universe. however very few of these inputs would look like valid mnist samples actual handwritten digits only occupy a tiny subspace of the parent space of all possible 28 28 uint8 arrays. what s more this subspace isn t just a set of points sprinkled at random in the parent space it is highly structured. first the subspace of valid handwritten digits is continuous if you take a sample and modify it a little it will still be recognizable as the same handwritten digit. fur.ther all samples in the valid subspace are connected by smooth paths that run through the subspace. this means that if you take two random mnist digits a and b there exists a sequence of intermediate images that morph a into b such that two consec.utive digits are very close to each other (see figure 5.7). perhaps there will be a few ambiguous shapes close to the boundary between two classes but even these shapes would still look very digit-like. in technical terms you would say that handwritten digits form a manifold within the space of possible 28 28 uint8 arrays. that s a big word but the concept is pretty intuitive. a manifold is a lower-dimensional subspace of some parent space that is locally similar to a linear (euclidian) space. for instance a smooth curve in the plane is a 1d manifold within a 2d space because for every point of the curve you can draw a tangent (the curve can be approximated by a line at every point). a smooth surface within a 3d space is a 2d manifold. and so on. generalization the goal of machine learning more generally the manifold hypothesis posits that all natural data lies on a low-dimen.sional manifold within the high-dimensional space where it is encoded. that s a pretty strong statement about the structure of information in the universe. as far as we know it s accurate and it s the reason why deep learning works. it s true for mnist digits but also for human faces tree morphology the sounds of the human voice and even natural language. the manifold hypothesis implies that machine learning models only have to fit relatively simple low-dimensional highly structured subspaces within their potential input space (latent mani.folds). within one of these manifolds it s always possible to interpolate between two inputs that is to say morph one into another via a continuous path along which all points fall on the manifold. the ability to interpolate between samples is the key to understanding generalization in deep learning. interpolation as a source of generalization if you work with data points that can be interpolated you can start making sense of points you ve never seen before by relating them to other points that lie close on the manifold. in other words you can make sense of the totality of the space using only a sample of the space. you can use interpolation to fill in the blanks. note that interpolation on the latent manifold is different from linear interpola.tion in the parent space as illustrated in figure 5.8. for instance the average of pixels between two mnist digits is usually not a valid digit. crucially while deep learning achieves generalization via interpolation on a learned approximation of the data manifold it would be a mistake to assume that interpolation is all there is to generalization. it s the tip of the iceberg. interpolation can only help you make sense of things that are very close to what you ve seen before manifold interpolation (intermediate point on the latent manifold) figure 5.8 difference between linear interpolation and interpolation on the latent manifold. every point on the latent manifold of digits is a valid linear interpolation (average in the encoding space) digit but the average of two digits usually isn t. it enables local generalization. but remarkably humans deal with extreme novelty all the time and they do just fine. you don t need to be trained in advance on countless examples of every situation you ll ever have to encounter. every single one of your days is different from any day you ve experienced before and different from any day experienced by anyone since the dawn of humanity. you can switch between spending a week in nyc a week in shanghai and a week in bangalore without requiring thou.sands of lifetimes of learning and rehearsal for each city. humans are capable of extreme generalization which is enabled by cognitive mecha.nisms other than interpolation abstraction symbolic models of the world reasoning logic common sense innate priors about the world what we generally call reason as opposed to intuition and pattern recognition. the latter are largely interpolative in nature but the former isn t. both are essential to intelligence. we ll talk more about this in chapter 14. why deep learning works remember the crumpled paper ball metaphor from chapter 2? a sheet of paper rep.resents a 2d manifold within 3d space (see figure 5.9). a deep learning model is a tool for uncrumpling paper balls that is for disentangling latent manifolds. a deep learning model is basically a very high-dimensional curve a curve that is smooth and continuous (with additional constraints on its structure originating from model architecture priors) since it needs to be differentiable. and that curve is fitted to data points via gradient descent smoothly and incrementally. by its very nature deep learning is about taking a big complex curve a manifold and incrementally adjusting its parameters until it fits some training data points. generalization the goal of machine learning the curve involves enough parameters that it could fit anything indeed if you let your model train for long enough it will effectively end up purely memorizing its training data and won t generalize at all. however the data you re fitting to isn t made of isolated points sparsely distributed across the underlying space. your data forms a highly structured low-dimensional manifold within the input space that s the mani.fold hypothesis. and because fitting your model curve to this data happens gradually and smoothly over time as gradient descent progresses there will be an intermediate point during training at which the model roughly approximates the natural manifold of the data as you can see in figure 5.10. further training a robust before training beginning of training t is achieved transitively final state the model the model starts the model gradually in the process of morphing over ts the training data with a random initial state. moves toward a better t. the model from its initial reaching perfect training loss. state to its nal state. test time performance test time performance of robustly t model of over t model on new data points on new data points moving along the curve learned by the model at that point will come close to moving along the actual latent manifold of the data as such the model will be capable of making sense of never-before-seen inputs via interpolation between training inputs. besides the trivial fact that they have sufficient representational power there are a few properties of deep learning models that make them particularly well-suited to learning latent manifolds deep learning models implement a smooth continuous mapping from their inputs to their outputs. it has to be smooth and continuous because it must be differentiable by necessity (you couldn t do gradient descent otherwise). this smoothness helps approximate latent manifolds which follow the same properties. deep learning models tend to be structured in a way that mirrors the shape of the information in their training data (via architecture priors). this is particu.larly the case for image-processing models (discussed in chapters 8 and 9) and sequence-processing models (chapter 10). more generally deep neural net.works structure their learned representations in a hierarchical and modular way which echoes the way natural data is organized. training data is paramount while deep learning is indeed well suited to manifold learning the power to general.ize is more a consequence of the natural structure of your data than a consequence of any property of your model. you ll only be able to generalize if your data forms a man.ifold where points can be interpolated. the more informative and the less noisy your features are the better you will be able to generalize since your input space will be simpler and better structured. data curation and feature engineering are essential to generalization. further because deep learning is curve fitting for a model to perform well it needs to be trained on a dense sampling of its input space. a dense sampling in this context means that the training data should densely cover the entirety of the input data manifold (see figure 5.11). this is especially true near decision boundaries. with a sufficiently dense sampling it becomes possible to make sense of new inputs by inter.polating between past training inputs without having to use common sense abstract reasoning or external knowledge about the world all things that machine learning models have no access to. sparse sampling the model learned doesn t match the latent space and leads to incorrect interpolation. dense sampling the model learned approximates the latent space well and interpolation leads to generalization. evaluating machine learning models as such you should always keep in mind that the best way to improve a deep learning model is to train it on more data or better data (of course adding overly noisy or inac.curate data will harm generalization). a denser coverage of the input data manifold will yield a model that generalizes better. you should never expect a deep learning model to perform anything more than crude interpolation between its training sam.ples and thus you should do everything you can to make interpolation as easy as pos.sible. the only thing you will find in a deep learning model is what you put into it the priors encoded in its architecture and the data it was trained on. when getting more data isn t possible the next best solution is to modulate the quantity of information that your model is allowed to store or to add constraints on the smoothness of the model curve. if a network can only afford to memorize a small number of patterns or very regular patterns the optimization process will force it to focus on the most prominent patterns which have a better chance of generalizing well. the process of fighting overfitting this way is called regularization. we ll review regularization techniques in depth in section 5.4.4. before you can start tweaking your model to help it generalize better you ll need a way to assess how your model is currently doing. in the following section you ll learn how you can monitor generalization during model development model evaluation. 5.2 evaluating machine learning models you can only control what you can observe. since your goal is to develop models that can successfully generalize to new data it s essential to be able to reliably measure the generalization power of your model. in this section i ll formally introduce the differ.ent ways you can evaluate machine learning models. you ve already seen most of them in action in the previous chapter. 5.2.1 training validation and test sets evaluating a model always boils down to splitting the available data into three sets training validation and test. you train on the training data and evaluate your model on the validation data. once your model is ready for prime time you test it one final time on the test data which is meant to be as similar as possible to production data. then you can deploy the model in production. you may ask why not have two sets a training set and a test set? you d train on the training data and evaluate on the test data. much simpler! the reason is that developing a model always involves tuning its configuration for example choosing the number of layers or the size of the layers (called the hyperpa.rameters of the model to distinguish them from the parameters which are the network s weights). you do this tuning by using as a feedback signal the performance of the model on the validation data. in essence this tuning is a form of learning a search for a good configuration in some parameter space. as a result tuning the configuration of the model based on its performance on the validation set can quickly result in over-fitting to the validation set even though your model is never directly trained on it. central to this phenomenon is the notion of information leaks. every time you tune a hyperparameter of your model based on the model s performance on the validation set some information about the validation data leaks into the model. if you do this only once for one parameter then very few bits of information will leak and your val.idation set will remain reliable for evaluating the model. but if you repeat this many times running one experiment evaluating on the validation set and modifying your model as a result then you ll leak an increasingly significant amount of information about the validation set into the model. at the end of the day you ll end up with a model that performs artificially well on the validation data because that s what you optimized it for. you care about perfor.mance on completely new data not on the validation data so you need to use a com.pletely different never-before-seen dataset to evaluate the model the test dataset. your model shouldn t have had access to any information about the test set even indi.rectly. if anything about the model has been tuned based on test set performance then your measure of generalization will be flawed. splitting your data into training validation and test sets may seem straightforward but there are a few advanced ways to do it that can come in handy when little data is available. let s review three classic evaluation recipes simple holdout validation k-fold validation and iterated k-fold validation with shuffling. we ll also talk about the use of common-sense baselines to check that your training is going somewhere. simple holdout validation set apart some fraction of your data as your test set. train on the remaining data and evaluate on the test set. as you saw in the previous sections in order to prevent infor.mation leaks you shouldn t tune your model based on the test set and therefore you should also reserve a validation set. schematically holdout validation looks like figure 5.12. listing 5.5 shows a simple implementation. figure 5.12 simple train on this evaluate on this holdout validation split shuffling the data is num_validation_samples = 10000 usually appropriate. np.random.shuffle(data) evaluating machine learning models validation_data = data[num_validation_samples] defines the defines the training set training_data = data[num_validation_samples] validation model = get_model() trains a model on the set model.fit(training_data ...) training data and evaluates it on the validation data at this point you can tune your model validation_score = model.evaluate(validation_data ...) ... retrain it evaluate it tune it again. model = get_model() once you ve tuned your model.fit(np.concatenate([training_data hyperparameters it s common to validation_data]) ...) train your final model from scratch test_score = model.evaluate(test_data ...) on all non-test data available. this is the simplest evaluation protocol and it suffers from one flaw if little data is available then your validation and test sets may contain too few samples to be statisti.cally representative of the data at hand. this is easy to recognize if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance then you re having this issue. k-fold validation and iterated k-fold validation are two ways to address this as discussed next. k-fold validation with this approach you split your data into k partitions of equal size. for each parti.tion i train a model on the remaining k -1 partitions and evaluate it on partition i. your final score is then the averages of the k scores obtained. this method is helpful when the performance of your model shows significant variance based on your train-test split. like holdout validation this method doesn t exempt you from using a dis.tinct validation set for model calibration. schematically k-fold cross-validation looks like figure 5.13. listing 5.6 shows a sim.ple implementation. data split into 3 partitions validation fold 1 score #1 validation final score fold 2 score #2 average validation fold 3 score #3 figure 5.13 k-fold cross-validation with k=3 k=3 num_validation_samples = len(data) // k np.random.shuffle(data) validation_scores = [] for fold in range(k) validation_data = data[num_validation_samples * fold num_validation_samples * (fold + 1)] selects the validation-data partition training_data = np.concatenate( data[num_validation_samples * fold] creates a brand-new instance of the model data[num_validation_samples * (fold + 1)]) (untrained) model = get_model() model.fit(training_data ...) validation score validation_score = model.evaluate(validation_data ...) average of the validation validation_scores.append(validation_score) scores of the k folds validation_score = np.average(validation_scores) model = get_model() model.fit(data ...) trains the final test_score = model.evaluate(test_data ...) model on all non-test data available uses the remainder of the data as training data. note that the + operator represents list concatenation not summation. iterated k-fold validation with shuffling this one is for situations in which you have relatively little data available and you need to evaluate your model as precisely as possible. i ve found it to be extremely helpful in kaggle competitions. it consists of applying k-fold validation multiple times shuffling the data every time before splitting it k ways. the final score is the average of the scores obtained at each run of k-fold validation. note that you end up training and evaluating p * k models (where p is the number of iterations you use) which can be very expensive. 5.2.2 beating a common-sense baseline besides the different evaluation protocols you have available one last thing you should know about is the use of common-sense baselines. training a deep learning model is a bit like pressing a button that launches a rocket in a parallel world. you can t hear it or see it. you can t observe the manifold learning process it s happening in a space with thousands of dimensions and even if you projected it to 3d you couldn t interpret it. the only feedback you have is your validation metrics like an altitude meter on your invisible rocket. it s particularly important to be able to tell whether you re getting off the ground at all. what was the altitude you started at? your model seems to have an accuracy of 15% is that any good? before you start working with a dataset you should always pick a trivial baseline that you ll try to beat. if you cross that threshold you ll know you re doing something right your model is actually using the information in the input data to make predictions that generalize and you can keep going. this baseline could be evaluating machine learning models the performance of a random classifier or the performance of the simplest non-machine learning technique you can imagine. for instance in the mnist digit-classification example a simple baseline would be a validation accuracy greater than 0.1 (random classifier) in the imdb example it would be a validation accuracy greater than 0.5. in the reuters example it would be around 0.18-0.19 due to class imbalance. if you have a binary classification problem where 90% of samples belong to class a and 10% belong to class b then a classifier that always predicts a already achieves 0.9 in validation accuracy and you ll need to do better than that. having a common-sense baseline you can refer to is essential when you re getting started on a problem no one has solved before. if you can t beat a trivial solution your model is worthless perhaps you re using the wrong model or perhaps the problem you re tackling can t even be approached with machine learning in the first place. time to go back to the drawing board. 5.2.3 things to keep in mind about model evaluation keep an eye out for the following when you re choosing an evaluation protocol data representativeness you want both your training set and test set to be rep.resentative of the data at hand. for instance if you re trying to classify images of digits and you re starting from an array of samples where the samples are ordered by their class taking the first 80% of the array as your training set and the remaining 20% as your test set will result in your training set contain.ing only classes 0 7 whereas your test set will contain only classes 8 9. this seems like a ridiculous mistake but it s surprisingly common. for this reason you usually should randomly shuffle your data before splitting it into training and test sets. the arrow of time if you re trying to predict the future given the past (for exam.ple tomorrow s weather stock movements and so on) you should not ran.domly shuffle your data before splitting it because doing so will create a temporal leak your model will effectively be trained on data from the future. in such situations you should always make sure all data in your test set is posterior to the data in the training set. redundancy in your data if some data points in your data appear twice (fairly common with real-world data) then shuffling the data and splitting it into a training set and a validation set will result in redundancy between the training and validation sets. in effect you ll be testing on part of your training data which is the worst thing you can do! make sure your training set and validation set are disjoint. having a reliable way to evaluate the performance of your model is how you ll be able to monitor the tension at the heart of machine learning between optimization and generalization underfitting and overfitting. 5.3 improving model fit to achieve the perfect fit you must first overfit. since you don t know in advance where the boundary lies you must cross it to find it. thus your initial goal as you start working on a problem is to achieve a model that shows some generalization power and that is able to overfit. once you have such a model you ll focus on refining gener.alization by fighting overfitting. there are three common problems you ll encounter at this stage training doesn t get started your training loss doesn t go down over time. training gets started just fine but your model doesn t meaningfully generalize you can t beat the common-sense baseline you set. training and validation loss both go down over time and you can beat your baseline but you don t seem to be able to overfit which indicates you re still underfitting. let s see how you can address these issues to achieve the first big milestone of a machine learning project getting a model that has some generalization power (it can beat a trivial baseline) and that is able to overfit. 5.3.1 tuning key gradient descent parameters sometimes training doesn t get started or it stalls too early. your loss is stuck. this is always something you can overcome remember that you can fit a model to random data. even if nothing about your problem makes sense you should still be able to train something if only by memorizing the training data. when this happens it s always a problem with the configuration of the gradient descent process your choice of optimizer the distribution of initial values in the weights of your model your learning rate or your batch size. all these parameters are interdependent and as such it is usually sufficient to tune the learning rate and the batch size while keeping the rest of the parameters constant. let s look at a concrete example let s train the mnist model from chapter 2 with an inappropriately large learning rate of value 1. (train_images train_labels) _ = mnist.load_data() train_images = train_images.reshape((60000 28 * 28)) train_images = train_images.astype("float32") / 255 model = keras.sequential([ layers.dense(512 activation="relu") layers.dense(10 activation="softmax") ]) model.compile(optimizer=keras.optimizers.rmsprop(1.) loss="sparse_categorical_crossentropy" metrics=["accuracy"]) model.fit(train_images train_labels epochs=10 batch_size=128 validation_split=0.2) the model quickly reaches a training and validation accuracy in the 30% 40% range but cannot get past that. let s try to lower the learning rate to a more reasonable value of 1e-2. model = keras.sequential([ layers.dense(512 activation="relu") layers.dense(10 activation="softmax") ]) model.compile(optimizer=keras.optimizers.rmsprop(1e-2) loss="sparse_categorical_crossentropy" metrics=["accuracy"]) model.fit(train_images train_labels epochs=10 batch_size=128 validation_split=0.2) the model is now able to train. if you find yourself in a similar situation try lowering or increasing the learning rate. a learning rate that is too high may lead to updates that vastly overshoot a proper fit like in the preceding example and a learning rate that is too low may make training so slow that it appears to stall. increasing the batch size. a batch with more samples will lead to gradients that are more informative and less noisy (lower variance). you will eventually find a configuration that gets training started. 5.3.2 leveraging better architecture priors you have a model that fits but for some reason your validation metrics aren t improv.ing at all. they remain no better than what a random classifier would achieve your model trains but doesn t generalize. what s going on? this is perhaps the worst machine learning situation you can find yourself in. it indicates that something is fundamentally wrong with your approach and it may not be easy to tell what. here are some tips. first it may be that the input data you re using simply doesn t contain sufficient information to predict your targets the problem as formulated is not solvable. this is what happened earlier when we tried to fit an mnist model where the labels were shuffled the model would train just fine but validation accuracy would stay stuck at 10% because it was plainly impossible to generalize with such a dataset. it may also be that the kind of model you re using is not suited for the problem at hand. for instance in chapter 10 you ll see an example of a timeseries prediction problem where a densely connected architecture isn t able to beat a trivial baseline whereas a more appropriate recurrent architecture does manage to generalize well. using a model that makes the right assumptions about the problem is essential to achieve generalization you should leverage the right architecture priors. in the following chapters you ll learn about the best architectures to use for a vari.ety of data modalities images text timeseries and so on. in general you should always make sure to read up on architecture best practices for the kind of task you re attacking chances are you re not the first person to attempt it. 5.3.3 increasing model capacity if you manage to get to a model that fits where validation metrics are going down and that seems to achieve at least some level of generalization power congratulations you re almost there. next you need to get your model to start overfitting. consider the following small model a simple logistic regression trained on mnist pixels. model = keras.sequential([layers.dense(10 activation="softmax")]) model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) history_small_model = model.fit( train_images train_labels epochs=20 batch_size=128 validation_split=0.2) you get loss curves that look like figure 5.14 import matplotlib.pyplot as plt val_loss = history_small_model.history["val_loss"] epochs = range(1 21) plt.plot(epochs val_loss "b--" label="validation loss") plt.title("effect of insufficient model capacity on validation loss") plt.xlabel("epochs") plt.ylabel("loss") plt.legend() validation metrics seem to stall or to improve very slowly instead of peaking and reversing course. the validation loss goes to 0.26 and just stays there. you can fit but you can t clearly overfit even after many iterations over the training data. you re likely to encounter similar curves often in your career. remember that it should always be possible to overfit. much like the problem where the training loss doesn t go down this is an issue that can always be solved. if you can t seem to be able to overfit it s likely a problem with the representational power of your model you re going to need a bigger model one with more capacity that is to say one able to store more information. you can increase representational power by adding more layers using bigger layers (layers with more parameters) or using kinds of layers that are more appropriate for the problem at hand (better architecture priors). let s try training a bigger model one with two intermediate layers with 96 units each model = keras.sequential([ layers.dense(96 activation="relu") layers.dense(96 activation="relu") layers.dense(10 activation="softmax") ]) model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) history_large_model = model.fit( train_images train_labels epochs=20 batch_size=128 validation_split=0.2) the validation curve now looks exactly like it should the model fits fast and starts overfitting after 8 epochs (see figure 5.15). 5.4 improving generalization once your model has shown itself to have some generalization power and to be able to overfit it s time to switch your focus to maximizing generalization. 5.4.1 dataset curation you ve already learned that generalization in deep learning originates from the latent structure of your data. if your data makes it possible to smoothly interpolate between samples you will be able to train a deep learning model that generalizes. if your prob.lem is overly noisy or fundamentally discrete like say list sorting deep learning will not help you. deep learning is curve fitting not magic. as such it is essential that you make sure that you re working with an appropriate dataset. spending more effort and money on data collection almost always yields a much greater return on investment than spending the same on developing a better model. make sure you have enough data. remember that you need a dense sampling of the input-cross-output space. more data will yield a better model. sometimes problems that seem impossible at first become solvable with a larger dataset. minimize labeling errors visualize your inputs to check for anomalies and proofread your labels. clean your data and deal with missing values (we ll cover this in the next chapter). if you have many features and you aren t sure which ones are actually useful do feature selection. a particularly important way to improve the generalization potential of your data is feature engineering. for most machine learning problems feature engineering is a key ingredient for success. let s take a look. improving generalization 5.4.2 feature engineering feature engineering is the process of using your own knowledge about the data and about the machine learning algorithm at hand (in this case a neural network) to make the algorithm work better by applying hardcoded (non-learned) transformations to the data before it goes into the model. in many cases it isn t reasonable to expect a machine learning model to be able to learn from completely arbitrary data. the data needs to be presented to the model in a way that will make the model s job easier. let s look at an intuitive example. suppose you re trying to develop a model that can take as input an image of a clock and can output the time of day (see figure 5.16). raw data pixel grid better {x1 0.7 {x1 0.0 features y1 0.7} y2 1.0} clock hands {x2 0.5 {x2 -0.38 coordinates y2 0.0} y2 0.32} even better theta1 45 theta1 90 features theta2 0 theta2 140 figure 5.16 feature engineering angles of clock hands for reading the time on a clock if you choose to use the raw pixels of the image as input data you have a difficult machine learning problem on your hands. you ll need a convolutional neural net.work to solve it and you ll have to expend quite a bit of computational resources to train the network. but if you already understand the problem at a high level (you understand how humans read time on a clock face) you can come up with much better input features for a machine learning algorithm for instance it s easy to write a five-line python script to follow the black pixels of the clock hands and output the (x y) coordinates of the tip of each hand. then a simple machine learning algorithm can learn to asso.ciate these coordinates with the appropriate time of day. you can go even further do a coordinate change and express the (x y) coordi.nates as polar coordinates with regard to the center of the image. your input will become the angle theta of each clock hand. at this point your features are making the problem so easy that no machine learning is required a simple rounding opera.tion and dictionary lookup are enough to recover the approximate time of day. that s the essence of feature engineering making a problem easier by expressing it in a simpler way. make the latent manifold smoother simpler better organized. doing so usually requires understanding the problem in depth. before deep learning feature engineering used to be the most important part of the machine learning workflow because classical shallow algorithms didn t have hypothesis spaces rich enough to learn useful features by themselves. the way you pre.sented the data to the algorithm was absolutely critical to its success. for instance before convolutional neural networks became successful on the mnist digit-classifi.cation problem solutions were typically based on hardcoded features such as the number of loops in a digit image the height of each digit in an image a histogram of pixel values and so on. fortunately modern deep learning removes the need for most feature engineer.ing because neural networks are capable of automatically extracting useful features from raw data. does this mean you don t have to worry about feature engineering as long as you re using deep neural networks? no for two reasons good features still allow you to solve problems more elegantly while using fewer resources. for instance it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network. good features let you solve a problem with far less data. the ability of deep learning models to learn features on their own relies on having lots of training data available if you have only a few samples the information value in their fea.tures becomes critical. 5.4.3 using early stopping in deep learning we always use models that are vastly overparameterized they have way more degrees of freedom than the minimum necessary to fit to the latent mani.fold of the data. this overparameterization is not an issue because you never fully fit a deep learning model. such a fit wouldn t generalize at all. you will always interrupt train.ing long before you ve reached the minimum possible training loss. finding the exact point during training where you ve reached the most generaliz. able fit the exact boundary between an underfit curve and an overfit curve is one of the most effective things you can do to improve generalization. in the examples in the previous chapter we would start by training our models for longer than needed to figure out the number of epochs that yielded the best valida.tion metrics and then we would retrain a new model for exactly that number of epochs. this is pretty standard but it requires you to do redundant work which can sometimes be expensive. naturally you could just save your model at the end of each epoch and once you ve found the best epoch reuse the closest saved model you have. in keras it s typical to do this with an earlystopping callback which will interrupt training as soon as validation metrics have stopped improving while remembering the best known model state. you ll learn to use callbacks in chapter 7. improving generalization 5.4.4 regularizing your model regularization techniques are a set of best practices that actively impede the model s abil.ity to fit perfectly to the training data with the goal of making the model perform bet.ter during validation. this is called regularizing the model because it tends to make the model simpler more regular its curve smoother more generic thus it is less specific to the training set and better able to generalize by more closely approximat.ing the latent manifold of the data. keep in mind that regularizing a model is a process that should always be guided by an accurate evaluation procedure. you will only achieve generalization if you can measure it. let s review some of the most common regularization techniques and apply them in practice to improve the movie-classification model from chapter 4. reducing the network s size you ve already learned that a model that is too small will not overfit. the simplest way to mitigate overfitting is to reduce the size of the model (the number of learnable parameters in the model determined by the number of layers and the number of units per layer). if the model has limited memorization resources it won t be able to simply memorize its training data thus in order to minimize its loss it will have to resort to learning compressed representations that have predictive power regarding the targets precisely the type of representations we re interested in. at the same time keep in mind that you should use models that have enough parameters that they don t underfit your model shouldn t be starved for memorization resources. there is a compromise to be found between too much capacity and not enough capacity. unfortunately there is no magical formula to determine the right number of layers or the right size for each layer. you must evaluate an array of different architectures (on your validation set not on your test set of course) in order to find the correct model size for your data. the general workflow for finding an appropriate model size is to start with relatively few layers and parameters and increase the size of the layers or add new layers until you see diminishing returns with regard to validation loss. let s try this on the movie-review classification model. the following listing shows our original model. from tensorflow.keras.datasets import imdb (train_data train_labels) _ = imdb.load_data(num_words=10000) def vectorize_sequences(sequences dimension=10000) results = np.zeros((len(sequences) dimension)) for i sequence in enumerate(sequences) results[i sequence] = 1. return results train_data = vectorize_sequences(train_data) model = keras.sequential([ layers.dense(16 activation="relu") layers.dense(16 activation="relu") layers.dense(1 activation="sigmoid") ]) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) history_original = model.fit(train_data train_labels epochs=20 batch_size=512 validation_split=0.4) now let s try to replace it with this smaller model. model = keras.sequential([ layers.dense(4 activation="relu") layers.dense(4 activation="relu") layers.dense(1 activation="sigmoid") ]) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) history_smaller_model = model.fit( train_data train_labels epochs=20 batch_size=512 validation_split=0.4) figure 5.17 shows a comparison of the validation losses of the original model and the smaller model. improving generalization as you can see the smaller model starts overfitting later than the reference model (after six epochs rather than four) and its performance degrades more slowly once it starts overfitting. now let s add to our benchmark a model that has much more capacity far more than the problem warrants. while it is standard to work with models that are signifi.cantly overparameterized for what they re trying to learn there can definitely be such a thing as too much memorization capacity. you ll know your model is too large if it starts overfitting right away and if its validation loss curve looks choppy with high-variance (although choppy validation metrics could also be a symptom of using an unreliable validation process such as a validation split that s too small). model = keras.sequential([ layers.dense(512 activation="relu") layers.dense(512 activation="relu") layers.dense(1 activation="sigmoid") ]) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) history_larger_model = model.fit( train_data train_labels epochs=20 batch_size=512 validation_split=0.4) figure 5.18 shows how the bigger model fares compared with the reference model. the bigger model starts overfitting almost immediately after just one epoch and it overfits much more severely. its validation loss is also noisier. it gets training loss near zero very quickly. the more capacity the model has the more quickly it can model the training data (resulting in a low training loss) but the more susceptible it is to overfit.ting (resulting in a large difference between the training and validation loss). adding weight regularization you may be familiar with the principle of occam s razor given two explanations for something the explanation most likely to be correct is the simplest one the one that makes fewer assumptions. this idea also applies to the models learned by neural net.works given some training data and a network architecture multiple sets of weight values (multiple models) could explain the data. simpler models are less likely to over-fit than complex ones. a simple model in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters as you saw in the previous sec.tion). thus a common way to mitigate overfitting is to put constraints on the com.plexity of a model by forcing its weights to take only small values which makes the distribution of weight values more regular. this is called weight regularization and it s done by adding to the loss function of the model a cost associated with having large weights. this cost comes in two flavors l1 regularization the cost added is proportional to the absolute value of the weight coefficients (the l1 norm of the weights). l2 regularization the cost added is proportional to the square of the value of the weight coefficients (the l2 norm of the weights). l2 regularization is also called weight decay in the context of neural networks. don t let the different name con.fuse you weight decay is mathematically the same as l2 regularization. in keras weight regularization is added by passing weight regularizer instances to layers as keyword arguments. let s add l2 weight regularization to our initial movie-review classification model. from tensorflow.keras import regularizers model = keras.sequential([ layers.dense(16 kernel_regularizer=regularizers.l2(0.002) activation="relu") layers.dense(16 kernel_regularizer=regularizers.l2(0.002) activation="relu") layers.dense(1 activation="sigmoid") ]) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) improving generalization history_l2_reg = model.fit( train_data train_labels epochs=20 batch_size=512 validation_split=0.4) in the preceding listing l2(0.002) means every coefficient in the weight matrix of the layer will add 0.002 * weight_coefficient_value ** 2 to the total loss of the model. note that because this penalty is only added at training time the loss for this model will be much higher at training than at test time. figure 5.19 shows the impact of the l2 regularization penalty. as you can see the model with l2 regularization has become much more resistant to overfitting than the reference model even though both models have the same number of parameters. as an alternative to l2 regularization you can use one of the following keras weight regularizers. from tensorflow.keras import regularizers regularizers.l1(0.001) l1 regularization regularizers.l1_l2(l1=0.001l2=0.001) simultaneous l1 and l2 regularization note that weight regularization is more typically used for smaller deep learning mod.els. large deep learning models tend to be so overparameterized that imposing con.straints on weight values hasn t much impact on model capacity and generalization. in these cases a different regularization technique is preferred dropout. adding dropout dropout is one of the most effective and most commonly used regularization tech.niques for neural networks it was developed by geoff hinton and his students at the university of toronto. dropout applied to a layer consists of randomly dropping out (setting to zero) a number of output features of the layer during training. let s say a given layer would normally return a vector [0.2 0.5 1.3 0.8 1.1] for a given input sample during training. after applying dropout this vector will have a few zero entries distributed at random for example [0 0.5 1.3 0 1.1]. the dropout rate is the fraction of the features that are zeroed out it s usually set between 0.2 and 0.5. at test time no units are dropped out instead the layer s output values are scaled down by a factor equal to the dropout rate to balance for the fact that more units are active than at training time. consider a numpy matrix containing the output of a layer layer_output of shape (batch_size features). at training time we zero out at random a fraction of the values in the matrix layer_output *= np.random.randint(0high=2size=layer_output.shape) at training time drops out 50% of the units in the output at test time we scale down the output by the dropout rate. here we scale by 0.5 (because we previously dropped half the units) layer_output *= 0.5 at test time note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time which is often the way it s imple.mented in practice (see figure 5.20) at training time layer_output *= np.random.randint(0high=2size=layer_output.shape) layer_output /= 0.5 note that we re scaling up rather than scaling down in this case. 0.0 0.2 1.5 0.0 0.6 0.1 0.0 0.3 0.0 1.9 0.3 0.0 0.7 0.0 0.0 0.0 figure 5.20 dropout applied to * 2 an activation matrix at training time with rescaling happening during training. at test time the activation matrix is unchanged. this technique may seem strange and arbitrary. why would this help reduce overfit.ting? hinton says he was inspired by among other things a fraud-prevention mecha.nism used by banks. in his own words i went to my bank. the tellers kept changing and i asked one of them why. he said he didn t know but they got moved around a lot. improving generalization i figured it must be because it would require cooperation between employees to suc.cessfully defraud the bank. this made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce over-fitting. the core idea is that introducing noise in the output values of a layer can break up happenstance patterns that aren t significant (what hinton refers to as con.spiracies) which the model will start memorizing if no noise is present. in keras you can introduce dropout in a model via the dropout layer which is applied to the output of the layer right before it. let s add two dropout layers in the imdb model to see how well they do at reducing overfitting. model = keras.sequential([ layers.dense(16 activation="relu") layers.dropout(0.5) layers.dense(16 activation="relu") layers.dropout(0.5) layers.dense(1 activation="sigmoid") ]) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) history_dropout = model.fit( train_data train_labels epochs=20 batch_size=512 validation_split=0.4) figure 5.21 shows a plot of the results. this is a clear improvement over the reference model it also seems to be working much better than l2 regularization since the low.est validation loss reached has improved. to recap these are the most common ways to maximize generalization and prevent overfitting in neural networks get more training data or better training data. develop better features. reduce the capacity of the model. add weight regularization (for smaller models). add dropout. summary the purpose of a machine learning model is to generalize to perform accurately on never-before-seen inputs. it s harder than it seems. a deep neural network achieves generalization by learning a parametric model that can successfully interpolate between training samples such a model can be said to have learned the latent manifold of the training data. this is why deep learning models can only make sense of inputs that are very close to what they ve seen during training. the fundamental problem in machine learning is the tension between optimization and generalization to attain generalization you must first achieve a good fit to the training data but improving your model s fit to the training data will inevi.tably start hurting generalization after a while. every single deep learning best practice deals with managing this tension. the ability of deep learning models to generalize comes from the fact that they manage to learn to approximate the latent manifold of their data and can thus make sense of new inputs via interpolation. it s essential to be able to accurately evaluate the generalization power of your model while you re developing it. you have at your disposal an array of evalua.tion methods from simple holdout validation to k-fold cross-validation and iterated k-fold cross-validation with shuffling. remember to always keep a com.pletely separate test set for final model evaluation since information leaks from your validation data to your model may have occurred. when you start working on a model your goal is first to achieve a model that has some generalization power and that can overfit. best practices for doing this include tuning your learning rate and batch size leveraging better architec.ture priors increasing model capacity or simply training longer. as your model starts overfitting your goal switches to improving generalization through model regularization. you can reduce your model s capacity add dropout or weight regularization and use early stopping. and naturally a larger or bet.ter dataset is always the number one way to help a model generalize. the universal workflow of machine learning this chapter covers steps for framing a machine learning problem steps for developing a working model steps for deploying your model in production and maintaining it our previous examples have assumed that we already had a labeled dataset to start from and that we could immediately start training a model. in the real world this is often not the case. you don t start from a dataset you start from a problem. imagine that you re starting your own machine learning consulting shop. you incorporate you put up a fancy website you notify your network. the projects start rolling in a personalized photo search engine for a picture-sharing social network type in wedding and retrieve all the pictures you took at weddings without any manual tagging needed. flagging spam and offensive text content among the posts of a budding chat app. building a music recommendation system for users of an online radio. detecting credit card fraud for an e-commerce website. 153 predicting display ad click-through rate to decide which ad to serve to a given user at a given time. flagging anomalous cookies on the conveyor belt of a cookie-manufacturing line. using satellite images to predict the location of as-yet unknown archeological sites. note on ethics you may sometimes be offered ethically dubious projects such as building an ai that rates the trustworthiness of someone from a picture of their face. first of all the validity of the project is in doubt it isn t clear why trustworthiness would be reflected on someone s face. second such a task opens the door to all kinds of eth.ical problems. collecting a dataset for this task would amount to recording the biases and prejudices of the people who label the pictures. the models you would train on such data would merely encode these same biases into a black-box algorithm that would give them a thin veneer of legitimacy. in a largely tech-illiterate society like ours the ai algorithm said this person cannot be trusted strangely appears to carry more weight and objectivity than john smith said this person cannot be trusted despite the former being a learned approximation of the latter. your model would be laundering and operationalizing at scale the worst aspects of human judgement with negative effects on the lives of real people. technology is never neutral. if your work has any impact on the world this impact has a moral direction technical choices are also ethical choices. always be deliberate about the values you want your work to support. it would be very convenient if you could import the correct dataset from keras.data.sets and start fitting some deep learning models. unfortunately in the real world you ll have to start from scratch. in this chapter you ll learn about a universal step-by-step blueprint that you can use to approach and solve any machine learning problem like those in the previous list. this template will bring together and consolidate everything you ve learned in chapters 4 and 5 and will give you the wider context that should anchor what you ll learn in the next chapters. the universal workflow of machine learning is broadly structured in three parts 1 define the task understand the problem domain and the business logic under.lying what the customer asked for. collect a dataset understand what the data represents and choose how you will measure success on the task. 2 develop a model prepare your data so that it can be processed by a machine learning model select a model evaluation protocol and a simple baseline to beat train a first model that has generalization power and that can overfit and then regularize and tune your model until you achieve the best possible gener.alization performance. 3 deploy the model present your work to stakeholders ship the model to a web server a mobile app a web page or an embedded device monitor the model s performance in the wild and start collecting the data you ll need to build the next-generation model. let s dive in. 6.1 define the task you can t do good work without a deep understanding of the context of what you re doing. why is your customer trying to solve this particular problem? what value will they derive from the solution how will your model be used and how will it fit into your customer s business processes? what kind of data is available or could be col.lected? what kind of machine learning task can be mapped to the business problem? 6.1.1 frame the problem framing a machine learning problem usually involves many detailed discussions with stakeholders. here are the questions that should be on the top of your mind what will your input data be? what are you trying to predict? you can only learn to predict something if you have training data available for example you can only learn to classify the sentiment of movie reviews if you have both movie reviews and sentiment annotations available. as such data availability is usually the limiting factor at this stage. in many cases you will have to resort to collecting and annotating new datasets yourself (which we ll cover in the next section). what type of machine learning task are you facing? is it binary classification? multiclass classification? scalar regression? vector regression? multiclass multi-label classification? image segmentation? ranking? something else like cluster.ing generation or reinforcement learning? in some cases it may be that machine learning isn t even the best way to make sense of the data and you should use something else such as plain old-school statistical analysis. the photo search engine project is a multiclass multilabel classification task. the spam detection project is a binary classification task. if you set offensive content as a separate class it s a three-way classification task. the music recommendation engine turns out to be better handled not via deep learning but via matrix factorization (collaborative filtering). the credit card fraud detection project is a binary classification task. the click-through-rate prediction project is a scalar regression task. anomalous cookie detection is a binary classification task but it will also require an object detection model as a first stage in order to correctly crop out the cookies in raw images. note that the set of machine learning tech.niques known as anomaly detection would not be a good fit in this setting! the project for finding new archeological sites from satellite images is an image-similarity ranking task you need to retrieve new images that look the most like known archeological sites. what do existing solutions look like? perhaps your customer already has a handcrafted algorithm that handles spam filtering or credit card fraud detec.tion with lots of nested if statements. perhaps a human is currently in charge of manually handling the process under consideration monitoring the con.veyor belt at the cookie plant and manually removing the bad cookies or craft.ing playlists of song recommendations to be sent out to users who liked a specific artist. you should make sure you understand what systems are already in place and how they work. are there particular constraints you will need to deal with? for example you could find out that the app for which you re building a spam detection system is strictly end-to-end encrypted so that the spam detection model will have to live on the end user s phone and must be trained on an external dataset. perhaps the cookie-filtering model has such latency constraints that it will need to run on an embedded device at the factory rather than on a remote server. you should understand the full context in which your work will fit. once you ve done your research you should know what your inputs will be what your targets will be and what broad type of machine learning task the problem maps to. be aware of the hypotheses you re making at this stage you hypothesize that your targets can be predicted given your inputs. you hypothesize that the data that s available (or that you will soon collect) is sufficiently informative to learn the relationship between inputs and targets. until you have a working model these are merely hypotheses waiting to be validated or invalidated. not all problems can be solved with machine learning just because you ve assembled examples of inputs x and targets y doesn t mean x contains enough information to predict y. for instance if you re trying to predict the movements of a stock on the stock market given its recent price history you re unlikely to succeed because price history doesn t contain much predictive information. 6.1.2 collect a dataset once you understand the nature of the task and you know what your inputs and tar.gets are going to be it s time for data collection the most arduous time-consuming and costly part of most machine learning projects. the photo search engine project requires you to first select the set of labels you want to classify you settle on 10000 common image categories. then you need to manually tag hundreds of thousands of your past user-uploaded images with labels from this set. for the chat app s spam detection project because user chats are end-to-end encrypted you cannot use their contents for training a model. you need to gain access to a separate dataset of tens of thousands of unfiltered social media posts and manually tag them as spam offensive or acceptable. for the music recommendation engine you can just use the likes of your users. no new data needs to be collected. likewise for the click-through-rate prediction project you have an extensive record of click-through rate for your past ads going back years. for the cookie-flagging model you will need to install cameras above the con.veyor belts to collect tens of thousands of images and then someone will need to manually label these images. the people who know how to do this currently work at the cookie factory but it doesn t seem too difficult. you should be able to train people to do it. the satellite imagery project will require a team of archeologists to collect a database of existing sites of interest and for each site you will need to find exist.ing satellite images taken in different weather conditions. to get a good model you re going to need thousands of different sites. you learned in chapter 5 that a model s ability to generalize comes almost entirely from the properties of the data it is trained on the number of data points you have the reliability of your labels the quality of your features. a good dataset is an asset wor.thy of care and investment. if you get an extra 50 hours to spend on a project chances are that the most effective way to allocate them is to collect more data rather than search for incremental modeling improvements. the point that data matters more than algorithms was most famously made in a 2009 paper by google researchers titled the unreasonable effectiveness of data (the title is a riff on the well-known 1960 article the unreasonable effectiveness of mathematics in the natural sciences by eugene wigner). this was before deep learn.ing was popular but remarkably the rise of deep learning has only made the impor.tance of data greater. if you re doing supervised learning then once you ve collected inputs (such as images) you re going to need annotations for them (such as tags for those images) the targets you will train your model to predict. sometimes annotations can be retrieved automatically such as those for the music recommendation task or the click-through.rate prediction task. but often you have to annotate your data by hand. this is a labor-heavy process. investing in data annotation infrastructure your data annotation process will determine the quality of your targets which in turn determine the quality of your model. carefully consider the options you have available should you annotate the data yourself? should you use a crowdsourcing platform like mechanical turk to collect labels? should you use the services of a specialized data-labeling company? outsourcing can potentially save you time and money but it takes away control. using something like mechanical turk is likely to be inexpensive and to scale well but your annotations may end up being quite noisy. to pick the best option consider the constraints you re working with do the data labelers need to be subject matter experts or could anyone anno.tate the data? the labels for a cat-versus-dog image classification problem can be selected by anyone but those for a dog breed classification task require spe.cialized knowledge. meanwhile annotating ct scans of bone fractures pretty much requires a medical degree. if annotating the data requires specialized knowledge can you train people to do it? if not how can you get access to relevant experts? do you yourself understand the way experts come up with the annotations? if you don t you will have to treat your dataset as a black box and you won t be able to perform manual feature engineering this isn t critical but it can be limiting. if you decide to label your data in-house ask yourself what software you will use to record annotations. you may well need to develop that software yourself. productive data anno.tation software will save you a lot of time so it s worth investing in it early in a project. beware of non-representative data machine learning models can only make sense of inputs that are similar to what they ve seen before. as such it s critical that the data used for training should be repre.sentative of the production data. this concern should be the foundation of all your data collection work. suppose you re developing an app where users can take pictures of a plate of food to find out the name of the dish. you train a model using pictures from an image-sharing social network that s popular with foodies. come deployment time feedback from angry users starts rolling in your app gets the answer wrong 8 times out of 10. what s going on? your accuracy on the test set was well over 90%! a quick look at user-uploaded data reveals that mobile picture uploads of random dishes from random restaurants taken with random smartphones look nothing like the professional-quality well-lit appetizing pictures you trained the model on your training data wasn t representative of the production data. that s a cardinal sin welcome to machine learning hell. if possible collect data directly from the environment where your model will be used. a movie review sentiment classification model should be used on new imdb reviews not on yelp restaurant reviews nor on twitter status updates. if you want to rate the sentiment of a tweet start by collecting and annotating actual tweets from a similar set of users as those you re expecting in production. if it s not possible to train on production data then make sure you fully understand how your training and pro.duction data differ and that you are actively correcting for these differences. a related phenomenon you should be aware of is concept drift. you ll encounter concept drift in almost all real-world problems especially those that deal with user-generated data. concept drift occurs when the properties of the production data change over time causing model accuracy to gradually decay. a music recommenda.tion engine trained in the year 2013 may not be very effective today. likewise the imdb dataset you worked with was collected in 2011 and a model trained on it would likely not perform as well on reviews from 2020 compared to reviews from 2012 as vocabulary expressions and movie genres evolve over time. concept drift is particu.larly acute in adversarial contexts like credit card fraud detection where fraud pat.terns change practically every day. dealing with fast concept drift requires constant data collection annotation and model retraining. keep in mind that machine learning can only be used to memorize patterns that are present in your training data. you can only recognize what you ve seen before. using machine learning trained on past data to predict the future is making the assumption that the future will behave like the past. that often isn t the case. the problem of sampling bias a particularly insidious and common case of non-representative data is sampling bias. sampling bias occurs when your data collection process interacts with what you are trying to predict resulting in biased measurements. a famous historical example occurred in the 1948 us presidential election. on election night the chi.cago tribune printed the headline dewey defeats truman. the next morning truman emerged as the winner. the editor of the tribune had trusted the results of a phone survey but phone users in 1948 were not a random representative sam.ple of the voting population. they were more likely to be richer conservative and to vote for dewey the republican candidate. dewey defeats truman a famous example of sampling bias nowadays every phone survey takes sampling bias into account. that doesn t mean that sampling bias is a thing of the past in political polling far from it. but unlike in 1948 pollsters are aware of it and take steps to correct it. 6.1.3 understand your data it s pretty bad practice to treat a dataset as a black box. before you start training mod.els you should explore and visualize your data to gain insights about what makes it predictive which will inform feature engineering and screen for potential issues. if your data includes images or natural language text take a look at a few sam.ples (and their labels) directly. if your data contains numerical features it s a good idea to plot the histogram of feature values to get a feel for the range of values taken and the frequency of different values. if your data includes location information plot it on a map. do any clear pat.terns emerge? are some samples missing values for some features? if so you ll need to deal with this when you prepare the data (we ll cover how to do this in the next section). if your task is a classification problem print the number of instances of each class in your data. are the classes roughly equally represented? if not you will need to account for this imbalance. check for target leaking the presence of features in your data that provide infor.mation about the targets and which may not be available in production. if you re training a model on medical records to predict whether someone will be treated for cancer in the future and the records include the feature this per.son has been diagnosed with cancer then your targets are being artificially leaked into your data. always ask yourself is every feature in your data some.thing that will be available in the same form in production? 6.1.4 choose a measure of success to control something you need to be able to observe it. to achieve success on a proj.ect you must first define what you mean by success. accuracy? precision and recall? customer retention rate? your metric for success will guide all of the technical choices you make throughout the project. it should directly align with your higher-level goals such as the business success of your customer. for balanced classification problems where every class is equally likely accuracy and the area under a receiver operating characteristic (roc) curve abbreviated as roc auc are common metrics. for class-imbalanced problems ranking problems or multilabel classification you can use precision and recall as well as a weighted form of accuracy or roc auc. and it isn t uncommon to have to define your own custom metric by which to measure success. to get a sense of the diversity of machine learning success metrics and how they relate to different problem domains it s helpful to browse the data science competitions on kaggle (https//kaggle.com) they showcase a wide range of problems and evaluation metrics. 6.2 develop a model once you know how you will measure your progress you can get started with model development. most tutorials and research projects assume that this is the only step skipping problem definition and dataset collection which are assumed already done and skipping model deployment and maintenance which are assumed to be handled by someone else. in fact model development is only one step in the machine learning workflow and if you ask me it s not the most difficult one. the hardest things in machine learning are framing problems and collecting annotating and cleaning data. so cheer up what comes next will be easy in comparison! 6.2.1 prepare the data as you ve learned before deep learning models typically don t ingest raw data. data preprocessing aims at making the raw data at hand more amenable to neural net.works. this includes vectorization normalization or handling missing values. many preprocessing techniques are domain-specific (for example specific to text data or image data) we ll cover those in the following chapters as we encounter them in practical examples. for now we ll review the basics that are common to all data domains. vectorization all inputs and targets in a neural network must typically be tensors of floating-point data (or in specific cases tensors of integers or strings). whatever data you need to process sound images text you must first turn into tensors a step called data vec.torization. for instance in the two previous text-classification examples in chapter 4 we started with text represented as lists of integers (standing for sequences of words) and we used one-hot encoding to turn them into a tensor of float32 data. in the examples of classifying digits and predicting house prices the data came in vectorized form so we were able to skip this step. value normalization in the mnist digit-classification example from chapter 2 we started with image data encoded as integers in the 0 255 range encoding grayscale values. before we fed this data into our network we had to cast it to float32 and divide by 255 so we d end up with floating-point values in the 0 1 range. similarly when predicting house prices we started with features that took a variety of ranges some features had small floating-point values and others had fairly large integer values. before we fed this data into our network we had to normalize each feature independently so that it had a stan.dard deviation of 1 and a mean of 0. in general it isn t safe to feed into a neural network data that takes relatively large values (for example multi-digit integers which are much larger than the ini.tial values taken by the weights of a network) or data that is heterogeneous (for example data where one feature is in the range 0 1 and another is in the range 100 200). doing so can trigger large gradient updates that will prevent the network from converging. to make learning easier for your network your data should have the following characteristics take small values typically most values should be in the 0 1 range. be homogenous all features should take values in roughly the same range. additionally the following stricter normalization practice is common and can help although it isn t always necessary (for example we didn t do this in the digit-classifica.tion example) normalize each feature independently to have a mean of 0. normalize each feature independently to have a standard deviation of 1. this is easy to do with numpy arrays x-=x.mean(axis=0) assuming x is a 2d data matrix x/=x.std(axis=0) of shape (samples features) handling missing values you may sometimes have missing values in your data. for instance in the house-price example the first feature (the column of index 0 in the data) was the per capita crime rate. what if this feature wasn t available for all samples? you d then have missing val.ues in the training or test data. you could just discard the feature entirely but you don t necessarily have to. if the feature is categorical it s safe to create a new category that means the value is missing. the model will automatically learn what this implies with respect to the targets. if the feature is numerical avoid inputting an arbitrary value like "0" because it may create a discontinuity in the latent space formed by your features mak.ing it harder for a model trained on it to generalize. instead consider replac.ing the missing value with the average or median value for the feature in the dataset. you could also train a model to predict the feature value given the val.ues of other features. note that if you re expecting missing categorial features in the test data but the network was trained on data without any missing values the network won t have learned to ignore missing values! in this situation you should artificially generate training samples with missing entries copy some training samples several times and drop some of the categorical features that you expect are likely to be missing in the test data. 6.2.2 choose an evaluation protocol as you learned in the previous chapter the purpose of a model is to achieve general.ization and every modeling decision you will make throughout the model develop.ment process will be guided by validation metrics that seek to measure generalization performance. the goal of your validation protocol is to accurately estimate what your success metric of choice (such as accuracy) will be on actual production data. the reli. ability of that process is critical to building a useful model. in chapter 5 we reviewed three common evaluation protocols maintaining a holdout validation set this is the way to go when you have plenty of data. doing k-fold cross-validation this is the right choice when you have too few sam.ples for holdout validation to be reliable. doing iterated k-fold validation this is for performing highly accurate model evaluation when little data is available. pick one of these. in most cases the first will work well enough. as you learned though always be mindful of the representativity of your validation set and be careful not to have redundant samples between your training set and your validation set. 6.2.3 beat a baseline as you start working on the model itself your initial goal is to achieve statistical power as you saw in chapter 5 that is to develop a small model that is capable of beating a simple baseline. at this stage these are the three most important things you should focus on feature engineering filter out uninformative features (feature selection) and use your knowledge of the problem to develop new features that are likely to be useful. selecting the correct architecture priors what type of model architecture will you use? a densely connected network a convnet a recurrent neural network a transformer? is deep learning even a good approach for the task or should you use something else? selecting a good-enough training configuration what loss function should you use? what batch size and learning rate? picking the right loss function it s often not possible to directly optimize for the metric that measures success on a problem. sometimes there is no easy way to turn a metric into a loss function loss functions after all need to be computable given only a mini-batch of data (ideally a loss function should be computable for as little as a single data point) and must be differentiable (otherwise you can t use backpropagation to train your network). for instance the widely used classification metric roc auc can t be directly optimized. hence in classification tasks it s common to optimize for a proxy metric of roc auc such as crossentropy. in general you can hope that the lower the crossentropy gets the higher the roc auc will be. the following table can help you choose a last-layer activation and a loss function for a few common problem types. (continued) choosing the right last-layer activation and loss function for your model problem type last-layer activation loss function binary classification multiclass single-label classification multiclass multilabel classification sigmoid softmax sigmoid binary_crossentropy categorical_crossentropy binary_crossentropy for most problems there are existing templates you can start from. you re not the first person to try to build a spam detector a music recommendation engine or an image classifier. make sure you research prior art to identify the feature engineering techniques and model architectures that are most likely to perform well on your task. note that it s not always possible to achieve statistical power. if you can t beat a sim.ple baseline after trying multiple reasonable architectures it may be that the answer to the question you re asking isn t present in the input data. remember that you re making two hypotheses you hypothesize that your outputs can be predicted given your inputs. you hypothesize that the available data is sufficiently informative to learn the relationship between inputs and outputs. it may well be that these hypotheses are false in which case you must go back to the drawing board. 6.2.4 scale up develop a model that overfits once you ve obtained a model that has statistical power the question becomes is your model sufficiently powerful? does it have enough layers and parameters to properly model the problem at hand? for instance a logistic regression model has statistical power on mnist but wouldn t be sufficient to solve the problem well. remember that the universal tension in machine learning is between optimization and generalization. the ideal model is one that stands right at the border between underfitting and over-fitting between undercapacity and overcapacity. to figure out where this border lies first you must cross it. to figure out how big a model you ll need you must develop a model that overfits. this is fairly easy as you learned in chapter 5 1 add layers. 2 make the layers bigger. 3 train for more epochs. always monitor the training loss and validation loss as well as the training and valida.tion values for any metrics you care about. when you see that the model s perfor.mance on the validation data begins to degrade you ve achieved overfitting. 6.2.5 regularize and tune your model once you ve achieved statistical power and you re able to overfit you know you re on the right path. at this point your goal becomes to maximize generalization performance. this phase will take the most time you ll repeatedly modify your model train it evaluate on your validation data (not the test data at this point) modify it again and repeat until the model is as good as it can get. here are some things you should try try different architectures add or remove layers. add dropout. if your model is small add l1 or l2 regularization. try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration. optionally iterate on data curation or feature engineering collect and anno.tate more data develop better features or remove features that don t seem to be informative. it s possible to automate a large chunk of this work by using automated hyperparameter tuning software such as kerastuner. we ll cover this in chapter 13. be mindful of the following every time you use feedback from your validation pro.cess to tune your model you leak information about the validation process into the model. repeated just a few times this is innocuous done systematically over many iterations it will eventually cause your model to overfit to the validation process (even though no model is directly trained on any of the validation data). this makes the evaluation process less reliable. once you ve developed a satisfactory model configuration you can train your final production model on all the available data (training and validation) and evalu.ate it one last time on the test set. if it turns out that performance on the test set is significantly worse than the performance measured on the validation data this may mean either that your validation procedure wasn t reliable after all or that you began overfitting to the validation data while tuning the parameters of the model. in this case you may want to switch to a more reliable evaluation protocol (such as iterated k-fold validation). 6.3 deploy the model your model has successfully cleared its final evaluation on the test set it s ready to be deployed and to begin its productive life. 6.3.1 explain your work to stakeholders and set expectations success and customer trust are about consistently meeting or exceeding people s expectations. the actual system you deliver is only half of that picture the other half is setting appropriate expectations before launch. the expectations of non-specialists towards ai systems are often unrealistic. for example they might expect that the system understands its task and is capable of exercising human-like common sense in the context of the task. to address this you should consider showing some examples of the failure modes of your model (for instance show what incorrectly classified samples look like especially those for which the misclassification seems surprising). they might also expect human-level performance especially for processes that were previously handled by people. most machine learning models because they are (imper.fectly) trained to approximate human-generated labels do not nearly get there. you should clearly convey model performance expectations. avoid using abstract statements like the model has 98% accuracy (which most people mentally round up to 100%) and prefer talking for instance about false negative rates and false positive rates. you could say with these settings the fraud detection model would have a 5% false nega.tive rate and a 2.5% false positive rate. every day an average of 200 valid transactions would be flagged as fraudulent and sent for manual review and an average of 14 fraudu.lent transactions would be missed. an average of 266 fraudulent transactions would be correctly caught. clearly relate the model s performance metrics to business goals. you should also make sure to discuss with stakeholders the choice of key launch parameters for instance the probability threshold at which a transaction should be flagged (different thresholds will produce different false negative and false positive rates). such decisions involve trade-offs that can only be handled with a deep under.standing of the business context. 6.3.2 ship an inference model a machine learning project doesn t end when you arrive at a colab notebook that can save a trained model. you rarely put in production the exact same python model object that you manipulated during training. first you may want to export your model to something other than python your production environment may not support python at all for instance if it s a mobile app or an embedded system. if the rest of the app isn t in python (it could be in javascript c++ etc.) the use of python to serve a model may induce significant overhead. second since your production model will only be used to output predictions (a phase called inference) rather than for training you have room to perform various optimiza.tions that can make the model faster and reduce its memory footprint. let s take a quick look at the different model deployment options you have available. deploying a model as a rest api this is perhaps the common way to turn a model into a product install tensorflow on a server or cloud instance and query the model s predictions via a rest api. you could build your own serving app using something like flask (or any other python web development library) or use tensorflow s own library for shipping models as apis called tensorflow serving (www.tensorflow.org/tfx/guide/serving). with tensor-flow serving you can deploy a keras model in minutes. you should use this deployment setup when the application that will consume the model s prediction will have reliable access to the internet (obviously). for instance if your application is a mobile app serving predictions from a remote api means that the application won t be usable in airplane mode or in a low-connectivity environment. the application does not have strict latency requirements the request infer.ence and answer round trip will typically take around 500 ms. the input data sent for inference is not highly sensitive the data will need to be available on the server in a decrypted form since it will need to be seen by the model (but note that you should use ssl encryption for the http request and answer). for instance the image search engine project the music recommender system the credit card fraud detection project and the satellite imagery project are all good fits for serving via a rest api. an important question when deploying a model as a rest api is whether you want to host the code on your own or whether you want to use a fully managed third-party cloud service. for instance cloud ai platform a google product lets you simply upload your tensorflow model to google cloud storage (gcs) and it gives you an api endpoint to query it. it takes care of many practical details such as batching pre.dictions load balancing and scaling. deploying a model on a device sometimes you may need your model to live on the same device that runs the applica.tion that uses it maybe a smartphone an embedded arm cpu on a robot or a microcontroller on a tiny device. you may have seen a camera capable of automati.cally detecting people and faces in the scenes you pointed it at that was probably a small deep learning model running directly on the camera. you should use this setup when your model has strict latency constraints or needs to run in a low-connectivity environment. if you re building an immersive augmented reality application querying a remote server is not a viable option. your model can be made sufficiently small that it can run under the memory and power constraints of the target device. you can use the tensorflow model opti.mization toolkit to help with this (www.tensorflow.org/model_optimization). getting the highest possible accuracy isn t mission critical for your task. there is always a trade-off between runtime efficiency and accuracy so memory and power constraints often require you to ship a model that isn t quite as good as the best model you could run on a large gpu. the input data is strictly sensitive and thus shouldn t be decryptable on a remote server. our spam detection model will need to run on the end user s smartphone as part of the chat app because messages are end-to-end encrypted and thus cannot be read by a remotely hosted model. likewise the bad-cookie detection model has strict latency constraints and will need to run at the factory. thankfully in this case we don t have any power or space constraints so we can actually run the model on a gpu. to deploy a keras model on a smartphone or embedded device your go-to solution is tensorflow lite (www.tensorflow.org/lite). it s a framework for efficient on-device deep learning inference that runs on android and ios smartphones as well as arm64.based computers raspberry pi or certain microcontrollers. it includes a converter that can straightforwardly turn your keras model into the tensorflow lite format. deploying a model in the browser deep learning is often used in browser-based or desktop-based javascript applications. while it is usually possible to have the application query a remote model via a rest api there can be key advantages in having the model run directly in the browser on the user s computer (utilizing gpu resources if they re available). use this setup when you want to offload compute to the end user which can dramatically reduce server costs. the input data needs to stay on the end user s computer or phone. for instance in our spam detection project the web version and the desktop ver.sion of the chat app (implemented as a cross-platform app written in java.script) should use a locally run model. your application has strict latency constraints. while a model running on the end user s laptop or smartphone is likely to be slower than one running on a large gpu on your own server you don t have the extra 100 ms of network round trip. you need your app to keep working without connectivity after the model has been downloaded and cached. you should only go with this option if your model is small enough that it won t hog the cpu gpu or ram of your user s laptop or smartphone. in addition since the entire model will be downloaded to the user s device you should make sure that nothing about the model needs to stay confidential. be mindful of the fact that given a trained deep learning model it is usually possible to recover some information about the train.ing data better not to make your trained model public if it was trained on sensitive data. to deploy a model in javascript the tensorflow ecosystem includes tensorflow.js (www.tensorflow.org/js) a javascript library for deep learning that implements almost all of the keras api (originally developed under the working name webkeras) as well as many lower-level tensorflow apis. you can easily import a saved keras model into tensorflow.js to query it as part of your browser-based javascript app or your desktop electron app. inference model optimization optimizing your model for inference is especially important when deploying in an environment with strict constraints on available power and memory (smartphones and embedded devices) or for applications with low latency requirements. you should always seek to optimize your model before importing into tensorflow.js or exporting it to tensorflow lite. there are two popular optimization techniques you can apply weight pruning not every coefficient in a weight tensor contributes equally to the predictions. it s possible to considerably lower the number of parameters in the layers of your model by only keeping the most significant ones. this reduces the memory and compute footprint of your model at a small cost in performance metrics. by deciding how much pruning you want to apply you are in control of the trade-off between size and accuracy. weight quantization deep learning models are trained with single-precision floating-point (float32) weights. however it s possible to quantize weights to 8-bit signed integers (int8) to get an inference-only model that s a quarter the size but remains near the accuracy of the original model. the tensorflow ecosystem includes a weight pruning and quantization toolkit (www .tensorflow.org/model_optimization) that is deeply integrated with the keras api. 6.3.3 monitor your model in the wild you ve exported an inference model you ve integrated it into your application and you ve done a dry run on production data the model behaved exactly as you expected. you ve written unit tests as well as logging and status-monitoring code perfect. now it s time to press the big red button and deploy to production. even this is not the end. once you ve deployed a model you need to keep moni.toring its behavior its performance on new data its interaction with the rest of the application and its eventual impact on business metrics. is user engagement in your online radio up or down after deploying the new music recommender system? has the average ad click-through rate increased after switching to the new click-through-rate prediction model? consider using randomized a/b testing to isolate the impact of the model itself from other changes a subset of cases should go through the new model while another control subset should stick to the old process. once sufficiently many cases have been processed the difference in outcomes between the two is likely attribut.able to the model. if possible do a regular manual audit of the model s predictions on production data. it s generally possible to reuse the same infrastructure as for data annotation send some fraction of the production data to be manually annotated and com.pare the model s predictions to the new annotations. for instance you should definitely do this for the image search engine and the bad-cookie flagging system. when manual audits are impossible consider alternative evaluation avenues such as user surveys (for example in the case of the spam and offensive-content flagging system). 6.3.4 maintain your model lastly no model lasts forever. you ve already learned about concept drift over time the characteristics of your production data will change gradually degrading the perfor.mance and relevance of your model. the lifespan of your music recommender system will be counted in weeks. for the credit card fraud detection systems it will be days. a couple of years in the best case for the image search engine. as soon as your model has launched you should be getting ready to train the next generation that will replace it. as such watch out for changes in the production data. are new features becoming avail.able? should you expand or otherwise edit the label set? keep collecting and annotating data and keep improving your annotation pipeline over time. in particular you should pay special attention to collecting samples that seem to be difficult for your current model to classify such sam.ples are the most likely to help improve performance. this concludes the universal workflow of machine learning that s a lot of things to keep in mind. it takes time and experience to become an expert but don t worry you re already a lot wiser than you were a few chapters ago. you are now familiar with the big picture the entire spectrum of what machine learning projects entail. while most of this book will focus on model development you re now aware that it s only one part of the entire workflow. always keep in mind the big picture! summary when you take on a new machine learning project first define the problem at hand understand the broader context of what you re setting out to do what s the end goal and what are the constraints? collect and annotate a dataset make sure you understand your data in depth. choose how you ll measure success for your problem what metrics will you monitor on your validation data? once you understand the problem and you have an appropriate dataset develop a model prepare your data. pick your evaluation protocol holdout validation? k-fold validation? which portion of the data should you use for validation? achieve statistical power beat a simple baseline. scale up develop a model that can overfit. regularize your model and tune its hyperparameters based on performance on the validation data. a lot of machine learning research tends to focus only on this step but keep the big picture in mind. when your model is ready and yields good performance on the test data it s time for deployment first make sure you set appropriate expectations with stakeholders. optimize a final model for inference and ship a model to the deployment environment of choice web server mobile browser embedded device etc. monitor your model s performance in production and keep collecting data so you can develop the next generation of the model. this chapter covers creating keras models with the sequential class the functional api and model subclassing using built-in keras training and evaluation loops using keras callbacks to customize training using tensorboard to monitor training and evaluation metrics writing training and evaluation loops from scratch you ve now got some experience with keras you re familiar with the sequential model dense layers and built-in apis for training evaluation and inference compile() fit() evaluate() and predict(). you even learned in chapter 3 how to inherit from the layer class to create custom layers and how to use the tensor-flow gradienttape to implement a step-by-step training loop. in the coming chapters we ll dig into computer vision timeseries forecast.ing natural language processing and generative deep learning. these complex applications will require much more than a sequential architecture and the default fit() loop. so let s first turn you into a keras expert! in this chapter you ll get a complete overview of the key ways to work with keras apis everything 172 different ways to build keras models you re going to need to handle the advanced deep learning use cases you ll encoun.ter next. 7.1 a spectrum of workflows the design of the keras api is guided by the principle of progressive disclosure of complex.ity make it easy to get started yet make it possible to handle high-complexity use cases only requiring incremental learning at each step. simple use cases should be easy and approachable and arbitrarily advanced workflows should be possible no mat.ter how niche and complex the thing you want to do there should be a clear path to it. a path that builds upon the various things you ve learned from simpler workflows. this means that you can grow from beginner to expert and still use the same tools only in different ways. as such there s not a single true way of using keras. rather keras offers a spec.trum of workflows from the very simple to the very flexible. there are different ways to build keras models and different ways to train them answering different needs. because all these workflows are based on shared apis such as layer and model com.ponents from any workflow can be used in any other workflow they can all talk to each other. 7.2 different ways to build keras models there are three apis for building models in keras (see figure 7.1) the sequential model the most approachable api it s basically a python list. as such it s limited to simple stacks of layers. the functional api which focuses on graph-like model architectures. it rep.resents a nice mid-point between usability and flexibility and as such it s the most commonly used model-building api. model subclassing a low-level option where you write everything yourself from scratch. this is ideal if you want full control over every little thing. however you won t get access to many built-in keras features and you will be more at risk of making mistakes. functional api + custom layers + custom metrics subclassing sequential api functional api + custom losses write everything + built-in layers + built-in layers + ... yourself from scratch novice users engineers with engineers with researchers simple models standard use niche use cases cases requiring bespoke solutions figure 7.1 progressive disclosure of complexity for model building 7.2.1 the sequential model the simplest way to build a keras model is to use the sequential model which you already know about. from tensorflow import keras from tensorflow.keras import layers model = keras.sequential([ layers.dense(64 activation="relu") layers.dense(10 activation="softmax") ]) note that it s possible to build the same model incrementally via the add() method which is similar to the append() method of a python list. model = keras.sequential() model.add(layers.dense(64 activation="relu")) model.add(layers.dense(10 activation="softmax")) you saw in chapter 4 that layers only get built (which is to say create their weights) when they are called for the first time. that s because the shape of the layers' weights depends on the shape of their input until the input shape is known they can t be created. as such the preceding sequential model does not have any weights (listing 7.3) until you actually call it on some data or call its build() method with an input shape (listing 7.4). at that point the model isn t built yet. >>> model.weights valueerror weights for model sequential_1 have not yet been created. now you can retrieve the model s weights. >>> model.build(input_shape=(none 3)) >>> model.weights [   ] builds the model now the model will expect samples of shape (3). the none in the input shape signals that the batch size could be anything. after the model is built you can display its contents via the summary() method which comes in handy for debugging. different ways to build keras models >>> model.summary() model "sequential_1" layer (type) output shape param # ================================================================= dense_2 (dense) (none 64) 256 dense_3 (dense) (none 10) 650 ================================================================= total params 906 trainable params 906 non-trainable params 0 as you can see this model happens to be named sequential_1. you can give names to everything in keras every model every layer. >>> model = keras.sequential(name="my_example_model") >>> model.add(layers.dense(64 activation="relu" name="my_first_layer")) >>> model.add(layers.dense(10 activation="softmax" name="my_last_layer")) >>> model.build((none 3)) >>> model.summary() model "my_example_model" layer (type) output shape param # ================================================================= my_first_layer (dense) (none 64) 256 my_last_layer (dense) (none 10) 650 ================================================================= total params 906 trainable params 906 non-trainable params 0 when building a sequential model incrementally it s useful to be able to print a sum.mary of what the current model looks like after you add each layer. but you can t print a summary until the model is built! there s actually a way to have your sequential built on the fly just declare the shape of the model s inputs in advance. you can do this via the input class. model = keras.sequential() model.add(keras.input(shape=(3))) model.add(layers.dense(64 activation="relu")) use input to declare the shape of the inputs. note that the shape argument must be the shape of each sample not the shape of one batch. now you can use summary() to follow how the output shape of your model changes as you add more layers >>> model.summary() model "sequential_2" layer (type) output shape param # ================================================================= dense_4 (dense) (none 64) 256 ================================================================= total params 256 trainable params 256 non-trainable params 0 >>> model.add(layers.dense(10 activation="softmax")) >>> model.summary() model "sequential_2" layer (type) output shape param # ================================================================= dense_4 (dense) (none 64) 256 dense_5 (dense) (none 10) 650 ================================================================= total params 906 trainable params 906 non-trainable params 0 this is a pretty common debugging workflow when dealing with layers that transform their inputs in complex ways such as the convolutional layers you ll learn about in chapter 8. 7.2.2 the functional api the sequential model is easy to use but its applicability is extremely limited it can only express models with a single input and a single output applying one layer after the other in a sequential fashion. in practice it s pretty common to encounter models with multiple inputs (say an image and its metadata) multiple outputs (different things you want to predict about the data) or a nonlinear topology. in such cases you d build your model using the functional api. this is what most keras models you ll encounter in the wild use. it s fun and powerful it feels like play.ing with lego bricks. a simple example let s start with something simple the stack of two layers we used in the previous sec.tion. its functional api version looks like the following listing. different ways to build keras models inputs = keras.input(shape=(3) name="my_input") features = layers.dense(64 activation="relu")(inputs) outputs = layers.dense(10 activation="softmax")(features) model = keras.model(inputs=inputs outputs=outputs) let s go over this step by step. we started by declaring an input (note that you can also give names to these input objects like everything else) inputs = keras.input(shape=(3) name="my_input") this inputs object holds information about the shape and dtype of the data that the model will process the model will process batches where each sample has shape (3). the number of samples per batch is >>> inputs.shape variable (indicated by the none batch size). (none 3) >>> inputs.dtype these batches will have float32 dtype float32. we call such an object a symbolic tensor. it doesn t contain any actual data but it encodes the specifications of the actual tensors of data that the model will see when you use it. it stands for future tensors of data. next we created a layer and called it on the input features = layers.dense(64 activation="relu")(inputs) all keras layers can be called both on real tensors of data and on these symbolic ten.sors. in the latter case they return a new symbolic tensor with updated shape and dtype information >>> features.shape (none 64) after obtaining the final outputs we instantiated the model by specifying its inputs and outputs in the model constructor outputs = layers.dense(10 activation="softmax")(features) model = keras.model(inputs=inputs outputs=outputs) here s the summary of our model >>> model.summary() model "functional_1" layer (type) output shape param # ================================================================= my_input (inputlayer) [(none 3)] 0 dense_6 (dense) (none 64) 256 dense_7 (dense) (none 10) 650 ================================================================= total params 906 trainable params 906 non-trainable params 0 multi-input multi-output models unlike this toy model most deep learning models don t look like lists they look like graphs. they may for instance have multiple inputs or multiple outputs. it s for this kind of model that the functional api really shines. let s say you re building a system to rank customer support tickets by priority and route them to the appropriate department. your model has three inputs the title of the ticket (text input) the text body of the ticket (text input) any tags added by the user (categorical input assumed here to be one-hot encoded) we can encode the text inputs as arrays of ones and zeros of size vocabulary_size (see chapter 11 for detailed information about text encoding techniques). your model also has two outputs the priority score of the ticket a scalar between 0 and 1 (sigmoid output) the department that should handle the ticket (a softmax over the set of depart.ments) you can build this model in a few lines with the functional api. define model inputs. define model outputs. vocabulary_size = 10000 combine input features into num_tags = 100 a single tensor features by num_departments = 4 concatenating them. title = keras.input(shape=(vocabulary_size) name="title") text_body = keras.input(shape=(vocabulary_size) name="text_body") tags = keras.input(shape=(num_tags) name="tags") features = layers.concatenate()([title text_body tags]) features = layers.dense(64 activation="relu")(features) priority = layers.dense(1 activation="sigmoid" name="priority")(features) department = layers.dense( num_departments activation="softmax" name="department")(features) model = keras.model(inputs=[title text_body tags] apply an intermediate outputs=[priority department]) layer to recombine input features into richer create the model by specifying its inputs and outputs. representations. different ways to build keras models dummy input data dummy target data the functional api is a simple lego-like yet very flexible way to define arbitrary graphs of layers like these. training a multi-input multi-output model you can train your model in much the same way as you would train a sequential model by calling fit() with lists of input and output data. these lists of data should be in the same order as the inputs you passed to the model constructor. import numpy as np num_samples = 1280 title_data = np.random.randint(0 2 size=(num_samples vocabulary_size)) text_body_data = np.random.randint(0 2 size=(num_samples vocabulary_size)) tags_data = np.random.randint(0 2 size=(num_samples num_tags)) priority_data = np.random.random(size=(num_samples 1)) department_data = np.random.randint(0 2 size=(num_samples num_departments)) model.compile(optimizer="rmsprop" loss=["mean_squared_error" "categorical_crossentropy"] metrics=[["mean_absolute_error"] ["accuracy"]]) model.fit([title_data text_body_data tags_data] [priority_data department_data] epochs=1) model.evaluate([title_data text_body_data tags_data] [priority_data department_data]) priority_preds department_preds = model.predict( [title_data text_body_data tags_data]) if you don t want to rely on input order (for instance because you have many inputs or outputs) you can also leverage the names you gave to the input objects and the output layers and pass data via dictionaries. model.compile(optimizer="rmsprop" loss={"priority" "mean_squared_error" "department" "categorical_crossentropy"} metrics={"priority" ["mean_absolute_error"] "department" ["accuracy"]}) model.fit({"title" title_data "text_body" text_body_data "tags" tags_data} {"priority" priority_data "department" department_data} epochs=1) model.evaluate({"title" title_data "text_body" text_body_data "tags" tags_data} {"priority" priority_data "department" department_data}) priority_preds department_preds = model.predict( {"title" title_data "text_body" text_body_data "tags" tags_data}) the power of the functional api access to layer connectivity a functional model is an explicit graph data structure. this makes it possible to inspect how layers are connected and reuse previous graph nodes (which are layer outputs) as part of new models. it also nicely fits the mental model that most research.ers use when thinking about a deep neural network a graph of layers. this enables two important use cases model visualization and feature extraction. let s visualize the connectivity of the model we just defined (the topology of the model). you can plot a functional model as a graph with the plot_model() utility (see figure 7.2). keras.utils.plot_model(model "ticket_classifier.png") you can add to this plot the input and output shapes of each layer in the model which can be helpful during debugging (see figure 7.3). keras.utils.plot_model( model "ticket_classifier_with_shape_info.png" show_shapes=true) different ways to build keras models the none in the tensor shapes represents the batch size this model allows batches of any size. access to layer connectivity also means that you can inspect and reuse individual nodes (layer calls) in the graph. the model.layers model property provides the list of layers that make up the model and for each layer you can query layer.input and layer.output. >>> model.layers [      ] >>> model.layers.input [  ] >>> model.layers.output  this enables you to do feature extraction creating models that reuse intermediate fea.tures from another model. let s say you want to add another output to the previous model you want to esti.mate how long a given issue ticket will take to resolve a kind of difficulty rating. you could do this via a classification layer over three categories quick medium and difficult. you don t need to recreate and retrain a model from scratch. you can start from the intermediate features of your previous model since you have access to them like this. layers is our intermediate dense layer features = model.layers.output difficulty = layers.dense(3 activation="softmax" name="difficulty")(features) new_model = keras.model( inputs=[title text_body tags] outputs=[priority department difficulty]) let s plot our new model (see figure 7.4) keras.utils.plot_model( new_model "updated_ticket_classifier.png" show_shapes=true) 7.2.3 subclassing the model class the last model-building pattern you should know about is the most advanced one model subclassing. you learned in chapter 3 how to subclass the layer class to create custom layers. subclassing model is pretty similar in the __init__() method define the layers the model will use. in the call() method define the forward pass of the model reusing the layers previously created. instantiate your subclass and call it on data to create its weights. rewriting our previous example as a subclassed model let s take a look at a simple example we will reimplement the customer support ticket management model using a model subclass. class customerticketmodel(keras.model) don t forget to call the super() def __init__(self num_departments) constructor! super().__init__() self.concat_layer = layers.concatenate() self.mixing_layer = layers.dense(64 activation="relu") self.priority_scorer = layers.dense(1 activation="sigmoid") self.department_classifier = layers.dense( num_departments activation="softmax") def call(self inputs) define the forward title = inputs["title"] pass in the call() text_body = inputs["text_body"] method. tags = inputs["tags"] features = self.concat_layer([title text_body tags]) features = self.mixing_layer(features) define sublayers in the constructor. different ways to build keras models priority = self.priority_scorer(features) department = self.department_classifier(features) return priority department once you ve defined the model you can instantiate it. note that it will only create its weights the first time you call it on some data much like layer subclasses model = customerticketmodel(num_departments=4) priority department = model( {"title" title_data "text_body" text_body_data "tags" tags_data}) so far everything looks very similar to layer subclassing a workflow you encountered in chapter 3. what then is the difference between a layer subclass and a model sub.class? it s simple a layer is a building block you use to create models and a model is the top-level object that you will actually train export for inference etc. in short a model has fit() evaluate() and predict() methods. layers don t. other than that the two classes are virtually identical. (another difference is that you can save a model to a file on disk which we will cover in a few sections.) you can compile and train a model subclass just like a sequential or functional model the structure of what you pass as the loss and metrics arguments must match exactly what gets returned by call() here a list of two elements. model.compile(optimizer="rmsprop" loss=["mean_squared_error" "categorical_crossentropy"] metrics=[["mean_absolute_error"] ["accuracy"]]) model.fit({"title" title_data "text_body" text_body_data "tags" tags_data} [priority_data department_data] the structure of the target epochs=1) data must match exactly what is model.evaluate({"title" title_data returned by the call() method "text_body" text_body_data here a list of two elements. "tags" tags_data} [priority_data department_data]) priority_preds department_preds = model.predict({"title" title_data "text_body" text_body_data the structure of the input data must match "tags" tags_data}) exactly what is expected by the call() method here a dict with keys title text_body and tags. the model subclassing workflow is the most flexible way to build a model. it enables you to build models that cannot be expressed as directed acyclic graphs of layers imagine for instance a model where the call() method uses layers inside a for loop or even calls them recursively. anything is possible you re in charge. beware what subclassed models don t support this freedom comes at a cost with subclassed models you are responsible for more of the model logic which means your potential error surface is much larger. as a result you will have more debugging work to do. you are developing a new python object not just snapping together lego bricks. functional and subclassed models are also substantially different in nature. a func.tional model is an explicit data structure a graph of layers which you can view inspect and modify. a subclassed model is a piece of bytecode a python class with a call() method that contains raw code. this is the source of the subclassing workflow s flexibil.ity you can code up whatever functionality you like but it introduces new limitations. for instance because the way layers are connected to each other is hidden inside the body of the call() method you cannot access that information. calling sum.mary() will not display layer connectivity and you cannot plot the model topology via plot_model(). likewise if you have a subclassed model you cannot access the nodes of the graph of layers to do feature extraction because there is simply no graph. once the model is instantiated its forward pass becomes a complete black box. 7.2.4 mixing and matching different components crucially choosing one of these patterns the sequential model the functional api or model subclassing does not lock you out of the others. all models in the keras api can smoothly interoperate with each other whether they re sequential models func.tional models or subclassed models written from scratch. they re all part of the same spectrum of workflows. for instance you can use a subclassed layer or model in a functional model. class classifier(keras.model) def __init__(self num_classes=2) super().__init__() if num_classes == 2 num_units = 1 activation = "sigmoid" else num_units = num_classes activation = "softmax" self.dense = layers.dense(num_units activation=activation) def call(self inputs) return self.dense(inputs) inputs = keras.input(shape=(3)) features = layers.dense(64 activation="relu")(inputs) outputs = classifier(num_classes=10)(features) model = keras.model(inputs=inputs outputs=outputs) inversely you can use a functional model as part of a subclassed layer or model. inputs = keras.input(shape=(64)) outputs = layers.dense(1 activation="sigmoid")(inputs) binary_classifier = keras.model(inputs=inputs outputs=outputs) using built-in training and evaluation loops class mymodel(keras.model) def __init__(self num_classes=2) super().__init__() self.dense = layers.dense(64 activation="relu") self.classifier = binary_classifier def call(self inputs) features = self.dense(inputs) return self.classifier(features) model = mymodel() 7.2.5 remember use the right tool for the job you ve learned about the spectrum of workflows for building keras models from the simplest workflow the sequential model to the most advanced one model subclass.ing. when should you use one over the other? each one has its pros and cons pick the one most suitable for the job at hand. in general the functional api provides you with a pretty good trade-off between ease of use and flexibility. it also gives you direct access to layer connectivity which is very powerful for use cases such as model plotting or feature extraction. if you can use the functional api that is if your model can be expressed as a directed acyclic graph of layers i recommend using it over model subclassing. going forward all examples in this book will use the functional api simply because all the models we will work with are expressible as graphs of layers. we will however make frequent use of subclassed layers. in general using functional models that include subclassed layers provides the best of both worlds high development flex.ibility while retaining the advantages of the functional api. 7.3 using built-in training and evaluation loops the principle of progressive disclosure of complexity access to a spectrum of work.flows that go from dead easy to arbitrarily flexible one step at a time also applies to model training. keras provides you with different workflows for training models. they can be as simple as calling fit() on your data or as advanced as writing a new train.ing algorithm from scratch. you are already familiar with the compile() fit() evaluate() predict() work.flow. as a reminder take a look at the following listing. create a model (we factor this from tensorflow.keras.datasets import mnist into a separate function so as to reuse it later). def get_mnist_model() inputs = keras.input(shape=(28 * 28)) features = layers.dense(512 activation="relu")(inputs) features = layers.dropout(0.5)(features) outputs = layers.dense(10 activation="softmax")(features) model = keras.model(inputs outputs) load your data reserving return model some for validation. (images labels) (test_images test_labels) = mnist.load_data() images = images.reshape((60000 28 * 28)).astype("float32") / 255 test_images = test_images.reshape((10000 28 * 28)).astype("float32") / 255 train_images val_images = images images train_labels val_labels = labels labels compile the model by model = get_mnist_model() specifying its optimizer the model.compile(optimizer="rmsprop" loss function to minimize loss="sparse_categorical_crossentropy" and the metrics to monitor. metrics=["accuracy"]) model.fit(train_images train_labels epochs=3 validation_data=(val_images val_labels)) test_metrics = model.evaluate(test_images test_labels) predictions = model.predict(test_images) use evaluate() to use predict() to compute compute the loss and classification probabilities metrics on new data. on new data. use fit() to train the model optionally providing validation data to monitor performance on unseen data. there are a couple of ways you can customize this simple workflow provide your own custom metrics. pass callbacks to the fit() method to schedule actions to be taken at specific points during training. let s take a look at these. 7.3.1 writing your own metrics metrics are key to measuring the performance of your model in particular to mea.suring the difference between its performance on the training data and its perfor.mance on the test data. commonly used metrics for classification and regression are already part of the built-in keras.metrics module and most of the time that s what you will use. but if you re doing anything out of the ordinary you will need to be able to write your own metrics. it s simple! a keras metric is a subclass of the keras.metrics.metric class. like layers a met.ric has an internal state stored in tensorflow variables. unlike layers these variables aren t updated via backpropagation so you have to write the state-update logic your.self which happens in the update_state() method. for example here s a simple custom metric that measures the root mean squared error (rmse). import tensorflow as tf subclass the metric class. class rootmeansquarederror(keras.metrics.metric) using built-in training and evaluation loops define the state variables in the constructor. like for layers you have access to the add_weight() method. to match our mnist model we expect categorical predictions and integer labels. def __init__(self name="rmse" **kwargs) super().__init__(name=name **kwargs) self.mse_sum = self.add_weight(name="mse_sum" initializer="zeros") self.total_samples = self.add_weight( name="total_samples" initializer="zeros" dtype="int32") def update_state(self y_true y_pred sample_weight=none) y_true = tf.one_hot(y_true depth=tf.shape(y_pred)) mse = tf.reduce_sum(tf.square(y_true - y_pred)) self.mse_sum.assign_add(mse) num_samples = tf.shape(y_pred) self.total_samples.assign_add(num_samples) implement the state update logic in update_state(). the y_true argument is the targets (or labels) for one batch while y_pred represents the corresponding predictions from the model. you can ignore the sample_weight argument we won t use it here. you use the result() method to return the current value of the metric def result(self) return tf.sqrt(self.mse_sum / tf.cast(self.total_samples tf.float32)) meanwhile you also need to expose a way to reset the metric state without having to reinstantiate it this enables the same metric objects to be used across different epochs of training or across both training and evaluation. you do this with the reset_state() method def reset_state(self) self.mse_sum.assign(0.) self.total_samples.assign(0) custom metrics can be used just like built-in ones. let s test-drive our own metric model = get_mnist_model() model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy" rootmeansquarederror()]) model.fit(train_images train_labels epochs=3 validation_data=(val_images val_labels)) test_metrics = model.evaluate(test_images test_labels) you can now see the fit() progress bar displaying the rmse of your model. 7.3.2 using callbacks launching a training run on a large dataset for tens of epochs using model.fit() can be a bit like launching a paper airplane past the initial impulse you don t have any control over its trajectory or its landing spot. if you want to avoid bad outcomes (and thus wasted paper airplanes) it s smarter to use not a paper plane but a drone that can sense its environment send data back to its operator and automatically make steering decisions based on its current state. the keras callbacks api will help you transform your call to model.fit() from a paper airplane into a smart autonomous drone that can self-introspect and dynamically take action. a callback is an object (a class instance implementing specific methods) that is passed to the model in the call to fit() and that is called by the model at various points during training. it has access to all the available data about the state of the model and its performance and it can take action interrupt training save a model load a different weight set or otherwise alter the state of the model. here are some examples of ways you can use callbacks model checkpointing saving the current state of the model at different points during training. early stopping interrupting training when the validation loss is no longer improving (and of course saving the best model obtained during training). dynamically adjusting the value of certain parameters during training such as the learning rate of the optimizer. logging training and validation metrics during training or visualizing the representa.tions learned by the model as they re updated the fit() progress bar that you re familiar with is in fact a callback! the keras.callbacks module includes a number of built-in callbacks (this is not an exhaustive list) keras.callbacks.modelcheckpoint keras.callbacks.earlystopping keras.callbacks.learningratescheduler keras.callbacks.reducelronplateau keras.callbacks.csvlogger let s review two of them to give you an idea of how to use them earlystopping and modelcheckpoint. the earlystopping and modelcheckpoint callbacks when you re training a model there are many things you can t predict from the start. in particular you can t tell how many epochs will be needed to get to an optimal vali.dation loss. our examples so far have adopted the strategy of training for enough epochs that you begin overfitting using the first run to figure out the proper number of epochs to train for and then finally launching a new training run from scratch using this optimal number. of course this approach is wasteful. a much better way to handle this is to stop training when you measure that the validation loss is no longer improving. this can be achieved using the earlystopping callback. the earlystopping callback interrupts training once a target metric being moni.tored has stopped improving for a fixed number of epochs. for instance this callback allows you to interrupt training as soon as you start overfitting thus avoiding having to retrain your model for a smaller number of epochs. this callback is typically used in using built-in training and evaluation loops 189 combination with modelcheckpoint which lets you continually save the model during training (and optionally save only the current best model so far the version of the model that achieved the best performance at the end of an epoch). callbacks are passed to the model via the callbacks argument in fit() which takes a list of callbacks. you can pass any number of callbacks. saves the current weights after every epoch path to the destination model file 7.3.3 callbacks_list = [ keras.callbacks.earlystopping( monitor="val_accuracy" patience=2 ) keras.callbacks.modelcheckpoint( filepath="checkpoint_path.keras" monitor="val_loss" save_best_only=true ) ] model = get_mnist_model() model.compile(optimizer="rmsprop" interrupts training when improvement stops monitors the model s validation accuracy interrupts training when accuracy has stopped improving for two epochs these two arguments mean you won t overwrite the model file unless val_loss has improved which allows you to keep the best model seen during training. loss="sparse_categorical_crossentropy" metrics=["accuracy"]) model.fit(train_images train_labels epochs=10 callbacks=callbacks_list validation_data=(val_images val_labels)) during training called at the start of every epoch on_epoch_begin(epoch logs) on_epoch_end(epoch logs) on_batch_begin(batch logs) on_batch_end(batch logs) on_train_begin(logs) on_train_end(logs) called at the end of training called at the end of every epoch called right before processing each batch called right after processing each batch called at the start of training you monitor accuracy so it should be part of the model s metrics. note that because the callback will monitor validation loss and validation accuracy you need to pass validation_data to the call to fit(). note that you can always save models manually after training as well just call model.save('my_checkpoint_path'). to reload the model you ve saved just use model = keras.models.load_model("checkpoint_path.keras") writing your own callbacks if you need to take a specific action during training that isn t covered by one of the built-in callbacks you can write your own callback. callbacks are implemented by sub-classing the keras.callbacks.callback class. you can then implement any number of the following transparently named methods which are called at various points these methods are all called with a logs argument which is a dictionary containing information about the previous batch epoch or training run training and valida.tion metrics and so on. the on_epoch_* and on_batch_* methods also take the epoch or batch index as their first argument (an integer). here s a simple example that saves a list of per-batch loss values during training and saves a graph of these values at the end of each epoch. from matplotlib import pyplot as plt class losshistory(keras.callbacks.callback) def on_train_begin(self logs) self.per_batch_losses = [] def on_batch_end(self batch logs) self.per_batch_losses.append(logs.get("loss")) def on_epoch_end(self epoch logs) plt.clf() plt.plot(range(len(self.per_batch_losses)) self.per_batch_losses label="training loss for each batch") plt.xlabel(f"batch (epoch {epoch})") plt.ylabel("loss") plt.legend() plt.savefig(f"plot_at_epoch_{epoch}") self.per_batch_losses = [] let s test-drive it model = get_mnist_model() model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) model.fit(train_images train_labels epochs=10 callbacks=[losshistory()] validation_data=(val_images val_labels)) we get plots that look like figure 7.5. 7.3.4 monitoring and visualization with tensorboard to do good research or develop good models you need rich frequent feedback about what s going on inside your models during your experiments. that s the point of run.ning experiments to get information about how well a model performs as much information as possible. making progress is an iterative process a loop you start with an idea and express it as an experiment attempting to validate or invalidate your idea. you run this experiment and process the information it generates. this inspires your next idea. the more iterations of this loop you re able to run the more refined and using built-in training and evaluation loops powerful your ideas become. keras helps you go from idea to experiment in the least possible time and fast gpus can help you get from experiment to result as quickly as possible. but what about processing the experiment s results? that s where tensor-board comes in (see figure 7.6). tensorboard (www.tensorflow.org/tensorboard) is a browser-based application that you can run locally. it s the best way to monitor everything that goes on inside your model during training. with tensorboard you can visually monitor metrics during training visualize your model architecture visualize histograms of activations and gradients explore embeddings in 3d if you re monitoring more information than just the model s final loss you can develop a clearer vision of what the model does and doesn t do and you can make progress more quickly. the easiest way to use tensorboard with a keras model and the fit() method is to use the keras.callbacks.tensorboard callback. in the simplest case just specify where you want the callback to write logs and you re good to go model = get_mnist_model() model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) tensorboard = keras.callbacks.tensorboard( log_dir="/full_path_to_your_log_dir" ) model.fit(train_images train_labels epochs=10 validation_data=(val_images val_labels) callbacks=[tensorboard]) once the model starts running it will write logs at the target location. if you are run.ning your python script on a local machine you can then launch the local tensor-board server using the following command (note that the tensorboard executable should be already available if you have installed tensorflow via pip if not you can install tensorboard manually via pipinstalltensorboard) tensorboard --logdir /full_path_to_your_log_dir you can then navigate to the url that the command returns in order to access the tensorboard interface. if you are running your script in a colab notebook you can run an embedded ten.sorboard instance as part of your notebook using the following commands %load_ext tensorboard %tensorboard --logdir /full_path_to_your_log_dir in the tensorboard interface you will be able to monitor live graphs of your training and evaluation metrics (see figure 7.7). 7.4 writing your own training and evaluation loops the fit() workflow strikes a nice balance between ease of use and flexibility. it s what you will use most of the time. however it isn t meant to support everything a deep learning researcher may want to do even with custom metrics custom losses and cus.tom callbacks. after all the built-in fit() workflow is solely focused on supervised learning a setup where there are known targets (also called labels or annotations) associated with your input data and where you compute your loss as a function of these targets and the model s predictions. however not every form of machine learning falls into this writing your own training and evaluation loops category. there are other setups where no explicit targets are present such as genera.tive learning (which we will discuss in chapter 12) self-supervised learning (where targets are obtained from the inputs) and reinforcement learning (where learning is driven by occasional rewards much like training a dog). even if you re doing regular super.vised learning as a researcher you may want to add some novel bells and whistles that require low-level flexibility. whenever you find yourself in a situation where the built-in fit() is not enough you will need to write your own custom training logic. you already saw simple exam.ples of low-level training loops in chapters 2 and 3. as a reminder the contents of a typical training loop look like this 1 run the forward pass (compute the model s output) inside a gradient tape to obtain a loss value for the current batch of data. 2 retrieve the gradients of the loss with regard to the model s weights. 3 update the model s weights so as to lower the loss value on the current batch of data. these steps are repeated for as many batches as necessary. this is essentially what fit() does under the hood. in this section you will learn to reimplement fit() from scratch which will give you all the knowledge you need to write any training algorithm you may come up with. let s go over the details. 7.4.1 training versus inference in the low-level training loop examples you ve seen so far step 1 (the forward pass) was done via predictions = model(inputs) and step 2 (retrieving the gradients computed by the gradient tape) was done via gradients = tape.gradient(loss model.weights). in the general case there are actually two subtleties you need to take into account. some keras layers such as the dropout layer have different behaviors during training and during inference (when you use them to generate predictions). such layers expose a training boolean argument in their call() method. calling dropout(inputs training=true) will drop some activation entries while calling dropout(inputs training=false) does nothing. by extension functional and sequential models also expose this training argument in their call() methods. remember to pass training =true when you call a keras model during the forward pass! our forward pass thus becomes predictions =model(inputs training=true). in addition note that when you retrieve the gradients of the weights of your model you should not use tape.gradients(loss model.weights) but rather tape .gradients(loss model.trainable_weights). indeed layers and models own two kinds of weights trainable weights these are meant to be updated via backpropagation to mini.mize the loss of the model such as the kernel and bias of a dense layer. non-trainable weights these are meant to be updated during the forward pass by the layers that own them. for instance if you wanted a custom layer to keep a counter of how many batches it has processed so far that information would be stored in a non-trainable weight and at each batch your layer would incre.ment the counter by one. among keras built-in layers the only layer that features non-trainable weights is the batchnormalization layer which we will discuss in chapter 9. the batchnormalization layer needs non-trainable weights in order to track information about the mean and standard deviation of the data that passes through it so as to perform an online approximation of feature normalization (a concept you learned about in chapter 6). taking into account these two details a supervised-learning training step ends up looking like this def train_step(inputs targets) with tf.gradienttape() as tape predictions = model(inputs training=true) loss = loss_fn(targets predictions) writing your own training and evaluation loops gradients = tape.gradients(loss model.trainable_weights) optimizer.apply_gradients(zip(model.trainable_weights gradients)) 7.4.2 low-level usage of metrics in a low-level training loop you will probably want to leverage keras metrics (whether custom ones or the built-in ones). you ve already learned about the metrics api sim.ply call update_state(y_true y_pred)for each batch of targets and predictions and then use result() to query the current metric value metric = keras.metrics.sparsecategoricalaccuracy() targets = [0 1 2] predictions = [[1 0 0] [0 1 0] [0 0 1]] metric.update_state(targets predictions) current_result = metric.result() print(f"result {current_result.2f}") you may also need to track the average of a scalar value such as the model s loss. you can do this via the keras.metrics.mean metric values = [0 1 2 3 4] mean_tracker = keras.metrics.mean() for value in values mean_tracker.update_state(value) print(f"mean of values {mean_tracker.result().2f}") remember to use metric.reset_state() when you want to reset the current results (at the start of a training epoch or at the start of evaluation). 7.4.3 a complete training and evaluation loop let s combine the forward pass backward pass and metrics tracking into a fit()-like training step function that takes a batch of data and targets and returns the logs that would get displayed by the fit() progress bar. prepare the loss prepare the function. model = get_mnist_model() optimizer. loss_fn = keras.losses.sparsecategoricalcrossentropy() prepare the list of optimizer = keras.optimizers.rmsprop() metrics to monitor. metrics = [keras.metrics.sparsecategoricalaccuracy()] loss_tracking_metric = keras.metrics.mean() prepare a mean metric tracker to keep track of the loss average. def train_step(inputs targets) with tf.gradienttape() as tape run the forward pass. note predictions = model(inputs training=true) that we pass training=true. loss = loss_fn(targets predictions) gradients = tape.gradient(loss model.trainable_weights) optimizer.apply_gradients(zip(gradients model.trainable_weights)) run the backward pass. note that we use model.trainable_weights. logs = {} for metric in metrics keep track metric.update_state(targets predictions) of metrics. logs[metric.name] = metric.result() loss_tracking_metric.update_state(loss) keep track of the logs["loss"] = loss_tracking_metric.result() loss average. return logs return the current values of the metrics and the loss. we will need to reset the state of our metrics at the start of each epoch and before run.ning evaluation. here s a utility function to do it. def reset_metrics() for metric in metrics metric.reset_state() loss_tracking_metric.reset_state() we can now lay out our complete training loop. note that we use a tf.data.dataset object to turn our numpy data into an iterator that iterates over the data in batches of size 32. training_dataset = tf.data.dataset.from_tensor_slices( (train_images train_labels)) training_dataset = training_dataset.batch(32) epochs = 3 for epoch in range(epochs) reset_metrics() for inputs_batch targets_batch in training_dataset logs = train_step(inputs_batch targets_batch) print(f"results at the end of epoch {epoch}") for key value in logs.items() print(f"...{key} {value.4f}") and here s the evaluation loop a simple for loop that repeatedly calls a test_step() function which processes a single batch of data. the test_step() function is just a sub.set of the logic of train_step(). it omits the code that deals with updating the weights of the model that is to say everything involving the gradienttape and the optimizer. def test_step(inputs targets) predictions = model(inputs training=false) note that we pass loss = loss_fn(targets predictions) training=false. logs = {} for metric in metrics metric.update_state(targets predictions) logs["val_" + metric.name] = metric.result() writing your own training and evaluation loops loss_tracking_metric.update_state(loss) logs["val_loss"] = loss_tracking_metric.result() return logs val_dataset = tf.data.dataset.from_tensor_slices((val_images val_labels)) val_dataset = val_dataset.batch(32) reset_metrics() for inputs_batch targets_batch in val_dataset logs = test_step(inputs_batch targets_batch) print("evaluation results") for key value in logs.items() print(f"...{key} {value.4f}") congrats you ve just reimplemented fit() and evaluate()! or almost fit() and evaluate() support many more features including large-scale distributed com.putation which requires a bit more work. it also includes several key performance optimizations. let s take a look at one of these optimizations tensorflow function compilation. 7.4.4 make it fast with tf.function you may have noticed that your custom loops are running significantly slower than the built-in fit() and evaluate() despite implementing essentially the same logic. that s because by default tensorflow code is executed line by line eagerly much like numpy code or regular python code. eager execution makes it easier to debug your code but it is far from optimal from a performance standpoint. it s more performant to compile your tensorflow code into a computation graph that can be globally optimized in a way that code interpreted line by line cannot. the syn.tax to do this is very simple just add a @tf.function to any function you want to com.pile before executing as shown in the following listing. @tf.function this is the def test_step(inputs targets) only line that predictions = model(inputs training=false) changed. loss = loss_fn(targets predictions) logs = {} for metric in metrics metric.update_state(targets predictions) logs["val_" + metric.name] = metric.result() loss_tracking_metric.update_state(loss) logs["val_loss"] = loss_tracking_metric.result() return logs val_dataset = tf.data.dataset.from_tensor_slices((val_images val_labels)) val_dataset = val_dataset.batch(32) reset_metrics() for inputs_batch targets_batch in val_dataset logs = test_step(inputs_batch targets_batch) print("evaluation results") for key value in logs.items() print(f"...{key} {value.4f}") on the colab cpu we go from taking 1.80 s to run the evaluation loop to only 0.8 s. much faster! remember while you are debugging your code prefer running it eagerly without any @tf.function decorator. it s easier to track bugs this way. once your code is work.ing and you want to make it fast add a @tf.function decorator to your training step and your evaluation step or any other performance-critical function. 7.4.5 leveraging fit() with a custom training loop in the previous sections we were writing our own training loop entirely from scratch. doing so provides you with the most flexibility but you end up writing a lot of code while simultaneously missing out on many convenient features of fit() such as call.backs or built-in support for distributed training. what if you need a custom training algorithm but you still want to leverage the power of the built-in keras training logic? there s actually a middle ground between fit() and a training loop written from scratch you can provide a custom training step function and let the framework do the rest. you can do this by overriding the train_step() method of the model class. this is the function that is called by fit() for every batch of data. you will then be able to call fit() as usual and it will be running your own learning algorithm under the hood. here s a simple example we create a new class that subclasses keras.model. we override the method train_step(selfdata). its contents are nearly iden.tical to what we used in the previous section. it returns a dictionary mapping metric names (including the loss) to their current values. we implement a metrics property that tracks the model s metric instances. this enables the model to automatically call reset_state() on the model s metrics at the start of each epoch and at the start of a call to evaluate() so you don t have to do it by hand. loss_fn = keras.losses.sparsecategoricalcrossentropy() loss_tracker = keras.metrics.mean(name="loss") class custommodel(keras.model) we override the def train_step(self data) train_step method. inputs targets = data with tf.gradienttape() as tape predictions = self(inputs training=true) loss = loss_fn(targets predictions) this metric object will be used to track the average of per-batch losses during training and evaluation. we use self(inputs training=true) instead of model(inputs training=true) since our model is the class itself. writing your own training and evaluation loops gradients = tape.gradient(loss model.trainable_weights) optimizer.apply_gradients(zip(gradients model.trainable_weights)) loss_tracker.update_state(loss) return {"loss" loss_tracker.result()} @property def metrics(self) any metric you would like to reset across epochs should be listed here. return [loss_tracker] we update the loss tracker metric that tracks the average of the loss. we return the average loss so far by querying the loss tracker metric. we can now instantiate our custom model compile it (we only pass the optimizer since the loss is already defined outside of the model) and train it using fit() as usual inputs = keras.input(shape=(28 * 28)) features = layers.dense(512 activation="relu")(inputs) features = layers.dropout(0.5)(features) outputs = layers.dense(10 activation="softmax")(features) model = custommodel(inputs outputs) model.compile(optimizer=keras.optimizers.rmsprop()) model.fit(train_images train_labels epochs=3) there are a couple of points to note this pattern does not prevent you from building models with the functional api. you can do this whether you re building sequential models functional api models or subclassed models. you don t need to use a @tf.function decorator when you override train_ step the framework does it for you. now what about metrics and what about configuring the loss via compile()? after you ve called compile() you get access to the following self.compiled_loss the loss function you passed to compile(). self.compiled_metrics a wrapper for the list of metrics you passed which allows you to call self.compiled_metrics.update_state() to update all of your metrics at once. self.metrics the actual list of metrics you passed to compile(). note that it also includes a metric that tracks the loss similar to what we did manually with our loss_tracking_metric earlier. we can thus write class custommodel(keras.model) def train_step(self data) inputs targets = data compute the loss via self.compiled_ with tf.gradienttape() as tape predictions = self(inputs training=true) loss. loss = self.compiled_loss(targets predictions) gradients = tape.gradient(loss model.trainable_weights) optimizer.apply_gradients(zip(gradients model.trainable_weights)) self.compiled_metrics.update_state(targets predictions) return {m.name m.result() for m in self.metrics} update the model s metrics return a dict mapping metric via self.compiled_metrics. names to their current value. let s try it inputs = keras.input(shape=(28 * 28)) features = layers.dense(512 activation="relu")(inputs) features = layers.dropout(0.5)(features) outputs = layers.dense(10 activation="softmax")(features) model = custommodel(inputs outputs) model.compile(optimizer=keras.optimizers.rmsprop() loss=keras.losses.sparsecategoricalcrossentropy() metrics=[keras.metrics.sparsecategoricalaccuracy()]) model.fit(train_images train_labels epochs=3) that was a lot of information but you now know enough to use keras to do almost anything. summary keras offers a spectrum of different workflows based on the principle of progres.sive disclosure of complexity. they all smoothly inter-operate together. you can build models via the sequential class via the functional api or by sub-classing the model class. most of the time you ll be using the functional api. the simplest way to train and evaluate a model is via the default fit() and evaluate() methods. keras callbacks provide a simple way to monitor models during your call to fit() and automatically take action based on the state of the model. you can also fully take control of what fit() does by overriding the train_ step() method. beyond fit() you can also write your own training loops entirely from scratch. this is useful for researchers implementing brand-new training algorithms. this chapter covers understanding convolutional neural networks (convnets) using data augmentation to mitigate overfitting using a pretrained convnet to do feature extraction fine-tuning a pretrained convnet computer vision is the earliest and biggest success story of deep learning. every day you re interacting with deep vision models via google photos google image search youtube video filters in camera apps ocr software and many more. these models are also at the heart of cutting-edge research in autonomous driving robotics ai-assisted medical diagnosis autonomous retail checkout systems and even autonomous farming. computer vision is the problem domain that led to the initial rise of deep learn.ing between 2011 and 2015. a type of deep learning model called convolutional neural networks started getting remarkably good results on image classification competitions around that time first with dan ciresan winning two niche competi.tions (the icdar 2011 chinese character recognition competition and the ijcnn 201 2011 german traffic signs recognition competition) and then more notably in fall 2012 with hinton s group winning the high-profile imagenet large-scale visual recog.nition challenge. many more promising results quickly started bubbling up in other computer vision tasks. interestingly these early successes weren t quite enough to make deep learning mainstream at the time it took a few years. the computer vision research commu.nity had spent many years investing in methods other than neural networks and it wasn t quite ready to give up on them just because there was a new kid on the block. in 2013 and 2014 deep learning still faced intense skepticism from many senior computer vision researchers. it was only in 2016 that it finally became dominant. i remember exhorting an ex-professor of mine in february 2014 to pivot to deep learning. it s the next big thing! i would say. well maybe it s just a fad he replied. by 2016 his entire lab was doing deep learning. there s no stopping an idea whose time has come. this chapter introduces convolutional neural networks also known as convnets the type of deep learning model that is now used almost universally in computer vision applications. you ll learn to apply convnets to image-classification problems in par.ticular those involving small training datasets which are the most common use case if you aren t a large tech company. 8.1 introduction to convnets we re about to dive into the theory of what convnets are and why they have been so successful at computer vision tasks. but first let s take a practical look at a simple conv.net example that classifies mnist digits a task we performed in chapter 2 using a densely connected network (our test accuracy then was 97.8%). even though the convnet will be basic its accuracy will blow our densely connected model from chap.ter 2 out of the water. the following listing shows what a basic convnet looks like. it s a stack of conv2d and maxpooling2d layers. you ll see in a minute exactly what they do. we ll build the model using the functional api which we introduced in the previous chapter. from tensorflow import keras from tensorflow.keras import layers inputs = keras.input(shape=(28 28 1)) x = layers.conv2d(filters=32 kernel_size=3 activation="relu")(inputs) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=64 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=128 kernel_size=3 activation="relu")(x) x = layers.flatten()(x) outputs = layers.dense(10 activation="softmax")(x) model = keras.model(inputs=inputs outputs=outputs) introduction to convnets importantly a convnet takes as input tensors of shape (image_height image_width image_channels) not including the batch dimension. in this case we ll configure the convnet to process inputs of size (28281) which is the format of mnist images. let s display the architecture of our convnet. >>> model.summary() model "model" layer (type) output shape param # ================================================================= input_1 (inputlayer) [(none 28 28 1)] 0 conv2d (conv2d) (none 26 26 32) 320 max_pooling2d (maxpooling2d) (none 13 13 32) 0 conv2d_1 (conv2d) (none 11 11 64) 18496 max_pooling2d_1 (maxpooling2 (none 5 5 64) 0 conv2d_2 (conv2d) (none 3 3 128) 73856 flatten (flatten) (none 1152) 0 dense (dense) (none 10) 11530 ================================================================= total params 104202 trainable params 104202 non-trainable params 0 you can see that the output of every conv2d and maxpooling2d layer is a rank-3 tensor of shape (height width channels). the width and height dimensions tend to shrink as you go deeper in the model. the number of channels is controlled by the first argument passed to the conv2d layers (32 64 or 128). after the last conv2d layer we end up with an output of shape (33128) a 3 ! 3 feature map of 128 channels. the next step is to feed this output into a densely con.nected classifier like those you re already familiar with a stack of dense layers. these classifiers process vectors which are 1d whereas the current output is a rank-3 tensor. to bridge the gap we flatten the 3d outputs to 1d with a flatten layer before adding the dense layers. finally we do 10-way classification so our last layer has 10 outputs and a softmax activation. now let s train the convnet on the mnist digits. we ll reuse a lot of the code from the mnist example in chapter 2. because we re doing 10-way classification with a softmax output we ll use the categorical crossentropy loss and because our labels are integers we ll use the sparse version sparse_categorical_crossentropy. from tensorflow.keras.datasets import mnist (train_images train_labels) (test_images test_labels) = mnist.load_data() train_images = train_images.reshape((60000 28 28 1)) train_images = train_images.astype("float32") / 255 test_images = test_images.reshape((10000 28 28 1)) test_images = test_images.astype("float32") / 255 model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) model.fit(train_images train_labels epochs=5 batch_size=64) let s evaluate the model on the test data. >>> test_loss test_acc = model.evaluate(test_images test_labels) >>> print(f"test accuracy {test_acc.3f}") test accuracy 0.991 whereas the densely connected model from chapter 2 had a test accuracy of 97.8% the basic convnet has a test accuracy of 99.1% we decreased the error rate by about 60% (relative). not bad! but why does this simple convnet work so well compared to a densely connected model? to answer this let s dive into what the conv2d and maxpooling2d layers do. 8.1.1 the convolution operation the fundamental difference between a densely connected layer and a convolution layer is this dense layers learn global patterns in their input feature space (for exam.ple for a mnist digit patterns involving all pixels) whereas convolution layers learn local patterns in the case of images patterns found in small 2d windows of the inputs (see figure 8.1). in the previous example these windows were all 3 ! 3. introduction to convnets this key characteristic gives convnets two interesting properties the patterns they learn are translation-invariant. after learning a certain pattern in the lower-right corner of a picture a convnet can recognize it anywhere for example in the upper-left corner. a densely connected model would have to learn the pattern anew if it appeared at a new location. this makes convnets data-efficient when processing images (because the visual world is fundamentally translation-invariant) they need fewer training samples to learn representations that have generalization power. they can learn spatial hierarchies of patterns. a first convolution layer will learn small local patterns such as edges a second convolution layer will learn larger patterns made of the features of the first layers and so on (see figure 8.2). this allows convnets to efficiently learn increasingly complex and abstract visual con.cepts because the visual world is fundamentally spatially hierarchical. convolutions operate over rank-3 tensors called feature maps with two spatial axes (height and width) as well as a depth axis (also called the channels axis). for an rgb image the dimension of the depth axis is 3 because the image has three color chan.nels red green and blue. for a black-and-white picture like the mnist digits the depth is 1 (levels of gray). the convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches producing an output feature map. this output feature map is still a rank-3 tensor it has a width and a height. its depth can be arbitrary because the output depth is a parameter of the layer and the different channels in that depth axis no longer stand for specific colors as in rgb input rather they stand for filters. filters encode specific aspects of the input data at a high level a single filter could encode the concept presence of a face in the input for instance. in the mnist example the first convolution layer takes a feature map of size (28 28 1) and outputs a feature map of size (26 26 32) it computes 32 filters over its input. each of these 32 output channels contains a 26 ! 26 grid of values which is a response map of the filter over the input indicating the response of that filter pattern at different locations in the input (see figure 8.3). response map quantifying the presence of the lter s pattern at original input di erent locations that is what the term feature map means every dimension in the depth axis is a feature (or filter) and the rank-2 tensor output[ n] is the 2d spatial map of the response of this filter over the input. convolutions are defined by two key parameters size of the patches extracted from the inputs these are typically 3 ! 3 or 5 ! 5. in the example they were 3 ! 3 which is a common choice. depth of the output feature map this is the number of filters computed by the con.volution. the example started with a depth of 32 and ended with a depth of 64. in keras conv2d layers these parameters are the first arguments passed to the layer conv2d(output_depth(window_height window_width)). a convolution works by sliding these windows of size 3 3 or 5 ! 5 over the 3d input feature map stopping at every possible location and extracting the 3d patch of surrounding features (shape (window_height window_width input_depth)). each such 3d patch is then transformed into a 1d vector of shape (output_depth) which is done via a tensor product with a learned weight matrix called the convolution kernel the same kernel is reused across every patch. all of these vectors (one per patch) are then spatially reassembled into a 3d output map of shape (height width output_ depth). every spatial location in the output feature map corresponds to the same location in the input feature map (for example the lower-right corner of the output contains information about the lower-right corner of the input). for instance with introduction to convnets 3 ! 3 windows the vector output[i j ] comes from the 3d patch input[i-1i+1 j-1j+1 ]. the full process is detailed in figure 8.4. input feature map 3 3 input patches transformed patches output feature map figure 8.4 how convolution works note that the output width and height may differ from the input width and height for two reasons border effects which can be countered by padding the input feature map the use of strides which i ll define in a second let s take a deeper look at these notions. understanding border effects and padding consider a 5 ! 5 feature map (25 tiles total). there are only 9 tiles around which you can center a 3 ! 3 window forming a 3 ! 3 grid (see figure 8.5). hence the output fea.ture map will be 3 ! 3. it shrinks a little by exactly two tiles alongside each dimension in this case. you can see this border effect in action in the earlier example you start with 28 ! 28 inputs which become 26 ! 26 after the first convolution layer. if you want to get an output feature map with the same spatial dimensions as the input you can use padding. padding consists of adding an appropriate number of rows figure 8.5 valid locations of 3 3 patches in a 5 5 input feature map and columns on each side of the input feature map so as to make it possible to fit cen.ter convolution windows around every input tile. for a 3 ! 3 window you add one col.umn on the right one column on the left one row at the top and one row at the bottom. for a 5 ! 5 window you add two rows (see figure 8.6). etc. in conv2d layers padding is configurable via the padding argument which takes two values "valid" which means no padding (only valid window locations will be used) and "same" which means pad in such a way as to have an output with the same width and height as the input. the padding argument defaults to "valid". understanding convolution strides the other factor that can influence output size is the notion of strides. our description of convolution so far has assumed that the center tiles of the convolution windows are all contiguous. but the distance between two successive windows is a parameter of the introduction to convnets convolution called its stride which defaults to 1. it s possible to have strided convolu.tions convolutions with a stride higher than 1. in figure 8.7 you can see the patches extracted by a 3 ! 3 convolution with stride 2 over a 5 ! 5 input (without padding). 1 2 3 4 figure 8.7 3 3 convolution patches with 2 2 strides 1 2 3 4 using stride 2 means the width and height of the feature map are downsampled by a factor of 2 (in addition to any changes induced by border effects). strided convolu.tions are rarely used in classification models but they come in handy for some types of models as you will see in the next chapter. in classification models instead of strides we tend to use the max-pooling operation to downsample feature maps which you saw in action in our first convnet example. let s look at it in more depth. 8.1.2 the max-pooling operation in the convnet example you may have noticed that the size of the feature maps is halved after every maxpooling2d layer. for instance before the first maxpooling2d lay.ers the feature map is 26 ! 26 but the max-pooling operation halves it to 13 ! 13. that s the role of max pooling to aggressively downsample feature maps much like strided convolutions. max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. it s conceptually similar to convolution except that instead of transforming local patches via a learned linear transforma.tion (the convolution kernel) they re transformed via a hardcoded max tensor operation. a big difference from convolution is that max pooling is usually done with 2 ! 2 windows and stride 2 in order to downsample the feature maps by a fac.tor of 2. on the other hand convolution is typically done with 3 ! 3 windows and no stride (stride 1). why downsample feature maps this way? why not remove the max-pooling layers and keep fairly large feature maps all the way up? let s look at this option. our model would then look like the following listing. inputs = keras.input(shape=(28 28 1)) x = layers.conv2d(filters=32 kernel_size=3 activation="relu")(inputs) x = layers.conv2d(filters=64 kernel_size=3 activation="relu")(x) x = layers.conv2d(filters=128 kernel_size=3 activation="relu")(x) x = layers.flatten()(x) outputs = layers.dense(10 activation="softmax")(x) model_no_max_pool = keras.model(inputs=inputs outputs=outputs) here s a summary of the model >>> model_no_max_pool.summary() model "model_1" layer (type) output shape param # ================================================================= input_2 (inputlayer) [(none 28 28 1)] 0 conv2d_3 (conv2d) (none 26 26 32) 320 conv2d_4 (conv2d) (none 24 24 64) 18496 conv2d_5 (conv2d) (none 22 22 128) 73856 flatten_1 (flatten) (none 61952) 0 dense_1 (dense) (none 10) 619530 ================================================================= total params 712202 trainable params 712202 non-trainable params 0 what s wrong with this setup? two things it isn t conducive to learning a spatial hierarchy of features. the 3 ! 3 windows in the third layer will only contain information coming from 7 ! 7 windows in the initial input. the high-level patterns learned by the convnet will still be very small with regard to the initial input which may not be enough to learn to clas.sify digits (try recognizing a digit by only looking at it through windows that are 7 ! 7 pixels!). we need the features from the last convolution layer to contain information about the totality of the input. the final feature map has 22 ! 22 ! 128 = 61952 total coefficients per sample. this is huge. when you flatten it to stick a dense layer of size 10 on top that layer would have over half a million parameters. this is far too large for such a small model and would result in intense overfitting. training a convnet from scratch on a small dataset in short the reason to use downsampling is to reduce the number of feature-map coefficients to process as well as to induce spatial-filter hierarchies by making succes.sive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover). note that max pooling isn t the only way you can achieve such downsampling. as you already know you can also use strides in the prior convolution layer. and you can use average pooling instead of max pooling where each local input patch is trans.formed by taking the average value of each channel over the patch rather than the max. but max pooling tends to work better than these alternative solutions. the rea.son is that features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map (hence the term feature map) and it s more informative to look at the maximal presence of different features than at their average presence. the most reasonable subsampling strategy is to first produce dense maps of features (via unstrided convolutions) and then look at the maximal activation of the features over small patches rather than looking at sparser windows of the inputs (via strided convolutions) or averaging input patches which could cause you to miss or dilute feature-presence information. at this point you should understand the basics of convnets feature maps convo.lution and max pooling and you should know how to build a small convnet to solve a toy problem such as mnist digits classification. now let s move on to more useful practical applications. 8.2 training a convnet from scratch on a small dataset having to train an image-classification model using very little data is a common situ.ation which you ll likely encounter in practice if you ever do computer vision in a professional context. a few samples can mean anywhere from a few hundred to a few tens of thousands of images. as a practical example we ll focus on classifying images as dogs or cats in a dataset containing 5000 pictures of cats and dogs (2500 cats 2500 dogs). we ll use 2000 pictures for training 1000 for validation and 2000 for testing. in this section we ll review one basic strategy to tackle this problem training a new model from scratch using what little data you have. we ll start by naively training a small convnet on the 2000 training samples without any regularization to set a base.line for what can be achieved. this will get us to a classification accuracy of about 70%. at that point the main issue will be overfitting. then we ll introduce data aug.mentation a powerful technique for mitigating overfitting in computer vision. by using data augmentation we ll improve the model to reach an accuracy of 80 85%. in the next section we ll review two more essential techniques for applying deep learning to small datasets feature extraction with a pretrained model (which will get us to an accuracy of 97.5%) and fine-tuning a pretrained model (which will get us to a final accu.racy of 98.5%). together these three strategies training a small model from scratch doing feature extraction using a pretrained model and fine-tuning a pretrained model will constitute your future toolbox for tackling the problem of performing image classification with small datasets. 8.2.1 the relevance of deep learning for small-data problems what qualifies as enough samples to train a model is relative relative to the size and depth of the model you re trying to train for starters. it isn t possible to train a convnet to solve a complex problem with just a few tens of samples but a few hundred can potentially suffice if the model is small and well regularized and the task is simple. because convnets learn local translation-invariant features they re highly data-efficient on perceptual problems. training a convnet from scratch on a very small image data.set will yield reasonable results despite a relative lack of data without the need for any custom feature engineering. you ll see this in action in this section. what s more deep learning models are by nature highly repurposable you can take say an image-classification or speech-to-text model trained on a large-scale data.set and reuse it on a significantly different problem with only minor changes. specifi.cally in the case of computer vision many pretrained models (usually trained on the imagenet dataset) are now publicly available for download and can be used to boot.strap powerful vision models out of very little data. this is one of the greatest strengths of deep learning feature reuse. you ll explore this in the next section. let s start by getting our hands on the data. 8.2.2 downloading the data the dogs vs. cats dataset that we will use isn t packaged with keras. it was made avail.able by kaggle as part of a computer vision competition in late 2013 back when conv.nets weren t mainstream. you can download the original dataset from www.kaggle .com/c/dogs-vs-cats/data (you ll need to create a kaggle account if you don t already have one don t worry the process is painless). you can also use the kaggle api to download the dataset in colab (see the downloading a kaggle dataset in google colaboratory sidebar). downloading a kaggle dataset in google colaboratory kaggle makes available an easy-to-use api to programmatically download kaggle.hosted datasets. you can use it to download the dogs vs. cats dataset to a colab notebook for instance. this api is available as the kaggle package which is prein.stalled on colab. downloading this dataset is as easy as running the following com.mand in a colab cell !kaggle competitions download -c dogs-vs-cats however access to the api is restricted to kaggle users so in order to run the pre.ceding command you first need to authenticate yourself. the kaggle package will look for your login credentials in a json file located at ~/.kaggle/kaggle.json. let s create this file. training a convnet from scratch on a small dataset first you need to create a kaggle api key and download it to your local machine. just navigate to the kaggle website in a web browser log in and go to the my account page. in your account settings you ll find an api section. clicking the create new api token button will generate a kaggle.json key file and will download it to your machine. second go to your colab notebook and upload the api s key json file to your colab session by running the following code in a notebook cell from google.colab import files files.upload() when you run this cell you will see a choose files button appear. click it and select the kaggle.json file you just downloaded. this uploads the file to the local colab run.time. finally create a ~/.kaggle folder (mkdir ~/.kaggle) and copy the key file to it (cp kaggle.json ~/.kaggle/). as a security best practice you should also make sure that the file is only readable by the current user yourself (chmod 600) !mkdir ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json you can now download the data we re about to use !kaggle competitions download -c dogs-vs-cats the first time you try to download the data you may get a 403 forbidden error. that s because you need to accept the terms associated with the dataset before you download it you ll have to go to www.kaggle.com/c/dogs-vs-cats/rules (while logged into your kaggle account) and click the i understand and accept button. you only need to do this once. finally the training data is a compressed file named train.zip. make sure you uncom.press it (unzip) silently (-qq) !unzip -qq train.zip the pictures in our dataset are medium-resolution color jpegs. figure 8.8 shows some examples. unsurprisingly the original dogs-versus-cats kaggle competition all the way back in 2013 was won by entrants who used convnets. the best entries achieved up to 95% accuracy. in this example we will get fairly close to this accuracy (in the next section) even though we will train our models on less than 10% of the data that was available to the competitors. this dataset contains 25000 images of dogs and cats (12500 from each class) and is 543 mb (compressed). after downloading and uncompressing the data we ll create a new dataset containing three subsets a training set with 1000 samples of each class a validation set with 500 samples of each class and a test set with 1000 samples of each class. why do this? because many of the image datasets you ll encounter in your career only contain a few thousand samples not tens of thousands. having more data available would make the problem easier so it s good practice to learn with a small dataset. the subsampled dataset we will work with will have the following directory structure cats_vs_dogs_small/ contains 1000 cat images ...train/ ......cat/ contains 1000 dog images ......dog/ ...validation/ contains 500 cat images ......cat/ ......dog/ contains 500 dog images ...test/ ......cat/ contains 1000 cat images ......dog/ contains 1000 dog images let s make it happen in a couple calls to shutil. path to the directory where the original dataset was uncompressed import os shutil pathlib original_dir = pathlib.path("train") new_base_dir = pathlib.path("cats_vs_dogs_small") directory where we will store our smaller dataset training a convnet from scratch on a small dataset def make_subset(subset_name start_index end_index) for category in ("cat" "dog") dir = new_base_dir / subset_name / category os.makedirs(dir) fnames = [f"{category}.{i}.jpg" for i in range(start_index end_index)] for fname in fnames shutil.copyfile(src=original_dir / fname dst=dir / fname) make_subset("train" start_index=0 end_index=1000) create the training subset with the first 1000 images of each category. create the validation subset with the next make_subset("validation" start_index=1000 end_index=1500) 500 images of each make_subset("test" start_index=1500 end_index=2500) category. utility function to copy cat (and dog) images from index create the test subset start_index to index end_index to the subdirectory with the next 1000new_base_dir/{subset_name}/cat (and /dog). the images of each category. "subset_name" will be either "train" "validation" or "test". we now have 2000 training images 1000 validation images and 2000 test images. each split contains the same number of samples from each class this is a balanced binary-classification problem which means classification accuracy will be an appropri.ate measure of success. 8.2.3 building the model we will reuse the same general model structure you saw in the first example the conv.net will be a stack of alternated conv2d (with relu activation) and maxpooling2d layers. but because we re dealing with bigger images and a more complex problem we ll make our model larger accordingly it will have two more conv2d and maxpooling2d stages. this serves both to augment the capacity of the model and to further reduce the size of the feature maps so they aren t overly large when we reach the flatten layer. here because we start from inputs of size 180 pixels ! 180 pixels (a somewhat arbitrary choice) we end up with feature maps of size 7 ! 7 just before the flatten layer. note the depth of the feature maps progressively increases in the model (from 32 to 256) whereas the size of the feature maps decreases (from 180 ! 180 to 7 ! 7). this is a pattern you ll see in almost all convnets. because we re looking at a binary-classification problem we ll end the model with a single unit (a dense layer of size 1) and a sigmoid activation. this unit will encode the probability that the model is looking at one class or the other. one last small difference we will start the model with a rescaling layer which will rescale image inputs (whose values are originally in the [0 255] range) to the [0 1] range. from tensorflow import keras from tensorflow.keras import layers the model expects rgb images of size 180 ! 180. inputs = keras.input(shape=(180 180 3)) x = layers.rescaling(1./255)(inputs) x = layers.conv2d(filters=32 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=64 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=128 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=256 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=256 kernel_size=3 activation="relu")(x) x = layers.flatten()(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs=inputs outputs=outputs) rescale inputs to the [0 1] range by dividing them by 255. let s look at how the dimensions of the feature maps change with every successive layer >>> model.summary() model "model_2" layer (type) output shape param # ================================================================= input_3 (inputlayer) [(none 180 180 3)] 0 rescaling (rescaling) (none 180 180 3) 0 conv2d_6 (conv2d) (none 178 178 32) 896 max_pooling2d_2 (maxpooling2 (none 89 89 32) 0 conv2d_7 (conv2d) (none 87 87 64) 18496 max_pooling2d_3 (maxpooling2 (none 43 43 64) 0 conv2d_8 (conv2d) (none 41 41 128) 73856 max_pooling2d_4 (maxpooling2 (none 20 20 128) 0 conv2d_9 (conv2d) (none 18 18 256) 295168 max_pooling2d_5 (maxpooling2 (none 9 9 256) 0 conv2d_10 (conv2d) (none 7 7 256) 590080 flatten_2 (flatten) (none 12544) 0 dense_2 (dense) (none 1) 12545 ================================================================= total params 991041 trainable params 991041 non-trainable params 0 training a convnet from scratch on a small dataset for the compilation step we ll go with the rmsprop optimizer as usual. because we ended the model with a single sigmoid unit we ll use binary crossentropy as the loss (as a reminder check out table 6.1 in chapter 6 for a cheat sheet on which loss func.tion to use in various situations). model.compile(loss="binary_crossentropy" optimizer="rmsprop" metrics=["accuracy"]) 8.2.4 data preprocessing as you know by now data should be formatted into appropriately preprocessed floating-point tensors before being fed into the model. currently the data sits on a drive as jpeg files so the steps for getting it into the model are roughly as follows 1 read the picture files. 2 decode the jpeg content to rgb grids of pixels. 3 convert these into floating-point tensors. 4 resize them to a shared size (we ll use 180 ! 180). 5 pack them into batches (we ll use batches of 32 images). it may seem a bit daunting but fortunately keras has utilities to take care of these steps automatically. in particular keras features the utility function image_dataset_from_ directory() which lets you quickly set up a data pipeline that can automatically turn image files on disk into batches of preprocessed tensors. this is what we ll use here. calling image_dataset_from_directory(directory) will first list the subdirecto.ries of directory and assume each one contains images from one of our classes. it will then index the image files in each subdirectory. finally it will create and return a tf.data.dataset object configured to read these files shuffle them decode them to tensors resize them to a shared size and pack them into batches. from tensorflow.keras.utils import image_dataset_from_directory train_dataset = image_dataset_from_directory( new_base_dir / "train" image_size=(180 180) batch_size=32) validation_dataset = image_dataset_from_directory( new_base_dir / "validation" image_size=(180 180) batch_size=32) test_dataset = image_dataset_from_directory( new_base_dir / "test" image_size=(180 180) batch_size=32) understanding tensorflow dataset objects tensorflow makes available the tf.data api to create efficient input pipelines for machine learning models. its core class is tf.data.dataset. a dataset object is an iterator you can use it in a for loop. it will typically return batches of input data and labels. you can pass a dataset object directly to the fit() method of a keras model. the dataset class handles many key features that would otherwise be cumbersome to implement yourself in particular asynchronous data prefetching (preprocessing the next batch of data while the previous one is being handled by the model which keeps execution flowing without interruptions). the dataset class also exposes a functional-style api for modifying datasets. here s a quick example let s create a dataset instance from a numpy array of random num.bers. we ll consider 1000 samples where each sample is a vector of size 16 the from_tensor_slices() class method can be import numpy as np used to create a dataset from a numpy array or a tuple or dict of numpy arrays. import tensorflow as tf random_numbers = np.random.normal(size=(1000 16)) dataset = tf.data.dataset.from_tensor_slices(random_numbers) at first our dataset just yields single samples >>> for i element in enumerate(dataset) >>> print(element.shape) >>> if i >= 2 >>> break (16) (16) (16) we can use the .batch() method to batch the data >>> batched_dataset = dataset.batch(32) >>> for i element in enumerate(batched_dataset) >>> print(element.shape) >>> if i >= 2 >>> break (32 16) (32 16) (32 16) more broadly we have access to a range of useful dataset methods such as .shuffle(buffer_size) shuffles elements within a buffer .prefetch(buffer_size) prefetches a buffer of elements in gpu memory to achieve better device utilization. .map(callable) applies an arbitrary transformation to each element of the dataset (the function callable which expects to take as input a single ele.ment yielded by the dataset). training a convnet from scratch on a small dataset the .map() method in particular is one that you will use often. here s an example. we ll use it to reshape the elements in our toy dataset from shape (16) to shape (4 4) >>> reshaped_dataset = dataset.map(lambda x tf.reshape(x (4 4))) >>> for i element in enumerate(reshaped_dataset) >>> print(element.shape) >>> if i >= 2 >>> break (4 4) (4 4) (4 4) you re about to see more map() action in this chapter. let s look at the output of one of these dataset objects it yields batches of 180 ! 180 rgb images (shape (32 180 180 3)) and integer labels (shape (32)). there are 32 samples in each batch (the batch size). >>> for data_batch labels_batch in train_dataset >>> print("data batch shape" data_batch.shape) >>> print("labels batch shape" labels_batch.shape) >>> break data batch shape (32 180 180 3) labels batch shape (32) let s fit the model on our dataset. we ll use the validation_data argument in fit() to monitor validation metrics on a separate dataset object. note that we ll also use a modelcheckpoint callback to save the model after each epoch. we ll configure it with the path specifying where to save the file as well as the arguments save_best_only=true and monitor="val_loss" they tell the callback to only save a new file (overwriting any previous one) when the current value of the val_loss metric is lower than at any previous time during training. this guarantees that your saved file will always contain the state of the model corresponding to its best-performing training epoch in terms of its performance on the validation data. as a result we won t have to retrain a new model for a lower number of epochs if we start overfitting we can just reload our saved file. callbacks = [ keras.callbacks.modelcheckpoint( filepath="convnet_from_scratch.keras" save_best_only=true monitor="val_loss") ] history = model.fit( train_dataset epochs=30 validation_data=validation_dataset callbacks=callbacks) let s plot the loss and accuracy of the model over the training and validation data during training (see figure 8.9). import matplotlib.pyplot as plt accuracy = history.history["accuracy"] val_accuracy = history.history["val_accuracy"] loss = history.history["loss"] val_loss = history.history["val_loss"] epochs = range(1 len(accuracy) + 1) plt.plot(epochs accuracy "bo" label="training accuracy") plt.plot(epochs val_accuracy "b" label="validation accuracy") plt.title("training and validation accuracy") plt.legend() plt.figure() plt.plot(epochs loss "bo" label="training loss") plt.plot(epochs val_loss "b" label="validation loss") plt.title("training and validation loss") plt.legend() plt.show() these plots are characteristic of overfitting. the training accuracy increases linearly over time until it reaches nearly 100% whereas the validation accuracy peaks at 75%. the validation loss reaches its minimum after only ten epochs and then stalls whereas the training loss keeps decreasing linearly as training proceeds. let s check the test accuracy. we ll reload the model from its saved file to evaluate it as it was before it started overfitting. training a convnet from scratch on a small dataset test_model = keras.models.load_model("convnet_from_scratch.keras") test_loss test_acc = test_model.evaluate(test_dataset) print(f"test accuracy {test_acc.3f}") we get a test accuracy of 69.5%. (due to the randomness of neural network initializa.tions you may get numbers within one percentage point of that.) because we have relatively few training samples (2000) overfitting will be our number one concern. you already know about a number of techniques that can help mitigate overfitting such as dropout and weight decay (l2 regularization). we re now going to work with a new one specific to computer vision and used almost universally when processing images with deep learning models data augmentation. 8.2.5 using data augmentation overfitting is caused by having too few samples to learn from rendering you unable to train a model that can generalize to new data. given infinite data your model would be exposed to every possible aspect of the data distribution at hand you would never overfit. data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield believable-looking images. the goal is that at training time your model will never see the exact same picture twice. this helps expose the model to more aspects of the data so it can generalize better. in keras this can be done by adding a number of data augmentation layers at the start of your model. let s get started with an example the following sequential model chains several random image transformations. in our model we d include it right before the rescaling layer. data_augmentation = keras.sequential( [ layers.randomflip("horizontal") layers.randomrotation(0.1) layers.randomzoom(0.2) ] ) these are just a few of the layers available (for more see the keras documentation). let s quickly go over this code randomflip("horizontal") applies horizontal flipping to a random 50% of the images that go through it randomrotation(0.1) rotates the input images by a random value in the range [ 10% +10%] (these are fractions of a full circle in degrees the range would be [ 36 degrees +36 degrees]) randomzoom(0.2) zooms in or out of the image by a random factor in the range [-20% +20%] let s look at the augmented images (see figure 8.10). plt.figure(figsize=(10 10)) for images _ in train_dataset.take(1) for i in range(9) augmented_images = data_augmentation(images) apply the ax = plt.subplot(3 3 i + 1) augmentation we can use take(n) to only sample n batches from the dataset. this is equivalent to inserting a break in the loop after the nth batch. plt.imshow(augmented_images.numpy().astype("uint8")) stage to the plt.axis("off") batch of display the first image in the output batch. images. for each of the nine iterations this is a different augmentation of the same image. if we train a new model using this data-augmentation configuration the model will never see the same input twice. but the inputs it sees are still heavily intercorrelated training a convnet from scratch on a small dataset because they come from a small number of original images we can t produce new information we can only remix existing information. as such this may not be enough to completely get rid of overfitting. to further fight overfitting we ll also add a dropout layer to our model right before the densely connected classifier. one last thing you should know about random image augmentation layers just like dropout they re inactive during inference (when we call predict() or evaluate()). during evaluation our model will behave just the same as when it did not include data augmentation and dropout. inputs = keras.input(shape=(180 180 3)) x = data_augmentation(inputs) x = layers.rescaling(1./255)(x) x = layers.conv2d(filters=32 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=64 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=128 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=256 kernel_size=3 activation="relu")(x) x = layers.maxpooling2d(pool_size=2)(x) x = layers.conv2d(filters=256 kernel_size=3 activation="relu")(x) x = layers.flatten()(x) x = layers.dropout(0.5)(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs=inputs outputs=outputs) model.compile(loss="binary_crossentropy" optimizer="rmsprop" metrics=["accuracy"]) let s train the model using data augmentation and dropout. because we expect over-fitting to occur much later during training we will train for three times as many epochs one hundred. callbacks = [ keras.callbacks.modelcheckpoint( filepath="convnet_from_scratch_with_augmentation.keras" save_best_only=true monitor="val_loss") ] history = model.fit( train_dataset epochs=100 validation_data=validation_dataset callbacks=callbacks) let s plot the results again see figure 8.11. thanks to data augmentation and drop.out we start overfitting much later around epochs 60 70 (compared to epoch 10 for the original model). the validation accuracy ends up consistently in the 80 85% range a big improvement over our first try. let s check the test accuracy. test_model = keras.models.load_model( "convnet_from_scratch_with_augmentation.keras") test_loss test_acc = test_model.evaluate(test_dataset) print(f"test accuracy {test_acc.3f}") we get a test accuracy of 83.5%. it s starting to look good! if you re using colab make sure you download the saved file (convnet_from_scratch_with_augmentation.keras) as we will use it for some experiments in the next chapter. by further tuning the model s configuration (such as the number of filters per convolution layer or the number of layers in the model) we might be able to get an even better accuracy likely up to 90%. but it would prove difficult to go any higher just by training our own convnet from scratch because we have so little data to work with. as a next step to improve our accuracy on this problem we ll have to use a pre.trained model which is the focus of the next two sections. 8.3 leveraging a pretrained model a common and highly effective approach to deep learning on small image datasets is to use a pretrained model. a pretrained model is a model that was previously trained on a large dataset typically on a large-scale image-classification task. if this original data.set is large enough and general enough the spatial hierarchy of features learned by the pretrained model can effectively act as a generic model of the visual world and hence its features can prove useful for many different computer vision problems even though these new problems may involve completely different classes than those of the original task. for instance you might train a model on imagenet (where classes leveraging a pretrained model are mostly animals and everyday objects) and then repurpose this trained model for something as remote as identifying furniture items in images. such portability of learned features across different problems is a key advantage of deep learning com.pared to many older shallow learning approaches and it makes deep learning very effective for small-data problems. in this case let s consider a large convnet trained on the imagenet dataset (1.4 million labeled images and 1000 different classes). imagenet contains many animal classes including different species of cats and dogs and you can thus expect it to per.form well on the dogs-versus-cats classification problem. we ll use the vgg16 architecture developed by karen simonyan and andrew zisserman in 2014.1 although it s an older model far from the current state of the art and somewhat heavier than many other recent models i chose it because its architec.ture is similar to what you re already familiar with and it s easy to understand without introducing any new concepts. this may be your first encounter with one of these cutesy model names vgg resnet inception xception and so on you ll get used to them because they will come up frequently if you keep doing deep learning for computer vision. there are two ways to use a pretrained model feature extraction and fine-tuning. we ll cover both of them. let s start with feature extraction. 8.3.1 feature extraction with a pretrained model feature extraction consists of using the representations learned by a previously trained model to extract interesting features from new samples. these features are then run through a new classifier which is trained from scratch. as you saw previously convnets used for image classification comprise two parts they start with a series of pooling and convolution layers and they end with a densely connected classifier. the first part is called the convolutional base of the model. in the case of convnets feature extraction consists of taking the convolutional base of a pre.viously trained network running the new data through it and training a new classifier on top of the output (see figure 8.12). why only reuse the convolutional base? could we reuse the densely connected classifier as well? in general doing so should be avoided. the reason is that the repre.sentations learned by the convolutional base are likely to be more generic and there.fore more reusable the feature maps of a convnet are presence maps of generic concepts over a picture which are likely to be useful regardless of the computer vision problem at hand. but the representations learned by the classifier will necessarily be specific to the set of classes on which the model was trained they will only contain information about the presence probability of this or that class in the entire picture. additionally representations found in densely connected layers no longer contain any 1 karen simonyan and andrew zisserman very deep convolutional networks for large-scale image recogni.tion arxiv (2014) https//arxiv.org/abs/1409.1556. input input input information about where objects are located in the input image these layers get rid of the notion of space whereas the object location is still described by convolutional fea.ture maps. for problems where object location matters densely connected features are largely useless. note that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. layers that come earlier in the model extract local highly generic feature maps (such as visual edges colors and textures) whereas layers that are higher up extract more-abstract concepts (such as cat ear or dog eye ). so if your new dataset differs a lot from the dataset on which the original model was trained you may be bet.ter off using only the first few layers of the model to do feature extraction rather than using the entire convolutional base. in this case because the imagenet class set contains multiple dog and cat classes it s likely to be beneficial to reuse the information contained in the densely connected layers of the original model. but we ll choose not to in order to cover the more general case where the class set of the new problem doesn t overlap the class set of the original model. let s put this into practice by using the convolu.tional base of the vgg16 network trained on imagenet to extract interesting fea.tures from cat and dog images and then train a dogs-versus-cats classifier on top of these features. the vgg16 model among others comes prepackaged with keras. you can import it from the keras.applications module. many other image-classification models (all pretrained on the imagenet dataset) are available as part of keras.applications leveraging a pretrained model xception resnet mobilenet efficientnet densenet etc. let s instantiate the vgg16 model. conv_base = keras.applications.vgg16.vgg16( weights="imagenet" include_top=false input_shape=(180 180 3)) we pass three arguments to the constructor weights specifies the weight checkpoint from which to initialize the model. include_top refers to including (or not) the densely connected classifier on top of the network. by default this densely connected classifier corresponds to the 1000 classes from imagenet. because we intend to use our own densely connected classifier (with only two classes cat and dog) we don t need to include it. input_shape is the shape of the image tensors that we ll feed to the network. this argument is purely optional if we don t pass it the network will be able to process inputs of any size. here we pass it so that we can visualize (in the follow.ing summary) how the size of the feature maps shrinks with each new convolu.tion and pooling layer. here s the detail of the architecture of the vgg16 convolutional base. it s similar to the simple convnets you re already familiar with >>> conv_base.summary() model "vgg16" layer (type) output shape param # ================================================================= input_19 (inputlayer) [(none 180 180 3)] 0 block1_conv1 (conv2d) (none 180 180 64) 1792 block1_conv2 (conv2d) (none 180 180 64) 36928 block1_pool (maxpooling2d) (none 90 90 64) 0 block2_conv1 (conv2d) (none 90 90 128) 73856 block2_conv2 (conv2d) (none 90 90 128) 147584 block2_pool (maxpooling2d) (none 45 45 128) 0 block3_conv1 (conv2d) (none 45 45 256) 295168 block3_conv2 (conv2d) (none 45 45 256) 590080 block3_conv3 (conv2d) (none 45 45 256) 590080 block3_pool (maxpooling2d) (none 22 22 256) 0 block4_conv1 (conv2d) (none 22 22 512) 1180160 block4_conv2 (conv2d) (none 22 22 512) 2359808 block4_conv3 (conv2d) (none 22 22 512) 2359808 block4_pool (maxpooling2d) (none 11 11 512) 0 block5_conv1 (conv2d) (none 11 11 512) 2359808 block5_conv2 (conv2d) (none 11 11 512) 2359808 block5_conv3 (conv2d) (none 11 11 512) 2359808 block5_pool (maxpooling2d) (none 5 5 512) 0 ================================================================= total params 14714688 trainable params 14714688 non-trainable params 0 the final feature map has shape (5 5 512). that s the feature map on top of which we ll stick a densely connected classifier. at this point there are two ways we could proceed run the convolutional base over our dataset record its output to a numpy array on disk and then use this data as input to a standalone densely connected clas.sifier similar to those you saw in chapter 4 of this book. this solution is fast and cheap to run because it only requires running the convolutional base once for every input image and the convolutional base is by far the most expensive part of the pipeline. but for the same reason this technique won t allow us to use data augmentation. extend the model we have (conv_base) by adding dense layers on top and run the whole thing from end to end on the input data. this will allow us to use data augmentation because every input image goes through the convolutional base every time it s seen by the model. but for the same reason this technique is far more expensive than the first. leveraging a pretrained model we ll cover both techniques. let s walk through the code required to set up the first one recording the output of conv_base on our data and using these outputs as inputs to a new model. fast feature extraction without data augmentation we ll start by extracting features as numpy arrays by calling the predict() method of the conv_base model on our training validation and testing datasets. let s iterate over our datasets to extract the vgg16 features. import numpy as np def get_features_and_labels(dataset) all_features = [] all_labels = [] for images labels in dataset preprocessed_images = keras.applications.vgg16.preprocess_input(images) features = conv_base.predict(preprocessed_images) all_features.append(features) all_labels.append(labels) return np.concatenate(all_features) np.concatenate(all_labels) train_features train_labels = get_features_and_labels(train_dataset) val_features val_labels = get_features_and_labels(validation_dataset) test_features test_labels = get_features_and_labels(test_dataset) importantly predict() only expects images not labels but our current dataset yields batches that contain both images and their labels. moreover the vgg16 model expects inputs that are preprocessed with the function keras.applications.vgg16.prepro.cess_input which scales pixel values to an appropriate range. the extracted features are currently of shape (samples 55512) >>> train_features.shape (2000 5 5 512) at this point we can define our densely connected classifier (note the use of dropout for regularization) and train it on the data and labels that we just recorded. inputs = keras.input(shape=(5 5 512)) note the use of the flatten x = layers.flatten()(inputs) layer before passing the x = layers.dense(256)(x) features to a dense layer. x = layers.dropout(0.5)(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(loss="binary_crossentropy" optimizer="rmsprop" metrics=["accuracy"]) callbacks = [ keras.callbacks.modelcheckpoint( filepath="feature_extraction.keras" save_best_only=true monitor="val_loss") ] history = model.fit( train_features train_labels epochs=20 validation_data=(val_features val_labels) callbacks=callbacks) training is very fast because we only have to deal with two dense layers an epoch takes less than one second even on cpu. let s look at the loss and accuracy curves during training (see figure 8.13). import matplotlib.pyplot as plt acc = history.history["accuracy"] val_acc = history.history["val_accuracy"] loss = history.history["loss"] val_loss = history.history["val_loss"] epochs = range(1 len(acc) + 1) plt.plot(epochs acc "bo" label="training accuracy") plt.plot(epochs val_acc "b" label="validation accuracy") plt.title("training and validation accuracy") plt.legend() plt.figure() plt.plot(epochs loss "bo" label="training loss") leveraging a pretrained model plt.plot(epochs val_loss "b" label="validation loss") plt.title("training and validation loss") plt.legend() plt.show() we reach a validation accuracy of about 97% much better than we achieved in the previous section with the small model trained from scratch. this is a bit of an unfair comparison however because imagenet contains many dog and cat instances which means that our pretrained model already has the exact knowledge required for the task at hand. this won t always be the case when you use pretrained features. however the plots also indicate that we re overfitting almost from the start despite using dropout with a fairly large rate. that s because this technique doesn t use data augmentation which is essential for preventing overfitting with small image datasets. feature extraction together with data augmentation now let s review the second technique i mentioned for doing feature extraction which is much slower and more expensive but which allows us to use data augmenta.tion during training creating a model that chains the conv_base with a new dense classifier and training it end to end on the inputs. in order to do this we will first freeze the convolutional base. freezing a layer or set of layers means preventing their weights from being updated during training. if we don t do this the representations that were previously learned by the convolutional base will be modified during training. because the dense layers on top are randomly initialized very large weight updates would be propagated through the network effectively destroying the representations previously learned. in keras we freeze a layer or model by setting its trainable attribute to false. conv_base = keras.applications.vgg16.vgg16( weights="imagenet" include_top=false) conv_base.trainable = false setting trainable to false empties the list of trainable weights of the layer or model. >>> conv_base.trainable = true >>> print("this is the number of trainable weights " "before freezing the conv base" len(conv_base.trainable_weights)) this is the number of trainable weights before freezing the conv base 26 >>> conv_base.trainable = false >>> print("this is the number of trainable weights " "after freezing the conv base" len(conv_base.trainable_weights)) this is the number of trainable weights after freezing the conv base 0 now we can create a new model that chains together 1 a data augmentation stage 2 our frozen convolutional base 3 a dense classifier data_augmentation = keras.sequential( [ layers.randomflip("horizontal") layers.randomrotation(0.1) layers.randomzoom(0.2) ] ) apply data inputs = keras.input(shape=(180 180 3)) augmentation. x = data_augmentation(inputs) x = keras.applications.vgg16.preprocess_input(x) apply input x = conv_base(x) value scaling. x = layers.flatten()(x) x = layers.dense(256)(x) x = layers.dropout(0.5)(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(loss="binary_crossentropy" optimizer="rmsprop" metrics=["accuracy"]) with this setup only the weights from the two dense layers that we added will be trained. that s a total of four weight tensors two per layer (the main weight matrix and the bias vector). note that in order for these changes to take effect you must first compile the model. if you ever modify weight trainability after compilation you should then recompile the model or these changes will be ignored. let s train our model. thanks to data augmentation it will take much longer for the model to start overfitting so we can train for more epochs let s do 50. note this technique is expensive enough that you should only attempt it if you have access to a gpu (such as the free gpu available in colab) it s intractable on cpu. if you can t run your code on gpu then the previous technique is the way to go. callbacks = [ keras.callbacks.modelcheckpoint( filepath="feature_extraction_with_data_augmentation.keras" save_best_only=true monitor="val_loss") ] leveraging a pretrained model history = model.fit( train_dataset epochs=50 validation_data=validation_dataset callbacks=callbacks) let s plot the results again (see figure 8.14). as you can see we reach a validation accuracy of over 98%. this is a strong improvement over the previous model. let s check the test accuracy. test_model = keras.models.load_model( "feature_extraction_with_data_augmentation.keras") test_loss test_acc = test_model.evaluate(test_dataset) print(f"test accuracy {test_acc.3f}") we get a test accuracy of 97.5%. this is only a modest improvement compared to the previous test accuracy which is a bit disappointing given the strong results on the vali.dation data. a model s accuracy always depends on the set of samples you evaluate it on! some sample sets may be more difficult than others and strong results on one set won t necessarily fully translate to all other sets. 8.3.2 fine-tuning a pretrained model another widely used technique for model reuse complementary to feature extraction is fine-tuning conv block 1 frozen (see figure 8.15). fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction and jointly training both the newly added part of the model (in this case the fully connected classifier) and these top layers. this conv block 2 is called fine-tuning because it slightly adjusts the frozen more abstract representations of the model being reused in order to make them more relevant for the problem at hand. i stated earlier that it s necessary to freeze the convolution base of vgg16 in order to be able to conv block 3 train a randomly initialized classifier on top. for the frozen same reason it s only possible to fine-tune the top layers of the convolutional base once the classifier on top has already been trained. if the classifier isn t already trained the error signal propagating through the network during training will be too large and the representations previously learned by conv block 4 the layers being fine-tuned will be destroyed. thus frozen the steps for fine-tuning a network are as follows 1 add our custom network on top of an already-trained base network. 2 freeze the base network. 3 train the part we added. 4 unfreeze some layers in the base network. we ne-tune (note that you should not unfreeze batch conv block 5. normalization layers which are not relevant here since there are no such layers in vgg16. batch normalization and its impact on fine-tuning is explained in the next chapter.) 5 jointly train both these layers and the part we we ne-tune our own fully added. connected classi er. you already completed the first three steps when doing feature extraction. let s proceed with step 4 we ll unfreeze our conv_base and then freeze indi- figure 8.15 fine-tuning the last vidual layers inside it. convolutional block of the vgg16 network leveraging a pretrained model as a reminder this is what our convolutional base looks like >>> conv_base.summary() model "vgg16" layer (type) output shape param # ================================================================= input_19 (inputlayer) [(none 180 180 3)] 0 block1_conv1 (conv2d) (none 180 180 64) 1792 block1_conv2 (conv2d) (none 180 180 64) 36928 block1_pool (maxpooling2d) (none 90 90 64) 0 block2_conv1 (conv2d) (none 90 90 128) 73856 block2_conv2 (conv2d) (none 90 90 128) 147584 block2_pool (maxpooling2d) (none 45 45 128) 0 block3_conv1 (conv2d) (none 45 45 256) 295168 block3_conv2 (conv2d) (none 45 45 256) 590080 block3_conv3 (conv2d) (none 45 45 256) 590080 block3_pool (maxpooling2d) (none 22 22 256) 0 block4_conv1 (conv2d) (none 22 22 512) 1180160 block4_conv2 (conv2d) (none 22 22 512) 2359808 block4_conv3 (conv2d) (none 22 22 512) 2359808 block4_pool (maxpooling2d) (none 11 11 512) 0 block5_conv1 (conv2d) (none 11 11 512) 2359808 block5_conv2 (conv2d) (none 11 11 512) 2359808 block5_conv3 (conv2d) (none 11 11 512) 2359808 block5_pool (maxpooling2d) (none 5 5 512) 0 ================================================================= total params 14714688 trainable params 14714688 non-trainable params 0 we ll fine-tune the last three convolutional layers which means all layers up to block4_ pool should be frozen and the layers block5_conv1 block5_conv2 and block5_conv3 should be trainable. why not fine-tune more layers? why not fine-tune the entire convolutional base? you could. but you need to consider the following earlier layers in the convolutional base encode more generic reusable features whereas layers higher up encode more specialized features. it s more useful to fine-tune the more specialized features because these are the ones that need to be repurposed on your new problem. there would be fast-decreasing returns in fine-tuning lower layers. the more parameters you re training the more you re at risk of overfitting. the convolutional base has 15 million parameters so it would be risky to attempt to train it on your small dataset. thus in this situation it s a good strategy to fine-tune only the top two or three layers in the convolutional base. let s set this up starting from where we left off in the previ.ous example. conv_base.trainable = true for layer in conv_base.layers[-4] layer.trainable = false now we can begin fine-tuning the model. we ll do this with the rmsprop optimizer using a very low learning rate. the reason for using a low learning rate is that we want to limit the magnitude of the modifications we make to the representations of the three layers we re fine-tuning. updates that are too large may harm these representations. model.compile(loss="binary_crossentropy" optimizer=keras.optimizers.rmsprop(learning_rate=1e-5) metrics=["accuracy"]) callbacks = [ keras.callbacks.modelcheckpoint( filepath="fine_tuning.keras" save_best_only=true monitor="val_loss") ] history = model.fit( train_dataset epochs=30 validation_data=validation_dataset callbacks=callbacks) we can finally evaluate this model on the test data model = keras.models.load_model("fine_tuning.keras") test_loss test_acc = model.evaluate(test_dataset) print(f"test accuracy {test_acc.3f}") here we get a test accuracy of 98.5% (again your own results may be within one per.centage point). in the original kaggle competition around this dataset this would have been one of the top results. it s not quite a fair comparison however since we used pretrained features that already contained prior knowledge about cats and dogs which competitors couldn t use at the time. on the positive side by leveraging modern deep learning techniques we managed to reach this result using only a small fraction of the training data that was available for the competition (about 10%). there is a huge difference between being able to train on 20000 samples compared to 2000 samples! now you have a solid set of tools for dealing with image-classification problems in particular with small datasets. summary convnets are the best type of machine learning models for computer vision tasks. it s possible to train one from scratch even on a very small dataset with decent results. convnets work by learning a hierarchy of modular patterns and concepts to represent the visual world. on a small dataset overfitting will be the main issue. data augmentation is a powerful way to fight overfitting when you re working with image data. it s easy to reuse an existing convnet on a new dataset via feature extraction. this is a valuable technique for working with small image datasets. as a complement to feature extraction you can use fine-tuning which adapts to a new problem some of the representations previously learned by an existing model. this pushes performance a bit further. this chapter covers the different branches of computer vision image classification image segmentation object detection modern convnet architecture patterns residual connections batch normalization depthwise separable convolutions techniques for visualizing and interpreting what convnets learn the previous chapter gave you a first introduction to deep learning for computer vision via simple models (stacks of conv2d and maxpooling2d layers) and a simple use case (binary image classification). but there s more to computer vision than image classification! this chapter dives deeper into more diverse applications and advanced best practices. 9.1 three essential computer vision tasks so far we ve focused on image classification models an image goes in a label comes out. this image likely contains a cat this other one likely contains a dog. but image classification is only one of several possible applications of deep learning 238 three essential computer vision tasks in computer vision. in general there are three essential computer vision tasks you need to know about image classification where the goal is to assign one or more labels to an image. it may be either single-label classification (an image can only be in one cate.gory excluding the others) or multi-label classification (tagging all categories that an image belongs to as seen in figure 9.1). for example when you search for a keyword on the google photos app behind the scenes you re querying a very large multilabel classification model one with over 20000 different classes trained on millions of images. image segmentation where the goal is to segment or partition an image into different areas with each area usually representing a category (as seen in fig.ure 9.1). for instance when zoom or google meet diplays a custom back.ground behind you in a video call it s using an image segmentation model to tell your face apart from what s behind it at pixel precision. object detection where the goal is to draw rectangles (called bounding boxes) around objects of interest in an image and associate each rectangle with a class. a self-driving car could use an object-detection model to monitor cars pedestri.ans and signs in view of its cameras for instance. deep learning for computer vision also encompasses a number of somewhat more niche tasks besides these three such as image similarity scoring (estimating how visu.ally similar two images are) keypoint detection (pinpointing attributes of interest in an image such as facial features) pose estimation 3d mesh estimation and so on. but to start with image classification image segmentation and object detection form the foundation that every machine learning engineer should be familiar with. most computer vision applications boil down to one of these three. you ve seen image classification in action in the previous chapter. next let s dive into image segmentation. it s a very useful and versatile technique and you can straight. forwardly approach it with what you ve already learned so far. note that we won t cover object detection because it would be too specialized and too complicated for an introductory book. however you can check out the retinanet example on keras.io which shows how to build and train an object detection model from scratch in keras in around 450 lines of code (https//keras.io/examples/vision/ retinanet/). 9.2 an image segmentation example image segmentation with deep learning is about using a model to assign a class to each pixel in an image thus segmenting the image into different zones (such as background and foreground or road car and sidewalk ). this general cat.egory of techniques can be used to power a considerable variety of valuable applica.tions in image and video editing autonomous driving robotics medical imaging and so on. there are two different flavors of image segmentation that you should know about semantic segmentation where each pixel is independently classified into a seman.tic category like cat. if there are two cats in the image the corresponding pix.els are all mapped to the same generic cat category (see figure 9.2). instance segmentation which seeks not only to classify image pixels by category but also to parse out individual object instances. in an image with two cats in it instance segmentation would treat cat 1 and cat 2 as two separate classes of pixels (see figure 9.2). in this example we ll focus on semantic segmentation we ll be looking once again at images of cats and dogs and this time we ll learn how to tell apart the main subject and its background. we ll work with the oxford-iiit pets dataset (www.robots.ox.ac.uk/~vgg/data/ pets/) which contains 7390 pictures of various breeds of cats and dogs together with foreground-background segmentation masks for each picture. a segmentation mask is the image-segmentation equivalent of a label it s an image the same size as the input image with a single color channel where each integer value corresponds to the class an image segmentation example of the corresponding pixel in the input image. in our case the pixels of our segmen. tation masks can take one of three integer values 1 (foreground) 2 (background) 3 (contour) let s start by downloading and uncompressing our dataset using the wget and tar shell utilities !wget http//www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz !wget http/ /www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz !tar -xf images.tar.gz !tar -xf annotations.tar.gz the input pictures are stored as jpg files in the images/ folder (such as images/abys.sinian_1.jpg) and the corresponding segmentation mask is stored as a png file with the same name in the annotations/trimaps/ folder (such as annotations/trimaps/ abyssinian_1.png). let s prepare the list of input file paths as well as the list of the corresponding mask file paths import os input_dir = "images/" target_dir = "annotations/trimaps/" input_img_paths = sorted( [os.path.join(input_dir fname) for fname in os.listdir(input_dir) if fname.endswith(".jpg")]) target_paths = sorted( [os.path.join(target_dir fname) for fname in os.listdir(target_dir) if fname.endswith(".png") and not fname.startswith(".")]) now what does one of these inputs and its mask look like? let s take a quick look. here s a sample image (see figure 9.3) import matplotlib.pyplot as plt from tensorflow.keras.utils import load_img img_to_array display input plt.axis("off") image number 9. plt.imshow(load_img(input_img_paths)) and here is its corresponding target (see figure 9.4) the original labels are 1 2 and 3. we subtract 1 so that the labels range from 0 to 2 and then we multiply by 127 so that the labels become 0 (black) 127 (gray) 254 (near-white). def display_target(target_array) normalized_array = (target_array.astype("uint8") -1) * 127 plt.axis("off") plt.imshow(normalized_array[ 0]) img = img_to_array(load_img(target_paths color_mode="grayscale")) display_target(img) we use color_mode="grayscale" so that the image we load is treated as having a single color channel. an image segmentation example reserve 1000 samples for validation. split the data into a training and a validation set. next let s load our inputs and targets into two numpy arrays and let s split the arrays into a training and a validation set. since the dataset is very small we can just load everything into memory we resize everything import numpy as np to 200 ! 200. import random total number of samples img_size = (200 200) in the data num_imgs = len(input_img_paths) shuffle the file paths (they were originally sorted by breed). we use the random.random(1337).shuffle(input_img_paths) same seed (1337) in both statements to ensure that the input paths and target paths stay in the same order. random.random(1337).shuffle(target_paths) def path_to_input_image(path) return img_to_array(load_img(path target_size=img_size)) def path_to_target(path) img = img_to_array( load_img(path target_size=img_size color_mode="grayscale")) img = img.astype("uint8") -1 subtract 1 so that our return img labels become 0 1 and 2. input_imgs = np.zeros((num_imgs) + img_size + (3) dtype="float32") targets = np.zeros((num_imgs) + img_size + (1) dtype="uint8") for i in range(num_imgs) input_imgs[i] = path_to_input_image(input_img_paths[i]) targets[i] = path_to_target(target_paths[i]) num_val_samples = 1000 load all images in the input_imgs train_input_imgs = input_imgs[-num_val_samples] float32 array and their masks in the train_targets = targets[-num_val_samples] targets uint8 array (same order). the val_input_imgs = input_imgs[-num_val_samples] inputs have three channels (rbg values) val_targets = targets[-num_val_samples] and the targets have a single channel (which contains integer labels). now it s time to define our model don t forget to from tensorflow import keras rescale input from tensorflow.keras import layers images to the [0-1] range. def get_model(img_size num_classes) inputs = keras.input(shape=img_size + (3)) x = layers.rescaling(1./255)(inputs) note how we use padding="same" everywhere to avoid the influence of border padding on feature map size. x = layers.conv2d(64 3 strides=2 activation="relu" padding="same")(x) x = layers.conv2d(64 3 activation="relu" padding="same")(x) x = layers.conv2d(128 3 strides=2 activation="relu" padding="same")(x) x = layers.conv2d(128 3 activation="relu" padding="same")(x) x = layers.conv2d(256 3 strides=2 padding="same" activation="relu")(x) x = layers.conv2d(256 3 activation="relu" padding="same")(x) x = layers.conv2dtranspose(256 3 activation="relu" padding="same")(x) x = layers.conv2dtranspose( 256 3 activation="relu" padding="same" strides=2)(x) x = layers.conv2dtranspose(128 3 activation="relu" padding="same")(x) x = layers.conv2dtranspose( 128 3 activation="relu" padding="same" strides=2)(x) x = layers.conv2dtranspose(64 3 activation="relu" padding="same")(x) x = layers.conv2dtranspose( 64 3 activation="relu" padding="same" strides=2)(x) outputs = layers.conv2d(num_classes 3 activation="softmax" padding="same")(x) model = keras.model(inputs outputs) return model model = get_model(img_size=img_size num_classes=3) model.summary() here s the output of the model.summary() call model "model" we end the model with a per-pixel three-way softmax to classify each output pixel into one of our three categories. layer (type) output shape param # ================================================================= input_1 (inputlayer) [(none 200 200 3)] 0 rescaling (rescaling) (none 200 200 3) 0 conv2d (conv2d) (none 100 100 64) 1792 conv2d_1 (conv2d) (none 100 100 64) 36928 conv2d_2 (conv2d) (none 50 50 128) 73856 conv2d_3 (conv2d) (none 50 50 128) 147584 an image segmentation example conv2d_4 (conv2d) (none 25 25 256) 295168 conv2d_5 (conv2d) (none 25 25 256) 590080 conv2d_transpose (conv2dtran (none 25 25 256) 590080 conv2d_transpose_1 (conv2dtr (none 50 50 256) 590080 conv2d_transpose_2 (conv2dtr (none 50 50 128) 295040 conv2d_transpose_3 (conv2dtr (none 100 100 128) 147584 conv2d_transpose_4 (conv2dtr (none 100 100 64) 73792 conv2d_transpose_5 (conv2dtr (none 200 200 64) 36928 conv2d_6 (conv2d) (none 200 200 3) 1731 ================================================================= total params 2880643 trainable params 2880643 non-trainable params 0 the first half of the model closely resembles the kind of convnet you d use for image classification a stack of conv2d layers with gradually increasing filter sizes. we down-sample our images three times by a factor of two each ending up with activations of size (2525256). the purpose of this first half is to encode the images into smaller feature maps where each spatial location (or pixel) contains information about a large spatial chunk of the original image. you can understand it as a kind of compression. one important difference between the first half of this model and the classifica.tion models you ve seen before is the way we do downsampling in the classification convnets from the last chapter we used maxpooling2d layers to downsample feature maps. here we downsample by adding strides to every other convolution layer (if you don t remember the details of how convolution strides work see understanding con.volution strides in section 8.1.1). we do this because in the case of image segmenta.tion we care a lot about the spatial location of information in the image since we need to produce per-pixel target masks as output of the model. when you do 2 ! 2 max pooling you are completely destroying location information within each pooling win.dow you return one scalar value per window with zero knowledge of which of the four locations in the windows the value came from. so while max pooling layers per.form well for classification tasks they would hurt us quite a bit for a segmentation task. meanwhile strided convolutions do a better job at downsampling feature maps while retaining location information. throughout this book you ll notice that we tend to use strides instead of max pooling in any model that cares about feature loca.tion such as the generative models in chapter 12. the second half of the model is a stack of conv2dtranspose layers. what are those? well the output of the first half of the model is a feature map of shape (25 25 256) but we want our final output to have the same shape as the target masks (200 200 3). therefore we need to apply a kind of inverse of the transformations we ve applied so far something that will upsample the feature maps instead of downsampling them. that s the purpose of the conv2dtranspose layer you can think of it as a kind of convolu.tion layer that learns to upsample. if you have an input of shape (100 100 64) and you run it through the layer conv2d(128 3 strides=2 padding="same") you get an output of shape (50 50 128). if you run this output through the layer conv2d.transpose(643strides=2padding="same") you get back an output of shape (100 100 64) the same as the original. so after compressing our inputs into feature maps of shape (25 25 256) via a stack of conv2d layers we can simply apply the corresponding sequence of conv2dtranspose layers to get back to images of shape (2002003). we can now compile and fit our model model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy") callbacks = [ keras.callbacks.modelcheckpoint("oxford_segmentation.keras" save_best_only=true) ] history = model.fit(train_input_imgs train_targets epochs=50 callbacks=callbacks batch_size=64 validation_data=(val_input_imgs val_targets)) let s display our training and validation loss (see figure 9.5) epochs = range(1 len(history.history["loss"]) + 1) loss = history.history["loss"] an image segmentation example val_loss = history.history["val_loss"] plt.figure() plt.plot(epochs loss "bo" label="training loss") plt.plot(epochs val_loss "b" label="validation loss") plt.title("training and validation loss") plt.legend() you can see that we start overfitting midway around epoch 25. let s reload our best performing model according to the validation loss and demonstrate how to use it to predict a segmentation mask (see figure 9.6) from tensorflow.keras.utils import array_to_img model = keras.models.load_model("oxford_segmentation.keras") i = 4 test_image = val_input_imgs[i] plt.axis("off") plt.imshow(array_to_img(test_image)) mask = model.predict(np.expand_dims(test_image 0)) def display_mask(pred) utility to display mask = np.argmax(pred axis=-1) a model s mask *= 127 prediction plt.axis("off") plt.imshow(mask) display_mask(mask) there are a couple of small artifacts in our predicted mask caused by geometric shapes in the foreground and background. nevertheless our model appears to work nicely. by this point throughout chapter 8 and the beginning of chapter 9 you ve learned the basics of how to perform image classification and image segmentation you can already accomplish a lot with what you know. however the convnets that experienced engineers develop to solve real-world problems aren t quite as simple as those we ve been using in our demonstrations so far. you re still lacking the essential mental models and thought processes that enable experts to make quick and accurate decisions about how to put together state-of-the-art models. to bridge that gap you need to learn about architecture patterns. let s dive in. 9.3 modern convnet architecture patterns a model s architecture is the sum of the choices that went into creating it which lay.ers to use how to configure them and in what arrangement to connect them. these choices define the hypothesis space of your model the space of possible functions that gradient descent can search over parameterized by the model s weights. like feature engineering a good hypothesis space encodes prior knowledge that you have about the problem at hand and its solution. for instance using convolution layers means that you know in advance that the relevant patterns present in your input images are translation-invariant. in order to effectively learn from data you need to make assumptions about what you re looking for. model architecture is often the difference between success and failure. if you make inappropriate architecture choices your model may be stuck with suboptimal metrics and no amount of training data will save it. inversely a good model architecture will accelerate learning and will enable your model to make efficient use of the training data available reducing the need for large datasets. a good model architecture is one that reduces the size of the search space or otherwise makes it easier to converge to a good point of the search space. just like feature engineering and data curation model architecture is all about making the problem simpler for gradient descent to solve. and remember that gradi.ent descent is a pretty stupid search process so it needs all the help it can get. model architecture is more an art than a science. experienced machine learning engineers are able to intuitively cobble together high-performing models on their first try while beginners often struggle to create a model that trains at all. the keyword here is intuitively no one can give you a clear explanation of what works and what doesn t. experts rely on pattern-matching an ability that they acquire through exten.sive practical experience. you ll develop your own intuition throughout this book. however it s not all about intuition either there isn t much in the way of actual sci.ence but as in any engineering discipline there are best practices. in the following sections we ll review a few essential convnet architecture best practices in particular residual connections batch normalization and separable convolu.tions. once you master how to use them you will be able to build highly effective image models. we will apply them to our cat vs. dog classification problem. modern convnet architecture patterns let s start from the bird s-eye view the modularity-hierarchy-reuse (mhr) formula for system architecture. 9.3.1 modularity hierarchy and reuse if you want to make a complex system simpler there s a universal recipe you can apply just structure your amorphous soup of complexity into modules organize the modules into a hierarchy and start reusing the same modules in multiple places as appropriate ( reuse is another word for abstraction in this context). that s the mhr formula (modularity-hierarchy-reuse) and it underlies system architecture across pretty much every domain where the term architecture is used. it s at the heart of the organization of any system of meaningful complexity whether it s a cathedral your own body the us navy or the keras codebase (see figure 9.7). if you re a software engineer you re already keenly familiar with these principles an effective codebase is one that is modular hierarchical and where you don t reimple.ment the same thing twice but instead rely on reusable classes and functions. if you factor your code by following these principles you could say you re doing software architecture. deep learning itself is simply the application of this recipe to continuous optimiza.tion via gradient descent you take a classic optimization technique (gradient descent over a continuous function space) and you structure the search space into modules (layers) organized into a deep hierarchy (often just a stack the simplest kind of hier.archy) where you reuse whatever you can (for instance convolutions are all about reusing the same information in different spatial locations). likewise deep learning model architecture is primarily about making clever use of modularity hierarchy and reuse. you ll notice that all popular convnet architectures are not only structured into layers they re structured into repeated groups of layers (called blocks or modules ). for instance the popular vgg16 architecture we used in the previous chapter is structured into repeated conv conv max pooling blocks (see figure 9.8). further most convnets often feature pyramid-like structures (feature hierarchies). recall for example the progression in the number of convolution filters we used in the first convnet we built in the previous chapter 32 64 128. the number of filters grows with layer depth while the size of the feature maps shrinks accordingly. you ll notice the same pattern in the blocks of the vgg16 model (see figure 9.8). 224 224 3 224 224 64 7 7 512 28 28 512 14 14 512 1 1 4096 1 1 1000 convolution+relu max pooling fully connected+relu softmax modern convnet architecture patterns deeper hierarchies are intrinsically good because they encourage feature reuse and therefore abstraction. in general a deep stack of narrow layers performs better than a shallow stack of large layers. however there s a limit to how deep you can stack layers due to the problem of vanishing gradients. this leads us to our first essential model architecture pattern residual connections. on the importance of ablation studies in deep learning research deep learning architectures are often more evolved than designed they were devel.oped by repeatedly trying things and selecting what seemed to work. much like in bio.logical systems if you take any complicated experimental deep learning setup chances are you can remove a few modules (or replace some trained features with random ones) with no loss of performance. this is made worse by the incentives that deep learning researchers face by making a system more complex than necessary they can make it appear more interesting or more novel and thus increase their chances of getting a paper through the peer-review process. if you read lots of deep learning papers you will notice that they re often optimized for peer review in both style and content in ways that actively hurt clarity of explanation and reliability of results. for instance mathematics in deep learning papers is rarely used for clearly formalizing concepts or deriving non-obvious results rather it gets leveraged as a signal of seriousness like an expensive suit on a salesman. the goal of research shouldn t be merely to publish but to generate reliable knowl.edge. crucially understanding causality in your system is the most straightforward way to generate reliable knowledge. and there s a very low-effort way to look into cau.sality ablation studies. ablation studies consist of systematically trying to remove parts of a system making it simpler to identify where its performance actually comes from. if you find that x + y + z gives you good results also try x y z x + y x + z and y + z and see what happens. if you become a deep learning researcher cut through the noise in the research pro.cess do ablation studies for your models. always ask could there be a simpler explanation? is this added complexity really necessary? why? 9.3.2 residual connections you probably know about the game of telephone also called chinese whispers in the uk and t l phone arabe in france where an initial message is whispered in the ear of a player who then whispers it in the ear of the next player and so on. the final message ends up bearing little resemblance to its original version. it s a fun metaphor for the cumulative errors that occur in sequential transmission over a noisy channel. as it happens backpropagation in a sequential deep learning model is pretty simi.lar to the game of telephone. you ve got a chain of functions like this one y = f4(f3(f2(f1(x)))) the name of the game is to adjust the parameters of each function in the chain based on the error recorded on the output of f4 (the loss of the model). to adjust f1 you ll need to percolate error information through f2 f3 and f4. however each successive function in the chain introduces some amount of noise. if your function chain is too deep this noise starts overwhelming gradient information and backpropagation stops working. your model won t train at all. this is the vanishing gradients problem. the fix is simple just force each function in the chain to be nondestructive to retain a noiseless version of the information contained in the previous input. the eas.iest way to implement this is to use a residual connection. it s dead easy just add the input of a layer or block of layers back to its output (see figure 9.9). the residual con.nection acts as an information shortcut around destructive or noisy blocks (such as blocks that contain relu activations or dropout layers) enabling error gradient infor.mation from early layers to propagate noiselessly through a deep network. this tech.nique was introduced in 2015 with the resnet family of models (developed by he et al. at microsoft).1 input residual connection figure 9.9 a residual connection around a processing block in practice you d implement a residual connection as follows. some input x=... residual = x this computation block can x=block(x) potentially be destructive or noisy and that s fine. 1 kaiming he et al. deep residual learning for image recognition conference on computer vision and pat.tern recognition (2015) https//arxiv.org/abs/1512.03385. modern convnet architecture patterns set aside the residual. set aside the residual. note that adding the input back to the output of a block implies that the output should have the same shape as the input. however this is not the case if your block includes convolutional layers with an increased number of filters or a max pooling layer. in such cases use a 1 ! 1 conv2d layer with no activation to linearly project the residual to the desired output shape (see listing 9.2). you d typically use padding= "same" in the convolution layers in your target block so as to avoid spatial downsam.pling due to padding and you d use strides in the residual projection to match any downsampling caused by a max pooling layer (see listing 9.3). this is the layer around which we create from tensorflow import keras a residual connection it increases the from tensorflow.keras import layers number of output filers from 32 to 64. note that we use padding="same" inputs = keras.input(shape=(32 32 3)) to avoid downsampling x = layers.conv2d(32 3 activation="relu")(inputs) due to padding. residual = x x = layers.conv2d(64 3 activation="relu" padding="same")(x) residual = layers.conv2d(64 1)(residual) the residual only had 32 x=layers.add([xresidual]) filters so we use a 1 ! 1 conv2d to project it to the now the block output and the correct shape. residual have the same shape and can be added. inputs = keras.input(shape=(32 32 3)) x = layers.conv2d(32 3 activation="relu")(inputs) residual = x x = layers.conv2d(64 3 activation="relu" padding="same")(x) x = layers.maxpooling2d(2 padding="same")(x) residual = layers.conv2d(64 1 strides=2)(residual) x=layers.add([xresidual]) this is the block of two layers around which now the block output and the residual we create a residual connection it includes a have the same shape and can be added. 2 ! 2 max pooling layer. note that we use padding="same" in both the convolution we use strides=2 in the residual layer and the max pooling layer to avoid projection to match the downsampling downsampling due to padding. created by the max pooling layer. to make these ideas more concrete here s an example of a simple convnet structured into a series of blocks each made of two convolution layers and one optional max pooling layer with a residual connection around each block utility function to apply a convolutional block with a inputs = keras.input(shape=(32 32 3)) x = layers.rescaling(1./255)(inputs) residual connection with an option to add max pooling def residual_block(x filters pooling=false) residual = x x = layers.conv2d(filters 3 activation="relu" padding="same")(x) x = layers.conv2d(filters 3 activation="relu" padding="same")(x) if pooling x = layers.maxpooling2d(2 padding="same")(x) residual = layers.conv2d(filters 1 strides=2)(residual) elif filters != residual.shape[-1] residual = layers.conv2d(filters 1)(residual) x = layers.add([x residual]) return x first block x = residual_block(x filters=32 pooling=true) x = residual_block(x filters=64 pooling=true) x=residual_block(xfilters=128pooling=false) x = layers.globalaveragepooling2d()(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs=inputs outputs=outputs) model.summary() the last block doesn t need a max pooling layer since we will apply global average pooling right after it. this is the model summary we get model "model" if we use max pooling we add a strided convolution to project the residual to the expected shape. if we don t use max pooling we only project the residual if the number of channels has changed. second block note the increasing filter count in each block. layer (type) output shape param # connected to ================================================================================================== input_1 (inputlayer) [(none 32 32 3)] 0 rescaling (rescaling) (none 32 32 3) 0 input_1 conv2d (conv2d) (none 32 32 32) 896 rescaling conv2d_1 (conv2d) (none 32 32 32) 9248 conv2d max_pooling2d (maxpooling2d) (none 16 16 32) 0 conv2d_1 conv2d_2 (conv2d) (none 16 16 32) 128 rescaling add (add) (none 16 16 32) 0 max_pooling2d conv2d_2 conv2d_3 (conv2d) (none 16 16 64) 18496 add conv2d_4 (conv2d) (none 16 16 64) 36928 conv2d_3 max_pooling2d_1 (maxpooling2d) (none 8 8 64) 0 conv2d_4 conv2d_5 (conv2d) (none 8 8 64) 2112 add add_1 (add) (none 8 8 64) 0 max_pooling2d_1 conv2d_5 conv2d_6 (conv2d) (none 8 8 128) 73856 add_1 conv2d_7 (conv2d) (none 8 8 128) 147584 conv2d_6 modern convnet architecture patterns conv2d_8 (conv2d) (none 8 8 128) 8320 add_1 add_2 (add) (none 8 8 128) 0 conv2d_7 conv2d_8 global_average_pooling2d (globa (none 128) 0 add_2 dense (dense) (none 1) 129 global_average_pooling2d ================================================================================================== total params 297697 trainable params 297697 non-trainable params 0 with residual connections you can build networks of arbitrary depth without having to worry about vanishing gradients. now let s move on to the next essential convnet architecture pattern batch normal. ization. 9.3.3 batch normalization normalization is a broad category of methods that seek to make different samples seen by a machine learning model more similar to each other which helps the model learn and generalize well to new data. the most common form of data normalization is one you ve already seen several times in this book centering the data on zero by subtract.ing the mean from the data and giving the data a unit standard deviation by dividing the data by its standard deviation. in effect this makes the assumption that the data follows a normal (or gaussian) distribution and makes sure this distribution is cen.tered and scaled to unit variance normalized_data = (data -np.mean(data axis=...)) / np.std(data axis=...) previous examples in this book normalized data before feeding it into models. but data normalization may be of interest after every transformation operated by the net.work even if the data entering a dense or conv2d network has a 0 mean and unit vari.ance there s no reason to expect a priori that this will be the case for the data coming out. could normalizing intermediate activations help? batch normalization does just that. it s a type of layer (batchnormalization in keras) introduced in 2015 by ioffe and szegedy2 it can adaptively normalize data even as the mean and variance change over time during training. during training it uses the mean and variance of the current batch of data to normalize samples and during inference (when a big enough batch of representative data may not be available) it uses an exponential moving average of the batch-wise mean and variance of the data seen during training. 2 sergey ioffe and christian szegedy batch normalization accelerating deep network training by reducing internal covariate shift proceedings of the 32nd international conference on machine learning (2015) https// arxiv.org/abs/1502.03167. although the original paper stated that batch normalization operates by reducing internal covariate shift no one really knows for sure why batch normalization helps. there are various hypotheses but no certitudes. you ll find that this is true of many things in deep learning deep learning is not an exact science but a set of ever-changing empirically derived engineering best practices woven together by unreli.able narratives. you will sometimes feel like the book you have in hand tells you how to do something but doesn t quite satisfactorily say why it works that s because we know the how but we don t know the why. whenever a reliable explanation is available i make sure to mention it. batch normalization isn t one of those cases. in practice the main effect of batch normalization appears to be that it helps with gradient propagation much like residual connections and thus allows for deeper networks. some very deep networks can only be trained if they include multiple batchnormalization layers. for instance batch normalization is used liberally in many of the advanced convnet architectures that come packaged with keras such as resnet50 efficientnet and xception. the batchnormalization layer can be used after any layer dense conv2d etc. x = ... x = layers.conv2d(32 3 use_bias=false)(x) x=layers.batchnormalization()(x) because the output of the conv2d layer gets normalized the layer doesn t need its own bias vector. note both dense and conv2d involve a bias vector a learned variable whose purpose is to make the layer affine rather than purely linear. for instance conv2d returns schematically y=conv(xkernel)+bias and dense returns y = dot(x kernel) + bias. because the normalization step will take care of centering the layer s output on zero the bias vector is no longer needed when using batchnormalization and the layer can be created without it via the option use_bias=false. this makes the layer slightly leaner. importantly i would generally recommend placing the previous layer s activation after the batch normalization layer (although this is still a subject of debate). so instead of doing what is shown in listing 9.4 you would do what s shown in listing 9.5. x = layers.conv2d(32 3 activation="relu")(x) x = layers.batchnormalization()(x) x = layers.conv2d(32 3 use_bias=false)(x) note the lack of x = layers.batchnormalization()(x) activation here. x = layers.activation("relu")(x) we place the activation after the batchnormalization layer. modern convnet architecture patterns the intuitive reason for this approach is that batch normalization will center your inputs on zero while your relu activation uses zero as a pivot for keeping or dropping activated channels doing normalization before the activation maximizes the utiliza.tion of the relu. that said this ordering best practice is not exactly critical so if you do convolution then activation and then batch normalization your model will still train and you won t necessarily see worse results. on batch normalization and fine-tuning batch normalization has many quirks. one of the main ones relates to fine-tuning when fine-tuning a model that includes batchnormalization layers i recommend leaving these layers frozen (set their trainable attribute to false). otherwise they will keep updating their internal mean and variance which can interfere with the very small updates applied to the surrounding conv2d layers. now let s take a look at the last architecture pattern in our series depthwise separable convolutions. 9.3.4 depthwise separable convolutions what if i told you that there s a layer you can use as a drop-in replacement for conv2d that will make your model smaller (fewer trainable weight parameters) and leaner (fewer floating-point operations) and cause it to perform a few percentage points bet.ter on its task? that is precisely what the depthwise separable convolution layer does (sep.arableconv2d in keras). this layer performs a spatial convolution on each channel of its input independently before mixing output channels via a pointwise convolution (a 1 ! 1 convolution) as shown in figure 9.10. this is equivalent to separating the learning of spatial features and the learning of channel-wise features. in much the same way that convolution relies on the assump.tion that the patterns in images are not tied to specific locations depthwise separable convolution relies on the assumption that spatial locations in intermediate activations are highly correlated but different channels are highly independent. because this assumption is generally true for the image representations learned by deep neural networks it serves as a useful prior that helps the model make more efficient use of its training data. a model with stronger priors about the structure of the information it will have to process is a better model as long as the priors are accurate. depthwise separable convolution requires significantly fewer parameters and involves fewer computations compared to regular convolution while having compara.ble representational power. it results in smaller models that converge faster and are less prone to overfitting. these advantages become especially important when you re training small models from scratch on limited data. when it comes to larger-scale models depthwise separable convolutions are the basis of the xception architecture a high-performing convnet that comes packaged with keras. you can read more about the theoretical grounding for depthwise separa.ble convolutions and xception in the paper xception deep learning with depth-wise separable convolutions. 3 the co-evolution of hardware software and algorithms consider a regular convolution operation with a 3 3 window 64 input channels and 64 output channels. it uses 3*3*64*64 = 36864 trainable parameters and when you apply it to an image it runs a number of floating-point operations that is propor.tional to this parameter count. meanwhile consider an equivalent depthwise separa.ble convolution it only involves 3*3*64 + 64*64 = 4672 trainable parameters and proportionally fewer floating-point operations. this efficiency improvement only increases as the number of filters or the size of the convolution windows gets larger. as a result you would expect depthwise separable convolutions to be dramatically faster right? hold on. this would be true if you were writing simple cuda or c imple.mentations of these algorithms in fact you do see a meaningful speedup when run.ning on cpu where the underlying implementation is parallelized c. but in practice you re probably using a gpu and what you re executing on it is far from a simple cuda implementation it s a cudnn kernel a piece of code that has been extraordi.narily optimized down to each machine instruction. it certainly makes sense to spend a lot of effort optimizing this code since cudnn convolutions on nvidia hard.ware are responsible for many exaflops of computation every day. but a side effect of this extreme micro-optimization is that alternative approaches have little chance to compete on performance even approaches that have significant intrinsic advan.tages like depthwise separable convolutions. 3 fran ois chollet xception deep learning with depthwise separable convolutions conference on com.puter vision and pattern recognition (2017) https//arxiv.org/abs/1610.02357. modern convnet architecture patterns despite repeated requests to nvidia depthwise separable convolutions have not benefited from nearly the same level of software and hardware optimization as regu.lar convolutions and as a result they remain only about as fast as regular convolu.tions even though they re using quadratically fewer parameters and floating-point operations. note though that using depthwise separable convolutions remains a good idea even if it does not result in a speedup their lower parameter count means that you are less at risk of overfitting and their assumption that channels should be uncorrelated leads to faster model convergence and more robust representations. what is a slight inconvenience in this case can become an impassable wall in other situations because the entire hardware and software ecosystem of deep learning has been micro-optimized for a very specific set of algorithms (in particular convnets trained via backpropagation) there s an extremely high cost to steering away from the beaten path. if you were to experiment with alternative algorithms such as gradient-free optimization or spiking neural networks the first few parallel c++ or cuda implementations you d come up with would be orders of magnitude slower than a good old convnet no matter how clever and efficient your ideas were. convincing other researchers to adopt your method would be a tough sell even if it were just plain better. you could say that modern deep learning is the product of a co-evolution process between hardware software and algorithms the availability of nvidia gpus and cuda led to the early success of backpropagation-trained convnets which led nvidia to optimize its hardware and software for these algorithms which in turn led to con.solidation of the research community behind these methods. at this point figuring out a different path would require a multi-year re-engineering of the entire ecosystem. 9.3.5 putting it together a mini xception-like model as a reminder here are the convnet architecture principles you ve learned so far your model should be organized into repeated blocks of layers usually made of multiple convolution layers and a max pooling layer. the number of filters in your layers should increase as the size of the spatial fea.ture maps decreases. deep and narrow is better than broad and shallow. introducing residual connections around blocks of layers helps you train deeper networks. it can be beneficial to introduce batch normalization layers after your convolu.tion layers. it can be beneficial to replace conv2d layers with separableconv2d layers which are more parameter-efficient. let s bring these ideas together into a single model. its architecture will resemble a smaller version of xception and we ll apply it to the dogs vs. cats task from the last chapter. for data loading and model training we ll simply reuse the setup we used in section 8.2.5 but we ll replace the model definition with the following convnet inputs = keras.input(shape=(180 180 3)) we use the same x = data_augmentation(inputs) data augmentation don t configuration as before. forget x = layers.rescaling(1./255)(x) input x = layers.conv2d(filters=32 kernel_size=5 use_bias=false)(x) rescaling! for size in [32 64 128 256 512] residual = x x = layers.batchnormalization()(x) x = layers.activation("relu")(x) x = layers.separableconv2d(size 3 padding="same" use_bias=false)(x) x = layers.batchnormalization()(x) x = layers.activation("relu")(x) x = layers.separableconv2d(size 3 padding="same" use_bias=false)(x) x = layers.maxpooling2d(3 strides=2 padding="same")(x) residual = layers.conv2d( size 1 strides=2 padding="same" use_bias=false)(residual) x = layers.add([x residual]) in the original model we used a flatten layer before the dense layer. here we go with a globalaveragepooling2d layer. x = layers.globalaveragepooling2d()(x) x = layers.dropout(0.5)(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs=inputs outputs=outputs) like in the original model we add a dropout layer for regularization. we apply a series of convolutional blocks with increasing feature depth. each block consists of two batch-normalized depthwise separable convolution layers and a max pooling layer with a residual connection around the entire block. note that the assumption that underlies separable convolution feature channels are largely independent does not hold for rgb images! red green and blue color channels are actually highly correlated in natural images. as such the first layer in our model is a regular conv2d layer. we ll start using separableconv2d afterwards. this convnet has a trainable parameter count of 721857 slightly lower than the 991041 trainable parameters of the original model but still in the same ballpark. fig.ure 9.11 shows its training and validation curves. interpreting what convnets learn you ll find that our new model achieves a test accuracy of 90.8% compared to 83.5% for the naive model in the last chapter. as you can see following architecture best practices does have an immediate sizable impact on model performance! at this point if you want to further improve performance you should start system.atically tuning the hyperparameters of your architecture a topic we ll cover in detail in chapter 13. we haven t gone through this step here so the configuration of the pre.ceding model is purely based on the best practices we discussed plus when it comes to gauging model size a small amount of intuition. note that these architecture best practices are relevant to computer vision in gen.eral not just image classification. for example xception is used as the standard convo.lutional base in deeplabv3 a popular state-of-the-art image segmentation solution.4 this concludes our introduction to essential convnet architecture best practices. with these principles in hand you ll be able to develop higher-performing models across a wide range of computer vision tasks. you re now well on your way to becom.ing a proficient computer vision practitioner. to further deepen your expertise there s one last important topic we need to cover interpreting how a model arrives at its predictions. 9.4 interpreting what convnets learn a fundamental problem when building a computer vision application is that of inter.pretability why did your classifier think a particular image contained a fridge when all you can see is a truck? this is especially relevant to use cases where deep learning is used to complement human expertise such as in medical imaging use cases. we will end this chapter by getting you familiar with a range of different techniques for visual.izing what convnets learn and understanding the decisions they make. it s often said that deep learning models are black boxes they learn representa.tions that are difficult to extract and present in a human-readable form. although this is partially true for certain types of deep learning models it s definitely not true for convnets. the representations learned by convnets are highly amenable to visualiza.tion in large part because they re representations of visual concepts. since 2013 a wide array of techniques has been developed for visualizing and interpreting these repre.sentations. we won t survey all of them but we ll cover three of the most accessible and useful ones visualizing intermediate convnet outputs (intermediate activations) useful for under.standing how successive convnet layers transform their input and for getting a first idea of the meaning of individual convnet filters visualizing convnet filters useful for understanding precisely what visual pattern or concept each filter in a convnet is receptive to visualizing heatmaps of class activation in an image useful for understanding which parts of an image were identified as belonging to a given class thus allow.ing you to localize objects in images 4 liang-chieh chen et al. encoder-decoder with atrous separable convolution for semantic image segmen.tation eccv (2018) https//arxiv.org/abs/1802.02611. for the first method activation visualization we ll use the small convnet that we trained from scratch on the dogs-versus-cats classification problem in section 8.2. for the next two methods we ll use a pretrained xception model. 9.4.1 visualizing intermediate activations visualizing intermediate activations consists of displaying the values returned by various convolution and pooling layers in a model given a certain input (the output of a layer is often called its activation the output of the activation function). this gives a view into how an input is decomposed into the different filters learned by the network. we want to visualize feature maps with three dimensions width height and depth (channels). each channel encodes relatively independent features so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2d image. let s start by loading the model that you saved in section 8.2 >>> from tensorflow import keras >>> model = keras.models.load_model( "convnet_from_scratch_with_augmentation.keras") >>> model.summary() model "model_1" layer (type) output shape param # ================================================================= input_2 (inputlayer) [(none 180 180 3)] 0 sequential (sequential) (none 180 180 3) 0 rescaling_1 (rescaling) (none 180 180 3) 0 conv2d_5 (conv2d) (none 178 178 32) 896 max_pooling2d_4 (maxpooling2 (none 89 89 32) 0 conv2d_6 (conv2d) (none 87 87 64) 18496 max_pooling2d_5 (maxpooling2 (none 43 43 64) 0 conv2d_7 (conv2d) (none 41 41 128) 73856 max_pooling2d_6 (maxpooling2 (none 20 20 128) 0 conv2d_8 (conv2d) (none 18 18 256) 295168 max_pooling2d_7 (maxpooling2 (none 9 9 256) 0 interpreting what convnets learn conv2d_9 (conv2d) (none 7 7 256) 590080 flatten_1 (flatten) (none 12544) 0 dropout (dropout) (none 12544) 0 dense_1 (dense) (none 1) 12545 ================================================================= total params 991041 trainable params 991041 non-trainable params 0 next we ll get an input image a picture of a cat not part of the images the network was trained on. from tensorflow import keras import numpy as np img_path = keras.utils.get_file( fname="cat.jpg" origin="https//img-datasets.s3.amazonaws.com/cat.jpg") def get_img_array(img_path target_size) img = keras.utils.load_img( open the image img_path target_size=target_size) file and resize it. array = keras.utils.img_to_array(img) download a test image. turn the image into a array = np.expand_dims(array axis=0) float32 numpy array of return array shape (180 180 3). img_tensor = get_img_array(img_path target_size=(180 180)) add a dimension to transform the array into a batch of a single sample. its shape is now (1 180 180 3). let s display the picture (see figure 9.12). import matplotlib.pyplot as plt plt.axis("off") plt.imshow(img_tensor.astype("uint8")) plt.show() in order to extract the feature maps we want to look at we ll create a keras model that takes batches of images as input and that outputs the activations of all convolution and pooling layers. from tensorflow.keras import layers extract the outputs of all conv2d and maxpooling2d layer_outputs = [] layers and put them in a list. layer_names = [] for layer in model.layers save the if isinstance(layer (layers.conv2d layers.maxpooling2d)) layer names layer_outputs.append(layer.output) for later. layer_names.append(layer.name) activation_model = keras.model(inputs=model.input outputs=layer_outputs) create a model that will return these outputs given the model input. when fed an image input this model returns the values of the layer activations in the original model as a list. this is the first time you ve encountered a multi-output model in this book in practice since you learned about them in chapter 7 until now the models you ve seen have had exactly one input and one output. this one has one input and nine outputs one output per layer activation. activations = activation_model.predict(img_tensor) return a list of nine numpy arrays one array per layer activation. for instance this is the activation of the first convolution layer for the cat image input >>> first_layer_activation = activations >>> print(first_layer_activation.shape) (1 178 178 32) interpreting what convnets learn it s a 178 ! 178 feature map with 32 channels. let s try plotting the fifth channel of the activation of the first layer of the original model (see figure 9.13). import matplotlib.pyplot as plt plt.matshow(first_layer_activation[0 5] cmap="viridis") this channel appears to encode a diagonal edge detector but note that your own channels may vary because the specific filters learned by convolution layers aren t deterministic. now let s plot a complete visualization of all the activations in the network (see fig.ure 9.14). we ll extract and plot every channel in each of the layer activations and we ll stack the results in one big grid with channels stacked side by side. iterate over the activations (and the names of the corresponding layers). images_per_row = 16 for layer_name layer_activation in zip(layer_names activations) n_features = layer_activation.shape[-1] the layer activation has shape size = layer_activation.shape (1 size size n_features). n_cols = n_features // images_per_row display_grid = np.zeros(((size + 1) * n_cols -1 prepare an empty grid for displaying images_per_row * (size + 1) -1)) all the channels in for col in range(n_cols) this activation. for row in range(images_per_row) channel_index = col * images_per_row + row channel_image = layer_activation[0 channel_index].copy() this is a single channel (or feature). normalize channel values within the [0 255] range. all-zero channels are kept at zero. if channel_image.sum() != 0 channel_image -= channel_image.mean() channel_image /= channel_image.std() channel_image *= 64 channel_image += 128 channel_image = np.clip(channel_image 0 255).astype("uint8") display_grid[ col * (size + 1) (col + 1) * size + col row * (size + 1) (row + 1) * size + row] = channel_image scale = 1. / size plt.figure(figsize=(scale * display_grid.shape scale * display_grid.shape)) display the plt.title(layer_name) grid for the plt.grid(false) layer. plt.axis("off") plt.imshow(display_grid aspect="auto" cmap="viridis") place the channel matrix in the empty grid we prepared. interpreting what convnets learn there are a few things to note here the first layer acts as a collection of various edge detectors. at that stage the activations retain almost all of the information present in the initial picture. as you go deeper the activations become increasingly abstract and less visually interpretable. they begin to encode higher-level concepts such as cat ear and cat eye. deeper presentations carry increasingly less information about the visual contents of the image and increasingly more information related to the class of the image. the sparsity of the activations increases with the depth of the layer in the first layer almost all filters are activated by the input image but in the following lay.ers more and more filters are blank. this means the pattern encoded by the fil.ter isn t found in the input image. we have just evidenced an important universal characteristic of the representations learned by deep neural networks the features extracted by a layer become increas.ingly abstract with the depth of the layer. the activations of higher layers carry less and less information about the specific input being seen and more and more infor.mation about the target (in this case the class of the image cat or dog). a deep neu.ral network effectively acts as an information distillation pipeline with raw data going in (in this case rgb pictures) and being repeatedly transformed so that irrelevant infor.mation is filtered out (for example the specific visual appearance of the image) and useful information is magnified and refined (for example the class of the image). this is analogous to the way humans and animals perceive the world after observ.ing a scene for a few seconds a human can remember which abstract objects were present in it (bicycle tree) but can t remember the specific appearance of these objects. in fact if you tried to draw a generic bicycle from memory chances are you couldn t get it even remotely right even though you ve seen thousands of bicycles in your lifetime (see for example figure 9.15). try it right now this effect is absolutely real. your brain has learned to completely abstract its visual input to transform it into high-level visual concepts while filtering out irrelevant visual details making it tremendously difficult to remember how things around you look. 9.4.2 visualizing convnet filters another easy way to inspect the filters learned by convnets is to display the visual pat.tern that each filter is meant to respond to. this can be done with gradient ascent in input space applying gradient descent to the value of the input image of a convnet so as to maximize the response of a specific filter starting from a blank input image. the resulting input image will be one that the chosen filter is maximally responsive to. let s try this with the filters of the xception model pretrained on imagenet. the process is simple we ll build a loss function that maximizes the value of a given filter in a given convolution layer and then we ll use stochastic gradient descent to adjust the values of the input image so as to maximize this activation value. this will be our second example of a low-level gradient descent loop leveraging the gradienttape object (the first one was in chapter 2). first let s instantiate the xception model loaded with weights pretrained on the imagenet dataset. model = keras.applications.xception.xception( weights="imagenet" the classification layers are irrelevant include_top=false) for this use case so we don t include the top stage of the model. we re interested in the convolutional layers of the model the conv2d and separa.bleconv2d layers. we ll need to know their names so we can retrieve their outputs. let s print their names in order of depth. for layer in model.layers if isinstance(layer (keras.layers.conv2d keras.layers.separableconv2d)) print(layer.name) you ll notice that the separableconv2d layers here are all named something like block6_sepconv1 block7_sepconv2 etc. xception is structured into blocks each containing several convolutional layers. now let s create a second model that returns the output of a specific layer a fea.ture extractor model. because our model is a functional api model it is inspectable we can query the output of one of its layers and reuse it in a new model. no need to copy the entire xception code. interpreting what convnets learn you could replace this with the name of any layer in the xception convolutional base. this is the layer object we re layer_name = "block3_sepconv1" interested in. layer = model.get_layer(name=layer_name) feature_extractor = keras.model(inputs=model.input outputs=layer.output) we use model.input and layer.output to create a model that given an input image returns the output of our target layer. to use this model simply call it on some input data (note that xception requires inputs to be preprocessed via the keras.applications.xception.preprocess_input function). activation = feature_extractor( keras.applications.xception.preprocess_input(img_tensor) ) let s use our feature extractor model to define a function that returns a scalar value quantifying how much a given input image activates a given filter in the layer. this is the loss function that we ll maximize during the gradient ascent process the loss function takes an image import tensorflow as tf tensor and the index of the filter we are considering (an integer). def compute_loss(image filter_index) activation = feature_extractor(image) filter_activation = activation[ 2-2 2-2 filter_index] return tf.reduce_mean(filter_activation) note that we avoid border artifacts by only involving return the mean of the activation non-border pixels in the loss we discard the first values for the filter. two pixels along the sides of the activation. the difference between model.predict(x) and model(x) in the previous chapter we used predict(x) for feature extraction. here we re using model(x). what gives? both y = model.predict(x) and y = model(x) (where x is an array of input data) mean run the model on x and retrieve the output y. yet they aren t exactly the same thing. predict() loops over the data in batches (in fact you can specify the batch size via predict(x batch_size=64)) and it extracts the numpy value of the outputs. it s schematically equivalent to this def predict(x) y_batches = [] for x_batch in get_batches(x) (continued) y_batch = model(x).numpy() y_batches.append(y_batch) return np.concatenate(y_batches) this means that predict() calls can scale to very large arrays. meanwhile model(x) happens in-memory and doesn t scale. on the other hand predict() is not differentiable you cannot retrieve its gradient if you call it in a gradienttape scope. you should use model(x) when you need to retrieve the gradients of the model call and you should use predict() if you just need the output value. in other words always use predict() unless you re in the middle of writing a low-level gradient descent loop (as we are now). let s set up the gradient ascent step function using the gradienttape. note that we ll use a @tf.function decorator to speed it up. a non-obvious trick to help the gradient descent process go smoothly is to normal.ize the gradient tensor by dividing it by its l2 norm (the square root of the average of the square of the values in the tensor). this ensures that the magnitude of the updates done to the input image is always within the same range. explicitly watch the image tensor since it isn t a tensorflow variable (only variables are automatically watched in a gradient tape). @tf.function def gradient_ascent_step(image filter_index learning_rate) with tf.gradienttape() as tape tape.watch(image) loss = compute_loss(image filter_index) grads = tape.gradient(loss image) grads = tf.math.l2_normalize(grads) image += learning_rate * grads return image move the image a little bit in a direction that return the updated image activates our target so we can run the step filter more strongly. function in a loop. compute the loss scalar indicating how much the current image activates the filter. compute the gradients of the loss with respect to the image. apply the gradient normalization trick. now we have all the pieces. let s put them together into a python function that takes as input a layer name and a filter index and returns a tensor representing the pattern that maximizes the activation of the specified filter. img_width = 200 img_height = 200 interpreting what convnets learn def generate_filter_pattern(filter_index) iterations = 30 number of gradient learning_rate = 10. ascent steps to applyamplitude image = tf.random.uniform( of a single minval=0.4 step maxval=0.6 shape=(1 img_width img_height 3)) for i in range(iterations) initialize an image tensor with random values (the xception model expects input values in the [0 1] range so here we pick a range centered on 0.5). image = gradient_ascent_step(image filter_index learning_rate) return image.numpy() repeatedly update the values of the image tensor so as to maximize our loss function. the resulting image tensor is a floating-point array of shape (200 200 3) with val.ues that may not be integers within [0 255]. hence we need to post-process this ten.sor to turn it into a displayable image. we do so with the following straightforward utility function. def deprocess_image(image) image -= image.mean() image /= image.std() image *= 64 image += 128 image = np.clip(image 0 255).astype("uint8") normalize image values within the [0 255] range. image = image[25-25 25-25 ] center crop to avoid return image border artifacts. let s try it (see figure 9.16) >>> plt.axis("off") >>> plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2))) it seems that filter 0 in layer block3_sepconv1 is responsive to a horizontal lines pat.tern somewhat water-like or fur-like. now the fun part you can start visualizing every filter in the layer and even every filter in every layer in the model. all_images = [] for filter_index in range(64) print(f"processing filter {filter_index}") image = deprocess_image( generate_filter_pattern(filter_index) ) all_images.append(image) generate and save visualizations for the first 64 filters in the layer. prepare a blank canvas for us to margin = 5 paste filter visualizations on. n = 8 cropped_width = img_width - 25 * 2 cropped_height = img_height - 25 * 2 width = n * cropped_width + (n -1) * margin height = n * cropped_height + (n -1) * margin stitched_filters = np.zeros((width height 3)) for i in range(n) fill the picture with for j in range(n) the saved filters. image = all_images[i * n + j] stitched_filters[ row_start = (cropped_width + margin) * i row_end = (cropped_width + margin) * i + cropped_width column_start = (cropped_height + margin) * j column_end = (cropped_height + margin) * j + cropped_height stitched_filters[ save the row_start row_end canvas to disk. column_start column_end ] = image keras.utils.save_img( f"filters_for_layer_{layer_name}.png" stitched_filters) these filter visualizations (see figure 9.17) tell you a lot about how convnet layers see the world each layer in a convnet learns a collection of filters such that their inputs can be expressed as a combination of the filters. this is similar to how the fourier transform decomposes signals onto a bank of cosine functions. the filters in these convnet filter banks get increasingly complex and refined as you go deeper in the model the filters from the first layers in the model encode simple directional edges and colors (or colored edges in some cases). the filters from layers a bit further up the stack such as block4_sepconv1 encode simple textures made from combinations of edges and colors. the filters in higher layers begin to resemble textures found in natural images feathers eyes leaves and so on. interpreting what convnets learn 9.4.3 visualizing heatmaps of class activation we ll introduce one last visualization technique one that is useful for understanding which parts of a given image led a convnet to its final classification decision. this is helpful for debugging the decision process of a convnet particularly in the case of a classification mistake (a problem domain called model interpretability). it can also allow you to locate specific objects in an image. this general category of techniques is called class activation map (cam) visualiza.tion and it consists of producing heatmaps of class activation over input images. a class activation heatmap is a 2d grid of scores associated with a specific output class computed for every location in any input image indicating how important each loca.tion is with respect to the class under consideration. for instance given an image fed into a dogs-versus-cats convnet cam visualization would allow you to generate a heat-map for the class cat indicating how cat-like different parts of the image are and also a heatmap for the class dog indicating how dog-like parts of the image are. the specific implementation we ll use is the one described in an article titled grad-cam visual explanations from deep networks via gradient-based localization. 5 grad-cam consists of taking the output feature map of a convolution layer given an input image and weighing every channel in that feature map by the gradient of the class with respect to the channel. intuitively one way to understand this trick is to imagine that you re weighting a spatial map of how intensely the input image acti.vates different channels by how important each channel is with regard to the class resulting in a spatial map of how intensely the input image activates the class. let s demonstrate this technique using the pretrained xception model. model = keras.applications.xception.xception(weights="imagenet") note that we include the densely connected classi.fier on top in all previous cases we discarded it. consider the image of two african elephants shown in figure 9.18 possibly a mother and her calf strolling on the savanna. let s convert this image into something the xcep.tion model can read the model was trained on images of size 299 ! 299 preprocessed according to a few rules that are packaged in the keras.applications.xception .preprocess_input utility function. so we need to load the image resize it to 299 299 convert it to a numpy float32 tensor and apply these preprocessing rules. return a float32 numpy array of shape (299 299 3). download the image and store it img_path = keras.utils.get_file( locally under the path img_path. fname="elephant.jpg" origin="https//img-datasets.s3.amazonaws.com/elephant.jpg") return a python imaging library (pil) image of size 299 ! 299. def get_img_array(img_path target_size) img = keras.utils.load_img(img_path target_size=target_size) array = keras.utils.img_to_array(img) array = np.expand_dims(array axis=0) array = keras.applications.xception.preprocess_input(array) return array img_array = get_img_array(img_path target_size=(299 299)) add a dimension to transform the array preprocess the batch (this does into a batch of size (1 299 299 3). channel-wise color normalization). 5 ramprasaath r. selvaraju et al. arxiv (2017) https//arxiv.org/abs/1610.02391. interpreting what convnets learn you can now run the pretrained network on the image and decode its prediction vec.tor back to a human-readable format >>> preds = model.predict(img_array) >>> print(keras.applications.xception.decode_predictions(preds top=3)) [("n02504458" "african_elephant" 0.8699266) ("n01871265" "tusker" 0.076968715) ("n02504013" "indian_elephant" 0.02353728)] the top three classes predicted for this image are as follows african elephant (with 87% probability) tusker (with 7% probability) indian elephant (with 2% probability) the network has recognized the image as containing an undetermined quantity of african elephants. the entry in the prediction vector that was maximally activated is the one corresponding to the african elephant class at index 386 >>> np.argmax(preds) 386 to visualize which parts of the image are the most african-elephant like let s set up the grad-cam process. first we create a model that maps the input image to the activations of the last convolutional layer. last_conv_layer_name = "block14_sepconv2_act" classifier_layer_names = [ "avg_pool" "predictions" ] last_conv_layer = model.get_layer(last_conv_layer_name) last_conv_layer_model = keras.model(model.inputs last_conv_layer.output) second we create a model that maps the activations of the last convolutional layer to the final class predictions. classifier_input = keras.input(shape=last_conv_layer.output.shape) x = classifier_input for layer_name in classifier_layer_names x = model.get_layer(layer_name)(x) classifier_model = keras.model(classifier_input x) then we compute the gradient of the top predicted class for our input image with respect to the activations of the last convolution layer. import tensorflow as tf compute activations of the last conv layer and make the tape watch it. with tf.gradienttape() as tape last_conv_layer_output = last_conv_layer_model(img_array) tape.watch(last_conv_layer_output) preds = classifier_model(last_conv_layer_output) retrieve the activation top_pred_index = tf.argmax(preds) channel corresponding to the top predicted class. top_class_channel = preds[ top_pred_index] grads = tape.gradient(top_class_channel last_conv_layer_output) this is the gradient of the top predicted class with regard to the output feature map of the last convolutional layer. interpreting what convnets learn now we apply pooling and importance weighting to the gradient tensor to obtain our heatmap of class activation. this is a vector where each entry is the mean intensity of the gradient for a given channel. it quantifies the importance of each channel with regard to the top predicted class. pooled_grads = tf.reduce_mean(grads axis=(0 1 2)).numpy() last_conv_layer_output = last_conv_layer_output.numpy() for i in range(pooled_grads.shape[-1]) multiply each channel in the output of the last last_conv_layer_output[ i] *= pooled_grads[i] convolutional layer by how important this heatmap = np.mean(last_conv_layer_output axis=-1) the channel-wise mean of the resulting feature channel is. map is our heatmap of class activation. for visualization purposes we ll also normalize the heatmap between 0 and 1. the result is shown in figure 9.19. heatmap = np.maximum(heatmap 0) heatmap /= np.max(heatmap) plt.matshow(heatmap) finally let s generate an image that superimposes the original image on the heatmap we just obtained (see figure 9.20). import matplotlib.cm as cm img = keras.utils.load_img(img_path) load the img = keras.utils.img_to_array(img) original image. rescale the heatmap to the range 0 255. heatmap = np.uint8(255 * heatmap) jet = cm.get_cmap("jet") use the "jet" colormap jet_colors = jet(np.arange(256))[ 3] to recolorize the heatmap. jet_heatmap = jet_colors[heatmap] jet_heatmap = keras.utils.array_to_img(jet_heatmap) jet_heatmap = jet_heatmap.resize((img.shape img.shape)) jet_heatmap = keras.utils.img_to_array(jet_heatmap) superimposed_img = jet_heatmap * 0.4 + img superimposed_img = keras.utils.array_to_img(superimposed_img) save_path = "elephant_cam.jpg" save the superimposed superimposed_img.save(save_path) image. create an image that contains the recolorized heatmap. superimpose the heatmap and the original image with the heatmap at 40% opacity. this visualization technique answers two important questions why did the network think this image contained an african elephant? where is the african elephant located in the picture? in particular it s interesting to note that the ears of the elephant calf are strongly acti.vated this is probably how the network can tell the difference between african and indian elephants. summary there are three essential computer vision tasks you can do with deep learning image classification image segmentation and object detection. following modern convnet architecture best practices will help you get the most out of your models. some of these best practices include using residual connections batch normalization and depthwise separable convolutions. the representations that convnets learn are easy to inspect convnets are the opposite of black boxes! you can generate visualizations of the filters learned by your convnets as well as heatmaps of class activity. this chapter covers examples of machine learning tasks that involve timeseries data understanding recurrent neural networks (rnns) applying rnns to a temperature-forecasting example advanced rnn usage patterns 10.1 different kinds of timeseries tasks a timeseries can be any data obtained via measurements at regular intervals like the daily price of a stock the hourly electricity consumption of a city or the weekly sales of a store. timeseries are everywhere whether we re looking at natu.ral phenomena (like seismic activity the evolution of fish populations in a river or the weather at a location) or human activity patterns (like visitors to a website a country s gdp or credit card transactions). unlike the types of data you ve encountered so far working with timeseries involves understanding the dynamics of a system its periodic cycles how it trends over time its regular regime and its sudden spikes. 280 a temperature-forecasting example by far the most common timeseries-related task is forecasting predicting what will happen next in a series. forecast electricity consumption a few hours in advance so you can anticipate demand forecast revenue a few months in advance so you can plan your budget forecast the weather a few days in advance so you can plan your sched.ule. forecasting is what this chapter focuses on. but there s actually a wide range of other things you can do with timeseries classification assign one or more categorical labels to a timeseries. for instance given the timeseries of the activity of a visitor on a website classify whether the visitor is a bot or a human. event detection identify the occurrence of a specific expected event within a continuous data stream. a particularly useful application is hotword detec.tion where a model monitors an audio stream and detects utterances like ok google or hey alexa. anomaly detection detect anything unusual happening within a continuous datastream. unusual activity on your corporate network? might be an attacker. unusual readings on a manufacturing line? time for a human to go take a look. anomaly detection is typically done via unsupervised learning because you often don t know what kind of anomaly you re looking for so you can t train on specific anomaly examples. when working with timeseries you ll encounter a wide range of domain-specific data-representation techniques. for instance you have likely already heard about the fou.rier transform which consists of expressing a series of values in terms of a superposition of waves of different frequencies. the fourier transform can be highly valuable when preprocessing any data that is primarily characterized by its cycles and oscillations (like sound the vibrations of the frame of a skyscraper or your brain waves). in the context of deep learning fourier analysis (or the related mel-frequency analysis) and other domain-specific representations can be useful as a form of feature engineering a way to prepare data before training a model on it to make the job of the model eas.ier. however we won t cover these techniques in these pages we will instead focus on the modeling part. in this chapter you ll learn about recurrent neural networks (rnns) and how to apply them to timeseries forecasting. 10.2 a temperature-forecasting example throughout this chapter all of our code examples will target a single problem pre.dicting the temperature 24 hours in the future given a timeseries of hourly measure.ments of quantities such as atmospheric pressure and humidity recorded over the recent past by a set of sensors on the roof of a building. as you will see it s a fairly chal.lenging problem! we ll use this temperature-forecasting task to highlight what makes timeseries data fundamentally different from the kinds of datasets you ve encountered so far. you ll see that densely connected networks and convolutional networks aren t well-equipped to deal with this kind of dataset while a different kind of machine learning tech.nique recurrent neural networks (rnns) really shines on this type of problem. we ll work with a weather timeseries dataset recorded at the weather station at the max planck institute for biogeochemistry in jena germany.1 in this dataset 14 differ.ent quantities (such as temperature pressure humidity wind direction and so on) were recorded every 10 minutes over several years. the original data goes back to 2003 but the subset of the data we ll download is limited to 2009 2016. let s start by downloading and uncompressing the data !wget https//s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip !unzip jena_climate_2009_2016.csv.zip now let s look at the data. import os fname = os.path.join("jena_climate_2009_2016.csv") with open(fname) as f data = f.read() lines = data.split("\n") header = lines.split("") lines = lines print(header) print(len(lines)) this outputs a count of 420551 lines of data (each line is a timestep a record of a date and 14 weather-related values) as well as the following header ["date time" "p (mbar)" "t (degc)" "tpot (k)" "tdew (degc)" "rh (%)" "vpmax (mbar)" "vpact (mbar)" "vpdef (mbar)" "sh (g/kg)" "h2oc (mmol/mol)" "rho (g/m**3)" "wv (m/s)" "max. wv (m/s)" "wd (deg)"] 1 adam erickson and olaf kolle www.bgc-jena.mpg.de/wetter. a temperature-forecasting example now convert all 420551 lines of data into numpy arrays one array for the tempera.ture (in degrees celsius) and another one for the rest of the data the features we will use to predict future temperatures. note that we discard the date time column. import numpy as np temperature = np.zeros((len(lines))) raw_data = np.zeros((len(lines) len(header) -1)) for i line in enumerate(lines) we store column 1 in the values = [float(x) for x in line.split("")] temperature array. temperature[i] = values raw_data[i ] = values[] we store all columns (including the temperature) in the raw_data array. figure 10.1 shows the plot of temperature (in degrees celsius) over time. on this plot you can clearly see the yearly periodicity of temperature the data spans 8 years. from matplotlib import pyplot as plt plt.plot(range(len(temperature)) temperature) figure 10.2 shows a more narrow plot of the first 10 days of temperature data. because the data is recorded every 10 minutes you get 24 ! 6 = 144 data points per day. plt.plot(range(1440) temperature) on this plot you can see daily periodicity especially for the last 4 days. also note that this 10-day period must be coming from a fairly cold winter month. always look for periodicity in your data periodicity over multiple timescales is an important and very common property of timeseries data. whether you re looking at the weather mall parking occupancy traf.fic to a website sales of a grocery store or steps logged in a fitness tracker you ll see daily cycles and yearly cycles (human-generated data also tends to feature weekly cycles). when exploring your data make sure to look for these patterns. with our dataset if you were trying to predict average temperature for the next month given a few months of past data the problem would be easy due to the reliable year-scale periodicity of the data. but looking at the data over a scale of days the temperature looks a lot more chaotic. is this timeseries predictable at a daily scale? let s find out. in all our experiments we ll use the first 50% of the data for training the follow.ing 25% for validation and the last 25% for testing. when working with timeseries data it s important to use validation and test data that is more recent than the train.ing data because you re trying to predict the future given the past not the reverse and your validation/test splits should reflect that. some problems happen to be con.siderably simpler if you reverse the time axis! >>> num_train_samples = int(0.5 * len(raw_data)) >>> num_val_samples = int(0.25 * len(raw_data)) >>> num_test_samples = len(raw_data) - num_train_samples - num_val_samples a temperature-forecasting example >>> print("num_train_samples" num_train_samples) >>> print("num_val_samples" num_val_samples) >>> print("num_test_samples" num_test_samples) num_train_samples 210225 num_val_samples 105112 num_test_samples 105114 10.2.1 preparing the data the exact formulation of the problem will be as follows given data covering the previ.ous five days and sampled once per hour can we predict the temperature in 24 hours? first let s preprocess the data to a format a neural network can ingest. this is easy the data is already numerical so you don t need to do any vectorization. but each timeseries in the data is on a different scale (for example atmospheric pres.sure measured in mbar is around 1000 while h2oc measured in millimoles per mole is around 3). we ll normalize each timeseries independently so that they all take small values on a similar scale. we re going to use the first 210225 timesteps as training data so we ll compute the mean and standard deviation only on this frac.tion of the data. mean = raw_data[num_train_samples].mean(axis=0) raw_data -= mean std = raw_data[num_train_samples].std(axis=0) raw_data /= std next let s create a dataset object that yields batches of data from the past five days along with a target temperature 24 hours in the future. because the samples in the dataset are highly redundant (sample n and sample n +1 will have most of their time-steps in common) it would be wasteful to explicitly allocate memory for every sample. instead we ll generate the samples on the fly while only keeping in memory the origi.nal raw_data and temperature arrays and nothing more. we could easily write a python generator to do this but there s a built-in dataset utility in keras that does just that (timeseries_dataset_from_array()) so we can save ourselves some work by using it. you can generally use it for any kind of timeseries forecasting task. understanding timeseries_dataset_from_array() to understand what timeseries_dataset_from_array() does let s look at a sim.ple example. the general idea is that you provide an array of timeseries data (the data argument) and timeseries_dataset_from_array() gives you windows extracted from the original timeseries (we ll call them sequences ). for example if you use data = [0 1 2 3 4 5 6] and sequence_length=3 then timeseries_dataset_from_array() will generate the following samples [0 1 2] [1 2 3] [2 3 4] [3 4 5] [4 5 6]. (continued) you can also pass a targets argument (an array) to timeseries_dataset_ from_array(). the first entry of the targets array should match the desired tar.get for the first sequence that will be generated from the data array. so if you re doing timeseries forecasting targets should be the same array as data offset by some amount. for instance with data =[0123456 ] and sequence_length=3 you could create a dataset to predict the next step in the series by passing targets=[3456 ]. let s try it generate an array import numpy as np of sorted integers from tensorflow import keras from 0 to 9. int_sequence = np.arange(10) dummy_dataset = keras.utils.timeseries_dataset_from_array( data=int_sequence[-3] targets=int_sequence the sequences we generate will be sampled from [0 1 2 3 4 5 6]. the target for the sequence that sequence_length=3 starts at data[n] will be data[n + 3]. batch_size=2 ) the sequences will be 3 steps long. the sequences will be for inputs targets in dummy_dataset batched in batches of size 2. for i in range(inputs.shape) print([int(x) for x in inputs[i]] int(targets[i])) this bit of code prints the following results [0 1 2] 3 [1 2 3] 4 [2 3 4] 5 [3 4 5] 6 [4 5 6] 7 we ll use timeseries_dataset_from_array() to instantiate three datasets one for training one for validation and one for testing. we ll use the following parameter values sampling_rate = 6 observations will be sampled at one data point per hour we will only keep one data point out of 6. sequence_length =120 observations will go back 5 days (120 hours). delay=sampling_rate*(sequence_length+24-1) the target for a sequence will be the temperature 24 hours after the end of the sequence. when making the training dataset we ll pass start_index = 0 and end_index = num_train_samples to only use the first 50% of the data. for the validation dataset we ll pass start_index = num_train_samples and end_index = num_train_samples + num_val_samples to use the next 25% of the data. finally for the test dataset we ll pass start_index =num_train_samples +num_val_samples to use the remaining samples. a temperature-forecasting example sampling_rate = 6 sequence_length = 120 delay = sampling_rate * (sequence_length + 24 -1) batch_size = 256 train_dataset = keras.utils.timeseries_dataset_from_array( raw_data[-delay] targets=temperature[delay] sampling_rate=sampling_rate sequence_length=sequence_length shuffle=true batch_size=batch_size start_index=0 end_index=num_train_samples) val_dataset = keras.utils.timeseries_dataset_from_array( raw_data[-delay] targets=temperature[delay] sampling_rate=sampling_rate sequence_length=sequence_length shuffle=true batch_size=batch_size start_index=num_train_samples end_index=num_train_samples + num_val_samples) test_dataset = keras.utils.timeseries_dataset_from_array( raw_data[-delay] targets=temperature[delay] sampling_rate=sampling_rate sequence_length=sequence_length shuffle=true batch_size=batch_size start_index=num_train_samples + num_val_samples) each dataset yields a tuple (samples targets) where samples is a batch of 256 sam.ples each containing 120 consecutive hours of input data and targets is the corre.sponding array of 256 target temperatures. note that the samples are randomly shuffled so two consecutive sequences in a batch (like samples and samples) aren t necessarily temporally close. >>> for samples targets in train_dataset >>> print("samples shape" samples.shape) >>> print("targets shape" targets.shape) >>> break samples shape (256 120 14) targets shape (256) 10.2.2 a common-sense non-machine learning baseline before we start using black-box deep learning models to solve the temperature-prediction problem let s try a simple common-sense approach. it will serve as a sanity check and it will establish a baseline that we ll have to beat in order to demonstrate the usefulness of more-advanced machine learning models. such common-sense base.lines can be useful when you re approaching a new problem for which there is no known solution (yet). a classic example is that of unbalanced classification tasks where some classes are much more common than others. if your dataset contains 90% instances of class a and 10% instances of class b then a common-sense approach to the classification task is to always predict a when presented with a new sample. such a classifier is 90% accurate overall and any learning-based approach should therefore beat this 90% score in order to demonstrate usefulness. sometimes such elementary baselines can prove surprisingly hard to beat. in this case the temperature timeseries can safely be assumed to be continuous (the temperatures tomorrow are likely to be close to the temperatures today) as well as periodical with a daily period. thus a common-sense approach is to always predict that the temperature 24 hours from now will be equal to the temperature right now. let s evaluate this approach using the mean absolute error (mae) met.ric defined as follows np.mean(np.abs(preds -targets)) here s the evaluation loop. def evaluate_naive_method(dataset) total_abs_err = 0. samples_seen = 0 for samples targets in dataset preds = samples[ -1 1] * std + mean total_abs_err += np.sum(np.abs(preds - targets)) samples_seen += samples.shape return total_abs_err / samples_seen print(f"validation mae {evaluate_naive_method(val_dataset).2f}") print(f"test mae {evaluate_naive_method(test_dataset).2f}") the temperature feature is at column 1 so samples[ -1 1] is the last temperature measurement in the input sequence. recall that we normalized our features so to retrieve a temperature in degrees celsius we need to un-normalize it by multiplying it by the standard deviation and adding back the mean. this common-sense baseline achieves a validation mae of 2.44 degrees celsius and a test mae of 2.62 degrees celsius. so if you always assume that the temperature 24 hours in the future will be the same as it is now you will be off by two and a half degrees on average. it s not too bad but you probably won t launch a weather fore.casting service based on this heuristic. now the game is to use your knowledge of deep learning to do better. a temperature-forecasting example 10.2.3 let s try a basic machine learning model in the same way that it s useful to establish a common-sense baseline before trying machine learning approaches it s useful to try simple cheap machine learning mod.els (such as small densely connected networks) before looking into complicated and computationally expensive models such as rnns. this is the best way to make sure any further complexity you throw at the problem is legitimate and delivers real benefits. the following listing shows a fully connected model that starts by flattening the data and then runs it through two dense layers. note the lack of an activation function on the last dense layer which is typical for a regression problem. we use mean squared error (mse) as the loss rather than mae because unlike mae it s smooth around zero which is a useful property for gradient descent. we will monitor mae by adding it as a metric in compile(). from tensorflow import keras from tensorflow.keras import layers inputs = keras.input(shape=(sequence_length raw_data.shape[-1])) x = layers.flatten()(inputs) x = layers.dense(16 activation="relu")(x) outputs = layers.dense(1)(x) model = keras.model(inputs outputs) we use a callback to save the best- callbacks = [ performing model. keras.callbacks.modelcheckpoint("jena_dense.keras" save_best_only=true) ] model.compile(optimizer="rmsprop" loss="mse" metrics=["mae"]) history = model.fit(train_dataset epochs=10 validation_data=val_dataset callbacks=callbacks) reload the best model and evaluate it on model = keras.models.load_model("jena_dense.keras") the test data. print(f"test mae {model.evaluate(test_dataset).2f}") let s display the loss curves for validation and training (see figure 10.3). import matplotlib.pyplot as plt loss = history.history["mae"] val_loss = history.history["val_mae"] epochs = range(1 len(loss) + 1) plt.figure() plt.plot(epochs loss "bo" label="training mae") plt.plot(epochs val_loss "b" label="validation mae") plt.title("training and validation mae") plt.legend() plt.show() some of the validation losses are close to the no-learning baseline but not reliably. this goes to show the merit of having this baseline in the first place it turns out to be not easy to outperform. your common sense contains a lot of valuable information to which a machine learning model doesn t have access. you may wonder if a simple well-performing model exists to go from the data to the targets (the common-sense baseline) why doesn t the model you re training find it and improve on it? well the space of models in which you re searching for a solution that is your hypothesis space is the space of all possible two-layer networks with the configuration you defined. the common-sense heuristic is just one model among mil.lions that can be represented in this space. it s like looking for a needle in a haystack. just because a good solution technically exists in your hypothesis space doesn t mean you ll be able to find it via gradient descent. that s a pretty significant limitation of machine learning in general unless the learning algorithm is hardcoded to look for a specific kind of simple model it can sometimes fail to find a simple solution to a simple problem. that s why leveraging good feature engineering and relevant architecture priors is essential you need to precisely tell your model what it should be looking for. 10.2.4 let s try a 1d convolutional model speaking of leveraging the right architecture priors since our input sequences fea.ture daily cycles perhaps a convolutional model could work. a temporal convnet could reuse the same representations across different days much like a spatial conv.net can reuse the same representations across different locations in an image. you already know about the conv2d and separableconv2d layers which see their inputs through small windows that swipe across 2d grids. there are also 1d and even a temperature-forecasting example 3d versions of these layers conv1d separableconv1d and conv3d.2 the conv1d layer relies on 1d windows that slide across input sequences and the conv3d layer relies on cubic windows that slide across input volumes. you can thus build 1d convnets strictly analogous to 2d convnets. they re a great fit for any sequence data that follows the translation invariance assumption (meaning that if you slide a window over the sequence the content of the window should follow the same properties independently of the location of the window). let s try one on our temperature-forecasting problem. we ll pick an initial window length of 24 so that we look at 24 hours of data at a time (one cycle). as we downsam.ple the sequences (via maxpooling1d layers) we ll reduce the window size accordingly inputs = keras.input(shape=(sequence_length raw_data.shape[-1])) x = layers.conv1d(8 24 activation="relu")(inputs) x = layers.maxpooling1d(2)(x) x = layers.conv1d(8 12 activation="relu")(x) x = layers.maxpooling1d(2)(x) x = layers.conv1d(8 6 activation="relu")(x) x = layers.globalaveragepooling1d()(x) outputs = layers.dense(1)(x) model = keras.model(inputs outputs) callbacks = [ keras.callbacks.modelcheckpoint("jena_conv.keras" save_best_only=true) ] model.compile(optimizer="rmsprop" loss="mse" metrics=["mae"]) history = model.fit(train_dataset epochs=10 validation_data=val_dataset callbacks=callbacks) model = keras.models.load_model("jena_conv.keras") print(f"test mae {model.evaluate(test_dataset).2f}") we get the training and validation curves shown in figure 10.4. as it turns out this model performs even worse than the densely connected one only achieving a validation mae of about 2.9 degrees far from the common-sense baseline. what went wrong here? two things first weather data doesn t quite respect the translation invariance assumption. while the data does feature daily cycles data from a morning follows different properties than data from an evening or from the middle of the night. weather data is only translation-invariant for a very specific timescale. second order in our data matters a lot. the recent past is far more informa.tive for predicting the next day s temperature than data from five days ago. a 1d convnet is not able to leverage this fact. in particular our max pooling and global average pooling layers are largely destroying order information. 2 note that there isn t a separableconv3d layer not for any theoretical reason but simply because i haven t implemented it. 10.2.5 a first recurrent baseline neither the fully connected approach nor the convolutional approach did well but that doesn t mean machine learning isn t applicable to this problem. the densely con.nected approach first flattened the timeseries which removed the notion of time from the input data. the convolutional approach treated every segment of the data in the same way even applying pooling which destroyed order information. let s instead look at the data as what it is a sequence where causality and order matter. there s a family of neural network architectures designed specifically for this use case recurrent neural networks. among them the long short term memory (lstm) layer has long been very popular. we ll see in a minute how these models work but let s start by giving the lstm layer a try. inputs = keras.input(shape=(sequence_length raw_data.shape[-1])) x = layers.lstm(16)(inputs) outputs = layers.dense(1)(x) model = keras.model(inputs outputs) callbacks = [ keras.callbacks.modelcheckpoint("jena_lstm.keras" save_best_only=true) ] model.compile(optimizer="rmsprop" loss="mse" metrics=["mae"]) history = model.fit(train_dataset epochs=10 validation_data=val_dataset callbacks=callbacks) understanding recurrent neural networks model = keras.models.load_model("jena_lstm.keras") print(f"test mae {model.evaluate(test_dataset).2f}") figure 10.5 shows the results. much better! we achieve a validation mae as low as 2.36 degrees and a test mae of 2.55 degrees. the lstm-based model can finally beat the common-sense baseline (albeit just by a bit for now) demonstrating the value of machine learning on this task. but why did the lstm model perform markedly better than the densely connected one or the convnet? and how can we further refine the model? to answer this let s take a closer look at recurrent neural networks. 10.3 understanding recurrent neural networks a major characteristic of all neural networks you ve seen so far such as densely con.nected networks and convnets is that they have no memory. each input shown to them is processed independently with no state kept between inputs. with such net.works in order to process a sequence or a temporal series of data points you have to show the entire sequence to the network at once turn it into a single data point. for instance this is what we did in the densely connected network example we flattened our five days of data into a single large vector and processed it in one go. such net.works are called feedforward networks. in contrast as you re reading the present sentence you re processing it word by word or rather eye saccade by eye saccade while keeping memories of what came before this gives you a fluid representation of the meaning conveyed by this sentence. biological intelligence processes information incrementally while maintaining an internal model of what it s processing built from past information and constantly updated as new information comes in. a recurrent neural network (rnn) adopts the same princi.ple albeit in an extremely simplified version it processes sequences by iterating through the sequence elements and maintaining a state that contains information relative to what it has seen so far. in effect an rnn is a type of neural net.work that has an internal loop (see figure 10.6). the state of the rnn is reset between processing two dif.ferent independent sequences (such as two samples in a figure 10.6 a recurrent network a network with batch) so you still consider one sequence to be a single data a loop point a single input to the network. what changes is that this data point is no longer processed in a single step rather the network internally loops over sequence elements. to make these notions of loop and state clear let s implement the forward pass of a toy rnn. this rnn takes as input a sequence of vectors which we ll encode as a rank-2 tensor of size (timesteps input_features). it loops over timesteps and at each time-step it considers its current state at t and the input at t (of shape (input_features) and combines them to obtain the output at t. we ll then set the state for the next step to be this previous output. for the first timestep the previous output isn t defined hence there is no current state. so we ll initialize the state as an all-zero vector called the initial state of the network. in pseudocode this is the rnn. the state at t state_t = 0 iterates over for input_t in input_sequence sequence elements output_t = f(input_t state_t) state_t = output_t the previous output becomes the state for the next iteration. you can even flesh out the function f the transformation of the input and state into an output will be parameterized by two matrices w and u and a bias vector. it s similar to the transformation operated by a densely connected layer in a feedforward network. state_t = 0 for input_t in input_sequence output_t = activation(dot(w input_t) + dot(u state_t) + b) state_t = output_t to make these notions absolutely unambiguous let s write a naive numpy implemen.tation of the forward pass of the simple rnn. understanding recurrent neural networks input data random noise number of timesteps for the sake of the example in the input sequence import numpy as np dimensionality of the dimensionality timesteps = 100 input feature space of the output input_features = 32 feature space initial state output_features = 64 an all-zero inputs = np.random.random((timesteps input_features)) vector state_t = np.zeros((output_features)) w=np.random.random((output_featuresinput_features)) creates random u=np.random.random((output_featuresoutput_features)) weight matrices b=np.random.random((output_features)) successive_outputs = [] stores this for input_t in inputs output in output_t = np.tanh(np.dot(w input_t) + np.dot(u state_t) + b) a list successive_outputs.append(output_t) state_t = output_t final_output_sequence = np.stack(successive_outputs axis=0) combines the input with the current state (the previous the final output is a rank-2 output) to obtain the current output. we use tanh to add tensor of shape (timesteps non-linearity (we could use any other activation function). output_features). input_t is a vector of shape updates the state of the (input_features). network for the next timestep that s easy enough. in summary an rnn is a for loop that reuses quantities com.puted during the previous iteration of the loop nothing more. of course there are many different rnns fitting this definition that you could build this example is one of the simplest rnn formulations. rnns are characterized by their step function such as the following function in this case (see figure 10.7). output_t = np.tanh(np.dot(w input_t) + np.dot(u state_t) + b) output t-1 output t output t+1 output_t = activation( wo input_t + uo state_t + bo) ... state t state t+1 input t-1 input t input t+1 note in this example the final output is a rank-2 tensor of shape (time-steps output_features) where each timestep is the output of the loop at time t. each timestep t in the output tensor contains information about time-steps 0 to t in the input sequence about the entire past. for this reason in many cases you don t need this full sequence of outputs you just need the last output (output_t at the end of the loop) because it already contains information about the entire sequence. 10.3.1 a recurrent layer in keras the process we just naively implemented in numpy corresponds to an actual keras layer the simplernn layer. there is one minor difference simplernn processes batches of sequences like all other keras layers not a single sequence as in the numpy example. this means it takes inputs of shape (batch_size timesteps input_features) rather than (timesteps input_features). when specifying the shape argument of the initial input() note that you can set the timesteps entry to none which enables your network to process sequences of arbitrary length. num_features = 14 inputs = keras.input(shape=(none num_features)) outputs = layers.simplernn(16)(inputs) this is especially useful if your model is meant to process sequences of variable length. however if all of your sequences have the same length i recommend specifying a complete input shape since it enables model.summary() to display output length information which is always nice and it can unlock some performance optimizations (see the note on rnn runtime performance sidebar a little later in this chapter). all recurrent layers in keras (simplernn lstm and gru) can be run in two differ.ent modes they can return either full sequences of successive outputs for each time-step (a rank-3 tensor of shape (batch_size timesteps output_features)) or return only the last output for each input sequence (a rank-2 tensor of shape (batch_ size output_features)). these two modes are controlled by the return_sequences constructor argument. let s look at an example that uses simplernn and returns only the output at the last timestep. >>> num_features = 14 >>> steps = 120 >>> inputs = keras.input(shape=(steps num_features)) >>> outputs = layers.simplernn(16 return_sequences=false)(inputs) >>> print(outputs.shape) note that (none 16) return_sequences=false is the default. understanding recurrent neural networks the following example returns the full state sequence. >>> num_features = 14 >>> steps = 120 >>> inputs = keras.input(shape=(steps num_features)) >>> outputs = layers.simplernn(16 return_sequences=true)(inputs) >>> print(outputs.shape) (120 16) it s sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. in such a setup you have to get all of the intermediate layers to return a full sequence of outputs. inputs = keras.input(shape=(steps num_features)) x = layers.simplernn(16 return_sequences=true)(inputs) x = layers.simplernn(16 return_sequences=true)(x) outputs = layers.simplernn(16)(x) in practice you ll rarely work with the simplernn layer. it s generally too simplistic to be of real use. in particular simplernn has a major issue although it should theoretically be able to retain at time t information about inputs seen many timesteps before such long-term dependencies prove impossible to learn in practice. this is due to the vanish.ing gradient problem an effect that is similar to what is observed with non-recurrent net.works (feedforward networks) that are many layers deep as you keep adding layers to a network the network eventually becomes untrainable. the theoretical reasons for this effect were studied by hochreiter schmidhuber and bengio in the early 1990s.3 thankfully simplernn isn t the only recurrent layer available in keras. there are two others lstm and gru which were designed to address these issues. let s consider the lstm layer. the underlying long short-term memory (lstm) algorithm was developed by hochreiter and schmidhuber in 19974 it was the culmi. nation of their research on the vanishing gradient problem. this layer is a variant of the simplernn layer you already know about it adds a way to carry information across many timesteps. imagine a conveyor belt running parallel to the sequence you re processing. information from the sequence can jump onto the conveyor belt at any point be transported to a later timestep and jump off intact when you need it. this is essentially what lstm does it saves information for later thus preventing older signals from gradually vanishing during processing. this should 3 see for example yoshua bengio patrice simard and paolo frasconi learning long-term dependencies with gradient descent is difficult ieee transactions on neural networks 5 no. 2 (1994). 4 sepp hochreiter and j rgen schmidhuber long short-term memory neural computation 9 no. 8 (1997). remind you of residual connections which you learned about in chapter 9 it s pretty much the same idea. to understand this process in detail let s start from the simplernn cell (see fig.ure 10.8). because you ll have a lot of weight matrices index the w and u matrices in the cell with the letter o (wo and uo) for output. output t-1 output t output t+1 output_t = activation( wo input_t + uo state_t + bo) ... state t state t+1 input t-1 input t input t+1 let s add to this picture an additional data flow that carries information across time-steps. call its values at different timesteps c_t where c stands for carry. this informa.tion will have the following impact on the cell it will be combined with the input connection and the recurrent connection (via a dense transformation a dot product with a weight matrix followed by a bias add and the application of an activation func.tion) and it will affect the state being sent to the next timestep (via an activation func.tion and a multiplication operation). conceptually the carry dataflow is a way to modulate the next output and the next state (see figure 10.9). simple so far. input t-1 input t input t+1 understanding recurrent neural networks now the subtlety the way the next value of the carry dataflow is computed. it involves three distinct transformations. all three have the form of a simplernn cell y=activation(dot(state_tu)+dot(input_t w)+b) but all three transformations have their own weight matrices which we ll index with the letters i f and k. here s what we have so far (it may seem a bit arbitrary but bear with me). output_t = activation(dot(state_t uo) + dot(input_t wo) + dot(c_t vo) + bo) i_t = activation(dot(state_t ui) + dot(input_t wi) + bi) f_t = activation(dot(state_t uf) + dot(input_t wf) + bf) k_t = activation(dot(state_t uk) + dot(input_t wk) + bk) we obtain the new carry state (the next c_t) by combining i_t f_t and k_t. c_t+1 = i_t * k_t + c_t * f_t add this as shown in figure 10.10 and that s it. not so complicated merely a tad complex. input t-1 input t input t+1 if you want to get philosophical you can interpret what each of these operations is meant to do. for instance you can say that multiplying c_t and f_t is a way to deliber.ately forget irrelevant information in the carry dataflow. meanwhile i_t and k_t pro.vide information about the present updating the carry track with new information. but at the end of the day these interpretations don t mean much because what these operations actually do is determined by the contents of the weights parameterizing them and the weights are learned in an end-to-end fashion starting over with each training round making it impossible to credit this or that operation with a specific purpose. the specification of an rnn cell (as just described) determines your hypoth.esis space the space in which you ll search for a good model configuration during training but it doesn t determine what the cell does that is up to the cell weights. the same cell with different weights can be doing very different things. so the combi.nation of operations making up an rnn cell is better interpreted as a set of constraints on your search not as a design in an engineering sense. arguably the choice of such constraints the question of how to implement rnn cells is better left to optimization algorithms (like genetic algorithms or reinforcement-learning processes) than to human engineers. in the future that s how we ll build our models. in summary you don t need to understand anything about the specific archi.tecture of an lstm cell as a human it shouldn t be your job to understand it. just keep in mind what the lstm cell is meant to do allow past information to be rein.jected at a later time thus fighting the vanishing-gradient problem. 10.4 advanced use of recurrent neural networks so far you ve learned what rnns are and how they work what lstm is and why it works better on long sequences than a naive rnn how to use keras rnn layers to process sequence data next we ll review a number of more advanced features of rnns which can help you get the most out of your deep learning sequence models. by the end of the section you ll know most of what there is to know about using recurrent networks with keras. we ll cover the following recurrent dropout this is a variant of dropout used to fight overfitting in recur.rent layers. stacking recurrent layers this increases the representational power of the model (at the cost of higher computational loads). bidirectional recurrent layers these present the same information to a recurrent network in different ways increasing accuracy and mitigating forgetting issues. we ll use these techniques to refine our temperature-forecasting rnn. 10.4.1 using recurrent dropout to fight overfitting let s go back to the lstm-based model we used in section 10.2.5 our first model able to beat the common-sense baseline. if you look at the training and validation curves (figure 10.5) it s evident that the model is quickly overfitting despite only hav.ing very few units the training and validation losses start to diverge considerably after a few epochs. you re already familiar with a classic technique for fighting this phe.nomenon dropout which randomly zeros out input units of a layer in order to break advanced use of recurrent neural networks happenstance correlations in the training data that the layer is exposed to. but how to correctly apply dropout in recurrent networks isn t a trivial question. it has long been known that applying dropout before a recurrent layer hinders learning rather than helping with regularization. in 2016 yarin gal as part of his phd thesis on bayesian deep learning5 determined the proper way to use dropout with a recurrent network the same dropout mask (the same pattern of dropped units) should be applied at every timestep instead of using a dropout mask that varies ran.domly from timestep to timestep. what s more in order to regularize the representa.tions formed by the recurrent gates of layers such as gru and lstm a temporally constant dropout mask should be applied to the inner recurrent activations of the layer (a recurrent dropout mask). using the same dropout mask at every timestep allows the network to properly propagate its learning error through time a tempo.rally random dropout mask would disrupt this error signal and be harmful to the learning process. yarin gal did his research using keras and helped build this mechanism directly into keras recurrent layers. every recurrent layer in keras has two dropout-related arguments dropout a float specifying the dropout rate for input units of the layer and recurrent_dropout specifying the dropout rate of the recurrent units. let s add recurrent dropout to the lstm layer of our first lstm example and see how doing so impacts overfitting. thanks to dropout we won t need to rely as much on network size for regulariza.tion so we ll use an lstm layer with twice as many units which should hopefully be more expressive (without dropout this network would have started overfitting right away try it). because networks being regularized with dropout always take much lon.ger to fully converge we ll train the model for five times as many epochs. inputs = keras.input(shape=(sequence_length raw_data.shape[-1])) x = layers.lstm(32 recurrent_dropout=0.25)(inputs) x = layers.dropout(0.5)(x) to regularize the dense layer outputs = layers.dense(1)(x) we also add a dropout layer model = keras.model(inputs outputs) after the lstm. callbacks = [ keras.callbacks.modelcheckpoint("jena_lstm_dropout.keras" save_best_only=true) ] model.compile(optimizer="rmsprop" loss="mse" metrics=["mae"]) history = model.fit(train_dataset epochs=50 validation_data=val_dataset callbacks=callbacks) 5 see yarin gal uncertainty in deep learning phd thesis (2016) http//mng.bz/wbq1. figure 10.11 shows the results. success! we re no longer overfitting during the first 20 epochs. we achieve a validation mae as low as 2.27 degrees (7% improvement over the no-learning baseline) and a test mae of 2.45 degrees (6.5% improvement over the baseline). not too bad. rnn runtime performance recurrent models with very few parameters like the ones in this chapter tend to be significantly faster on a multicore cpu than on gpu because they only involve small matrix multiplications and the chain of multiplications is not well paralleliz.able due to the presence of a for loop. but larger rnns can greatly benefit from a gpu runtime. when using a keras lstm or gru layer on gpu with default keyword arguments your layer will be leveraging a cudnn kernel a highly optimized low-level nvidia-provided implementation of the underlying algorithm (i mentioned these in the previous chap.ter). as usual cudnn kernels are a mixed blessing they re fast but inflexible if you try to do anything not supported by the default kernel you will suffer a dramatic slow.down which more or less forces you to stick to what nvidia happens to provide. for instance recurrent dropout isn t supported by the lstm and gru cudnn kernels so adding it to your layers forces the runtime to fall back to the regular tensorflow imple.mentation which is generally two to five times slower on gpu (even though its com.putational cost is the same). as a way to speed up your rnn layer when you can t use cudnn you can try unrolling it. unrolling a for loop consists of removing the loop and simply inlining its content n times. in the case of the for loop of an rnn unrolling can help tensorflow opti.mize the underlying computation graph. however it will also considerably increase advanced use of recurrent neural networks the memory consumption of your rnn as such it s only viable for relatively small sequences (around 100 steps or fewer). also note that you can only do this if the number of timesteps in the data is known in advance by the model (that is to say if you pass a shape without any none entries to your initial input()). it works like this sequence_length cannot be none. inputs = keras.input(shape=(sequence_length num_features)) x = layers.lstm(32 recurrent_dropout=0.2 unroll=true)(inputs) pass unroll=true to enable unrolling. 10.4.2 stacking recurrent layers because you re no longer overfitting but seem to have hit a performance bottleneck you should consider increasing the capacity and expressive power of the network. recall the description of the universal machine learning workflow it s generally a good idea to increase the capacity of your model until overfitting becomes the primary obstacle (assuming you re already taking basic steps to mitigate overfitting such as using drop.out). as long as you aren t overfitting too badly you re likely under capacity. increasing network capacity is typically done by increasing the number of units in the layers or adding more layers. recurrent layer stacking is a classic way to build more-powerful recurrent networks for instance not too long ago the google trans.late algorithm was powered by a stack of seven large lstm layers that s huge. to stack recurrent layers on top of each other in keras all intermediate layers should return their full sequence of outputs (a rank-3 tensor) rather than their output at the last timestep. as you ve already learned this is done by specifying return_sequences=true. in the following example we ll try a stack of two dropout-regularized recurrent lay.ers. for a change we ll use gated recurrent unit (gru) layers instead of lstm. gru is very similar to lstm you can think of it as a slightly simpler streamlined version of the lstm architecture. it was introduced in 2014 by cho et al. when recurrent networks were just starting to gain interest anew in the then-tiny research community.6 inputs = keras.input(shape=(sequence_length raw_data.shape[-1])) x = layers.gru(32 recurrent_dropout=0.5 return_sequences=true)(inputs) x = layers.gru(32 recurrent_dropout=0.5)(x) x = layers.dropout(0.5)(x) outputs = layers.dense(1)(x) model = keras.model(inputs outputs) callbacks = [ keras.callbacks.modelcheckpoint("jena_stacked_gru_dropout.keras" save_best_only=true) ] 6 see cho et al. on the properties of neural machine translation encoder-decoder approaches (2014) https//arxiv.org/abs/1409.1259. model.compile(optimizer="rmsprop" loss="mse" metrics=["mae"]) history = model.fit(train_dataset epochs=50 validation_data=val_dataset callbacks=callbacks) model = keras.models.load_model("jena_stacked_gru_dropout.keras") print(f"test mae {model.evaluate(test_dataset).2f}") figure 10.12 shows the results. we achieve a test mae of 2.39 degrees (an 8.8% improvement over the baseline). you can see that the added layer does improve the results a bit though not dramatically. you may be seeing diminishing returns from increasing network capacity at this point. 10.4.3 using bidirectional rnns the last technique we ll look at in this section is the bidirectional rnn. a bidirectional rnn is a common rnn variant that can offer greater performance than a regular rnn on certain tasks. it s frequently used in natural language processing you could call it the swiss army knife of deep learning for natural language processing. rnns are notably order-dependent they process the timesteps of their input sequences in order and shuffling or reversing the timesteps can completely change the representations the rnn extracts from the sequence. this is precisely the reason they perform well on problems where order is meaningful such as the temperature-forecasting problem. a bidirectional rnn exploits the order sensitivity of rnns it advanced use of recurrent neural networks uses two regular rnns such as the gru and lstm layers you re already familiar with each of which processes the input sequence in one direction (chronologically and antichronologically) and then merges their representations. by processing a sequence both ways a bidirectional rnn can catch patterns that may be overlooked by a unidi.rectional rnn. remarkably the fact that the rnn layers in this section have processed sequences in chronological order (with older timesteps first) may have been an arbitrary decision. at least it s a decision we ve made no attempt to question so far. could the rnns have performed well enough if they processed input sequences in antichronological order for instance (with newer timesteps first)? let s try this and see what happens. all you need to do is write a variant of the data generator where the input sequences are reverted along the time dimension (replace the last line with yield samples[ -1 ] targets). training the same lstm-based model that you used in the first experiment in this sec.tion you get the results shown in figure 10.13. the reversed-order lstm strongly underperforms even the common-sense baseline indicating that in this case chronological processing is important to the success of the approach. this makes perfect sense the underlying lstm layer will typically be better at remembering the recent past than the distant past and naturally the more recent weather data points are more predictive than older data points for the problem (that s what makes the common-sense baseline fairly strong). thus the chronological version of the layer is bound to outperform the reversed-order version. however this isn t true for many other problems including natural language intui.tively the importance of a word in understanding a sentence isn t usually dependent on its position in the sentence. on text data reversed-order processing works just as well as chronological processing you can read text backwards just fine (try it!). although word order does matter in understanding language which order you use isn t crucial. importantly an rnn trained on reversed sequences will learn different repre.sentations than one trained on the original sequences much as you would have dif.ferent mental models if time flowed backward in the real world if you lived a life where you died on your first day and were born on your last day. in machine learn.ing representations that are different yet useful are always worth exploiting and the more they differ the better they offer a new angle from which to look at your data capturing aspects of the data that were missed by other approaches and thus they can help boost performance on a task. this is the intuition behind ensembling a con.cept we ll explore in chapter 13. a bidirectional rnn exploits this idea to improve on the performance of chronological-order rnns. it looks at its input sequence both ways (see figure 10.14) obtaining potentially richer representations and capturing patterns that may have been missed by the chronological-order version alone. a b c d e rnn layer works to instantiate a bidirectional rnn in keras you use the bidirectional layer which takes as its first argument a recurrent layer instance. bidirectional creates a second separate instance of this recurrent layer and uses one instance for processing the input sequences in chronological order and the other instance for processing the input sequences in reversed order. you can try it on our temperature-forecasting task. inputs = keras.input(shape=(sequence_length raw_data.shape[-1])) x = layers.bidirectional(layers.lstm(16))(inputs) outputs = layers.dense(1)(x) model = keras.model(inputs outputs) advanced use of recurrent neural networks model.compile(optimizer="rmsprop" loss="mse" metrics=["mae"]) history = model.fit(train_dataset epochs=10 validation_data=val_dataset) you ll find that it doesn t perform as well as the plain lstm layer. it s easy to under.stand why all the predictive capacity must come from the chronological half of the network because the antichronological half is known to be severely underperforming on this task (again because the recent past matters much more than the distant past in this case). at the same time the presence of the antichronological half doubles the network s capacity and causes it to start overfitting much earlier. however bidirectional rnns are a great fit for text data or any other kind of data where order matters yet where which order you use doesn t matter. in fact for a while in 2016 bidirectional lstms were considered the state of the art on many natural lan.guage processing tasks (before the rise of the transformer architecture which you will learn about in the next chapter). 10.4.4 going even further there are many other things you could try in order to improve performance on the temperature-forecasting problem adjust the number of units in each recurrent layer in the stacked setup as well as the amount of dropout. the current choices are largely arbitrary and thus probably suboptimal. adjust the learning rate used by the rmsprop optimizer or try a different optimizer. try using a stack of dense layers as the regressor on top of the recurrent layer instead of a single dense layer. improve the input to the model try using longer or shorter sequences or a dif.ferent sampling rate or start doing feature engineering. as always deep learning is more an art than a science. we can provide guidelines that suggest what is likely to work or not work on a given problem but ultimately every dataset is unique you ll have to evaluate different strategies empirically. there is cur.rently no theory that will tell you in advance precisely what you should do to optimally solve a problem. you must iterate. in my experience improving on the no-learning baseline by about 10% is likely the best you can do with this dataset. this isn t so great but these results make sense while near-future weather is highly predictable if you have access to data from a wide grid of different locations it s not very predictable if you only have measurements from a single location. the evolution of the weather where you are depends on cur.rent weather patterns in surrounding locations. markets and machine learning some readers are bound to want to take the techniques i ve introduced here and try them on the problem of forecasting the future price of securities on the stock market (or currency exchange rates and so on). however markets have very different statis.tical characteristics than natural phenomena such as weather patterns. when it comes to markets past performance is not a good predictor of future returns looking in the rear-view mirror is a bad way to drive. machine learning on the other hand is appli.cable to datasets where the past is a good predictor of the future like weather elec.tricity consumption or foot traffic at a store. always remember that all trading is fundamentally information arbitrage gaining an advantage by leveraging data or insights that other market participants are missing. trying to use well-known machine learning techniques and publicly available data to beat the markets is effectively a dead end since you won t have any information advantage compared to everyone else. you re likely to waste your time and resources with nothing to show for it. summary as you first learned in chapter 5 when approaching a new problem it s good to first establish common-sense baselines for your metric of choice. if you don t have a baseline to beat you can t tell whether you re making real progress. try simple models before expensive ones to make sure the additional expense is justified. sometimes a simple model will turn out to be your best option. when you have data where ordering matters and in particular for timeseries data recurrent networks are a great fit and easily outperform models that first flat.ten the temporal data. the two essential rnn layers available in keras are the lstm layer and the gru layer. to use dropout with recurrent networks you should use a time-constant drop.out mask and recurrent dropout mask. these are built into keras recurrent lay.ers so all you have to do is use the recurrent_dropout arguments of recurrent layers. stacked rnns provide more representational power than a single rnn layer. they re also much more expensive and thus not always worth it. although they offer clear gains on complex problems (such as machine translation) they may not always be relevant to smaller simpler problems. this chapter covers preprocessing text data for machine learning applications bag-of-words approaches and sequence-modeling approaches for text processing the transformer architecture sequence-to-sequence learning 11.1 natural language processing the bird s eye view in computer science we refer to human languages like english or mandarin as natural languages to distinguish them from languages that were designed for machines like assembly lisp or xml. every machine language was designed its starting point was a human engineer writing down a set of formal rules to describe what statements you could make in that language and what they meant. rules came first and people only started using the language once the rule set was complete. with human language it s the reverse usage comes first rules arise later. natural language was shaped by an evolution process much like biological organisms that s what makes it natural. its rules like the grammar of english were formal.ized after the fact and are often ignored or broken by its users. as a result while 309 machine-readable language is highly structured and rigorous using precise syntactic rules to weave together exactly defined concepts from a fixed vocabulary natural lan.guage is messy ambiguous chaotic sprawling and constantly in flux. creating algorithms that can make sense of natural language is a big deal lan.guage and in particular text underpins most of our communications and our cul.tural production. the internet is mostly text. language is how we store almost all of our knowledge. our very thoughts are largely built upon language. however the abil.ity to understand natural language has long eluded machines. some people once naively thought that you could simply write down the rule set of english much like one can write down the rule set of lisp. early attempts to build natural language pro.cessing (nlp) systems were thus made through the lens of applied linguistics. engi.neers and linguists would handcraft complex sets of rules to perform basic machine translation or create simple chatbots like the famous eliza program from the 1960s which used pattern matching to sustain very basic conversation. but language is a rebellious thing it s not easily pliable to formalization. after several decades of effort the capabilities of these systems remained disappointing. handcrafted rules held out as the dominant approach well into the 1990s. but starting in the late 1980s faster computers and greater data availability started making a better alternative viable. when you find yourself building systems that are big piles of ad hoc rules as a clever engineer you re likely to start asking could i use a corpus of data to automate the process of finding these rules? could i search for the rules within some kind of rule space instead of having to come up with them myself? and just like that you ve graduated to doing machine learning. and so in the late 1980s we started seeing machine learning approaches to natural language processing. the earliest ones were based on decision trees the intent was literally to automate the development of the kind of if/then/else rules of previous systems. then statistical approaches started gaining speed starting with logistic regression. over time learned parametric models fully took over and linguistics came to be seen as more of a hin.drance than a useful tool. frederick jelinek an early speech recognition researcher joked in the 1990s every time i fire a linguist the performance of the speech recog.nizer goes up. that s what modern nlp is about using machine learning and large datasets to give computers the ability not to understand language which is a more lofty goal but to ingest a piece of language as input and return something useful like predicting the following what s the topic of this text? (text classification) does this text contain abuse? (content filtering) does this text sound positive or negative? (sentiment analysis) what should be the next word in this incomplete sentence? (language modeling) how would you say this in german? (translation) how would you summarize this article in one paragraph? (summarization) etc. of course keep in mind throughout this chapter that the text-processing models you will train won t possess a human-like understanding of language rather they simply look for statistical regularities in their input data which turns out to be sufficient to perform well on many simple tasks. in much the same way that computer vision is pat.tern recognition applied to pixels nlp is pattern recognition applied to words sen.tences and paragraphs. the toolset of nlp decision trees logistic regression only saw slow evolution from the 1990s to the early 2010s. most of the research focus was on feature engineer.ing. when i won my first nlp competition on kaggle in 2013 my model was you guessed it based on decision trees and logistic regression. however around 2014 2015 things started changing at last. multiple researchers began to investigate the language-understanding capabilities of recurrent neural networks in particular lstm a sequence-processing algorithm from the late 1990s that had stayed under the radar until then. in early 2015 keras made available the first open source easy-to-use implementa.tion of lstm just at the start of a massive wave of renewed interest in recurrent neu.ral networks until then there had only been research code that couldn t be readily reused. then from 2015 to 2017 recurrent neural networks dominated the booming nlp scene. bidirectional lstm models in particular set the state of the art on many important tasks from summarization to question-answering to machine translation. finally around 2017 2018 a new architecture rose to replace rnns the trans.former which you will learn about in the second half of this chapter. transformers unlocked considerable progress across the field in a short period of time and today most nlp systems are based on them. let s dive into the details. this chapter will take you from the very basics to doing machine translation with a transformer. 11.2 preparing text data deep learning models being differentiable functions can only process numeric ten.sors they can t take raw text as input. vectorizing text is the process of transforming text into numeric tensors. text vectorization processes come in many shapes and forms but they all follow the same template (see figure 11.1) first you standardize the text to make it easier to process such as by converting it to lowercase or removing punctuation. you split the text into units (called tokens) such as characters words or groups of words. this is called tokenization. you convert each such token into a numerical vector. this will usually involve first indexing all tokens present in the data. let s review each of these steps. the cat sat on the mat. standardization text standardized text tokens token indices vector encoding of indices 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 figure 11.1 from raw text to vectors 11.2.1 text standardization consider these two sentences sunset came. i was staring at the mexico sky. isnt nature splendid?? sunset came i stared at the m xico sky. isn t nature splendid? they re very similar in fact they re almost identical. yet if you were to convert them to byte strings they would end up with very different representations because i and i are two different characters mexico and m xico are two different words isnt isn t isn t and so on. a machine learning model doesn t know a priori that i and i are the same letter that ҏ is an e with an accent or that staring and stared are two forms of the same verb. text standardization is a basic form of feature engineering that aims to erase encoding differences that you don t want your model to have to deal with. it s not exclusive to machine learning either you d have to do the same thing if you were building a search engine. one of the simplest and most widespread standardization schemes is convert to lowercase and remove punctuation characters. our two sentences would become sunset came i was staring at the mexico sky isnt nature splendid sunset came i stared at the m xico sky isnt nature splendid much closer already. another common transformation is to convert special characters to a standard form such as replacing ҏ with e ҿ with ae and so on. our token m xico would then become mexico . lastly a much more advanced standardization pattern that is more rarely used in a machine learning context is stemming converting variations of a term (such as differ.ent conjugated forms of a verb) into a single shared representation like turning caught and been catching into [catch] or cats into [cat] . with stemming was staring and stared would become something like [stare] and our two similar sentences would finally end up with an identical encoding sunset came i [stare] at the mexico sky isnt nature splendid with these standardization techniques your model will require less training data and will generalize better it won t need abundant examples of both sunset and sun.set to learn that they mean the same thing and it will be able to make sense of m x.ico even if it has only seen mexico in its training set. of course standardization may also erase some amount of information so always keep the context in mind for instance if you re writing a model that extracts questions from interview articles it should definitely treat ? as a separate token instead of dropping it because it s a use.ful signal for this specific task. 11.2.2 text splitting (tokenization) once your text is standardized you need to break it up into units to be vectorized (tokens) a step called tokenization. you could do this in three different ways word-level tokenization where tokens are space-separated (or punctuation-separated) substrings. a variant of this is to further split words into subwords when applicable for instance treating staring as star+ing or called as call+ed. n-gram tokenization where tokens are groups of n consecutive words. for instance the cat or he was would be 2-gram tokens (also called bigrams). character-level tokenization where each character is its own token. in practice this scheme is rarely used and you only really see it in specialized contexts like text generation or speech recognition. in general you ll always use either word-level or n-gram tokenization. there are two kinds of text-processing models those that care about word order called sequence mod.els and those that treat input words as a set discarding their original order called bag-of-words models. if you re building a sequence model you ll use word-level tokeni.zation and if you re building a bag-of-words model you ll use n-gram tokenization. n-grams are a way to artificially inject a small amount of local word order information into the model. throughout this chapter you ll learn more about each type of model and when to use them. understanding n-grams and bag-of-words word n-grams are groups of n (or fewer) consecutive words that you can extract from a sentence. the same concept may also be applied to characters instead of words. here s a simple example. consider the sentence the cat sat on the mat. it may be decomposed into the following set of 2-grams {"the" "the cat" "cat" "cat sat" "sat" "sat on" "on" "on the" "the mat" "mat"} it may also be decomposed into the following set of 3-grams {"the" "the cat" "cat" "cat sat" "the cat sat" "sat" "sat on" "on" "cat sat on" "on the" "sat on the" "the mat" "mat" "on the mat"} such a set is called a bag-of-2-grams or bag-of-3-grams respectively. the term bag here refers to the fact that you re dealing with a set of tokens rather than a list or sequence the tokens have no specific order. this family of tokenization methods is called bag-of-words (or bag-of-n-grams). because bag-of-words isn t an order-preserving tokenization method (the tokens gen.erated are understood as a set not a sequence and the general structure of the sen.tences is lost) it tends to be used in shallow language-processing models rather than in deep learning models. extracting n-grams is a form of feature engineering and deep learning sequence models do away with this manual approach replacing it with hierarchical feature learning. one-dimensional convnets recurrent neural networks and transformers are capable of learning representations for groups of words and characters without being explicitly told about the existence of such groups by looking at continuous word or character sequences. 11.2.3 vocabulary indexing once your text is split into tokens you need to encode each token into a numerical representation. you could potentially do this in a stateless way such as by hashing each token into a fixed binary vector but in practice the way you d go about it is to build an index of all terms found in the training data (the vocabulary ) and assign a unique integer to each entry in the vocabulary. something like this vocabulary = {} for text in dataset text = standardize(text) tokens = tokenize(text) for token in tokens if token not in vocabulary vocabulary[token] = len(vocabulary) you can then convert that integer into a vector encoding that can be processed by a neural network like a one-hot vector def one_hot_encode_token(token) vector = np.zeros((len(vocabulary))) token_index = vocabulary[token] vector[token_index] = 1 return vector note that at this step it s common to restrict the vocabulary to only the top 20000 or 30000 most common words found in the training data. any text dataset tends to fea.ture an extremely large number of unique terms most of which only show up once or twice indexing those rare terms would result in an excessively large feature space where most features would have almost no information content. remember when you were training your first deep learning models on the imdb dataset in chapters 4 and 5? the data you were using from keras.datasets.imdb was already preprocessed into sequences of integers where each integer stood for a given word. back then we used the setting num_words=10000 in order to restrict our vocab.ulary to the top 10000 most common words found in the training data. now there s an important detail here that we shouldn t overlook when we look up a new token in our vocabulary index it may not necessarily exist. your training data may not have contained any instance of the word cherimoya (or maybe you excluded it from your index because it was too rare) so doing token_index = vocabulary["cherimoya"] may result in a keyerror. to handle this you should use an out of vocabulary index (abbreviated as oov index) a catch-all for any token that wasn t in the index. it s usually index 1 you re actually doing token_index = vocabulary.get(token 1). when decoding a sequence of integers back into words you ll replace 1 with something like [unk] (which you d call an oov token ). why use 1 and not 0? you may ask. that s because 0 is already taken. there are two special tokens that you will commonly use the oov token (index 1) and the mask token (index 0). while the oov token means here was a word we did not recog.nize the mask token tells us ignore me i m not a word. you d use it in particular to pad sequence data because data batches need to be contiguous all sequences in a batch of sequence data must have the same length so shorter sequences should be padded to the length of the longest sequence. if you want to make a batch of data with the sequences [5 7124 4 89] and [8 3421] it would have to look like this [[5 7 124 4 89] [8 34 21 0 0]] the batches of integer sequences for the imdb dataset that you worked with in chap.ters 4 and 5 were padded with zeros in this way. 11.2.4 using the textvectorization layer every step i ve introduced so far would be very easy to implement in pure python. maybe you could write something like this import string class vectorizer def standardize(self text) text = text.lower() return "".join(char for char in text if char not in string.punctuation) def tokenize(self text) text = self.standardize(text) return text.split() def make_vocabulary(self dataset) self.vocabulary = {"" 0 "[unk]" 1} for text in dataset text = self.standardize(text) tokens = self.tokenize(text) for token in tokens if token not in self.vocabulary self.vocabulary[token] = len(self.vocabulary) self.inverse_vocabulary = dict( (v k) for k v in self.vocabulary.items()) def encode(self text) text = self.standardize(text) tokens = self.tokenize(text) return [self.vocabulary.get(token 1) for token in tokens] def decode(self int_sequence) return " ".join( self.inverse_vocabulary.get(i "[unk]") for i in int_sequence) vectorizer = vectorizer() dataset = [ "i write erase rewrite" "erase again and then" "a poppy blooms." ] vectorizer.make_vocabulary(dataset) it does the job haiku by poet hokushi >>> test_sentence = "i write rewrite and still rewrite again" >>> encoded_sentence = vectorizer.encode(test_sentence) >>> print(encoded_sentence) [2 3 5 7 1 5 6] >>> decoded_sentence = vectorizer.decode(encoded_sentence) >>> print(decoded_sentence) "i write rewrite and [unk] rewrite again" however using something like this wouldn t be very performant. in practice you ll work with the keras textvectorization layer which is fast and efficient and can be dropped directly into a tf.data pipeline or a keras model. this is what the textvectorization layer looks like from tensorflow.keras.layers import textvectorization text_vectorization = textvectorization( output_mode="int" configures the layer to return sequences of words encoded ) as integer indices. there are several other output modes available which you will see in action in a bit. by default the textvectorization layer will use the setting convert to lowercase and remove punctuation for text standardization and split on whitespace for tokeniza.tion. but importantly you can provide custom functions for standardization and toke.nization which means the layer is flexible enough to handle any use case. note that such custom functions should operate on tf.string tensors not regular python strings! for instance the default layer behavior is equivalent to the following import re convert import string strings to import tensorflow as tf lowercase. def custom_standardization_fn(string_tensor) lowercase_string = tf.strings.lower(string_tensor) return tf.strings.regex_replace( replace punctuation characters with the empty string. lowercase_string f"[{re.escape(string.punctuation)}]" "") def custom_split_fn(string_tensor) return tf.strings.split(string_tensor) split strings on whitespace. text_vectorization = textvectorization( output_mode="int" standardize=custom_standardization_fn split=custom_split_fn ) to index the vocabulary of a text corpus just call the adapt() method of the layer with a dataset object that yields strings or just with a list of python strings dataset = [ "i write erase rewrite" "erase again and then" "a poppy blooms." ] text_vectorization.adapt(dataset) note that you can retrieve the computed vocabulary via get_vocabulary() this can be useful if you need to convert text encoded as integer sequences back into words. the first two entries in the vocabulary are the mask token (index 0) and the oov token (index 1). entries in the vocabulary list are sorted by frequency so with a real-world dataset very common words like the or a would come first. >>> text_vectorization.get_vocabulary() ["" "[unk]" "erase" "write" ...] for a demonstration let s try to encode and then decode an example sentence >>> vocabulary = text_vectorization.get_vocabulary() >>> test_sentence = "i write rewrite and still rewrite again" >>> encoded_sentence = text_vectorization(test_sentence) >>> print(encoded_sentence) tf.tensor([ 7 3 5 9 1 5 10] shape=(7) dtype=int64) >>> inverse_vocab = dict(enumerate(vocabulary)) >>> decoded_sentence = " ".join(inverse_vocab[int(i)] for i in encoded_sentence) >>> print(decoded_sentence) "i write rewrite and [unk] rewrite again" using the textvectorization layer in a tf.data pipeline or as part of a model importantly because textvectorization is mostly a dictionary lookup operation it can t be executed on a gpu (or tpu) only on a cpu. so if you re training your model on a gpu your textvectorization layer will run on the cpu before sending its out.put to the gpu. this has important performance implications. there are two ways we could use our textvectorization layer. the first option is to put it in the tf.data pipeline like this string_dataset would be a dataset that yields string tensors. int_sequence_dataset = string_dataset.map( text_vectorization num_parallel_calls=4) the num_parallel_calls argument is used to parallelize the map() call across multiple cpu cores. the second option is to make it part of the model (after all it s a keras layer) like this create a symbolic input that expects strings. apply the text vectorization text_input = keras.input(shape=() dtype="string") layer to it. vectorized_text = text_vectorization(text_input) embedded_input = keras.layers.embedding(...)(vectorized_text) output = ... model = keras.model(text_input output) you can keep chaining new layers on top just your regular functional api model. there s an important difference between the two if the vectorization step is part of the model it will happen synchronously with the rest of the model. this means that at each training step the rest of the model (placed on the gpu) will have to wait for the output of the textvectorization layer (placed on the cpu) to be ready in order to get to work. meanwhile putting the layer in the tf.data pipeline enables you to two approaches for representing groups of words sets and sequences do asynchronous preprocessing of your data on cpu while the gpu runs the model on one batch of vectorized data the cpu stays busy by vectorizing the next batch of raw strings. so if you re training the model on gpu or tpu you ll probably want to go with the first option to get the best performance. this is what we will do in all practical examples throughout this chapter. when training on a cpu though synchronous processing is fine you will get 100% utilization of your cores regardless of which option you go with. now if you were to export our model to a production environment you would want to ship a model that accepts raw strings as input like in the code snippet for the second option above otherwise you would have to reimplement text standardization and tokenization in your production environment (maybe in javascript?) and you would face the risk of introducing small preprocessing discrepancies that would hurt the model s accuracy. thankfully the textvectorization layer enables you to include text preprocessing right into your model making it easier to deploy even if you were originally using the layer as part of a tf.data pipeline. in the sidebar exporting a model that processes raw strings you ll learn how to export an inference-only trained model that incorporates text preprocessing. you ve now learned everything you need to know about text preprocessing let s move on to the modeling stage. 11.3 two approaches for representing groups of words sets and sequences how a machine learning model should represent individual words is a relatively uncon.troversial question they re categorical features (values from a predefined set) and we know how to handle those. they should be encoded as dimensions in a feature space or as category vectors (word vectors in this case). a much more problematic question however is how to encode the way words are woven into sentences word order. the problem of order in natural language is an interesting one unlike the steps of a timeseries words in a sentence don t have a natural canonical order. different lan.guages order similar words in very different ways. for instance the sentence structure of english is quite different from that of japanese. even within a given language you can typically say the same thing in different ways by reshuffling the words a bit. even further if you fully randomize the words in a short sentence you can still largely fig.ure out what it was saying though in many cases significant ambiguity seems to arise. order is clearly important but its relationship to meaning isn t straightforward. how to represent word order is the pivotal question from which different kinds of nlp architectures spring. the simplest thing you could do is just discard order and treat text as an unordered set of words this gives you bag-of-words models. you could also decide that words should be processed strictly in the order in which they appear one at a time like steps in a timeseries you could then leverage the recurrent models from the last chapter. finally a hybrid approach is also possible the transformer architecture is technically order-agnostic yet it injects word-position information into the representations it processes which enables it to simultaneously look at different parts of a sentence (unlike rnns) while still being order-aware. because they take into account word order both rnns and transformers are called sequence models. historically most early applications of machine learning to nlp just involved bag-of-words models. interest in sequence models only started rising in 2015 with the rebirth of recurrent neural networks. today both approaches remain relevant. let s see how they work and when to leverage which. we ll demonstrate each approach on a well-known text classification benchmark the imdb movie review sentiment-classification dataset. in chapters 4 and 5 you worked with a prevectorized version of the imdb dataset now let s process the raw imdb text data just like you would do when approaching a new text-classification problem in the real world. 11.3.1 preparing the imdb movie reviews data let s start by downloading the dataset from the stanford page of andrew maas and uncompressing it !curl -o https//ai.stanford.edu/~amaas/data/sentiment/aclimdb_v1.tar.gz !tar -xf aclimdb_v1.tar.gz you re left with a directory named aclimdb with the following structure aclimdb/ ...train/ ......pos/ ......neg/ ...test/ ......pos/ ......neg/ for instance the train/pos/ directory contains a set of 12500 text files each of which contains the text body of a positive-sentiment movie review to be used as training data. the negative-sentiment reviews live in the neg directories. in total there are 25000 text files for training and another 25000 for testing. there s also a train/unsup subdirectory in there which we don t need. let s delete it !rm -r aclimdb/train/unsup take a look at the content of a few of these text files. whether you re working with text data or image data remember to always inspect what your data looks like before you dive into modeling it. it will ground your intuition about what your model is actu.ally doing !cat aclimdb/train/pos/4077_10.txt two approaches for representing groups of words sets and sequences next let s prepare a validation set by setting apart 20% of the training text files in a new directory aclimdb/val import os pathlib shutil random base_dir = pathlib.path("aclimdb") val_dir = base_dir / "val" train_dir = base_dir / "train" for category in ("neg" "pos") os.makedirs(val_dir / category) files = os.listdir(train_dir / category) random.random(1337).shuffle(files) num_val_samples = int(0.2 * len(files)) val_files = files[-num_val_samples] for fname in val_files shutil.move(train_dir / category / fname val_dir / category / fname) shuffle the list of training files using a seed to ensure we get the same validation set every time we run the code. take 20% of the training files to use for validation. move the files to aclimdb/val/neg and aclimdb/val/pos. remember how in chapter 8 we used the image_dataset_from_directory utility to create a batched dataset of images and their labels for a directory structure? you can do the exact same thing for text files using the text_dataset_from_directory utility. let s create three dataset objects for training validation and testing from tensorflow import keras batch_size = 32 train_ds = keras.utils.text_dataset_from_directory( "aclimdb/train" batch_size=batch_size ) val_ds = keras.utils.text_dataset_from_directory( "aclimdb/val" batch_size=batch_size ) test_ds = keras.utils.text_dataset_from_directory( "aclimdb/test" batch_size=batch_size ) running this line should output found 20000 files belonging to 2 classes if you see found 70000 files belonging to 3 classes it means you forgot to delete the aclimdb/train/unsup directory. these datasets yield inputs that are tensorflow tf.string tensors and targets that are int32 tensors encoding the value 0 or 1. >>> for inputs targets in train_ds >>> print("inputs.shape" inputs.shape) >>> print("inputs.dtype" inputs.dtype) >>> print("targets.shape" targets.shape) >>> print("targets.dtype" targets.dtype) >>> print("inputs" inputs) >>> print("targets" targets) >>> break inputs.shape (32) inputs.dtype  targets.shape (32) targets.dtype  inputs tf.tensor(b"this string contains the movie review." shape=() dtype=string) targets tf.tensor(1 shape=() dtype=int32) all set. now let s try learning something from this data. 11.3.2 processing words as a set the bag-of-words approach the simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set (a bag ) of tokens. you could either look at individual words (unigrams) or try to recover some local order information by looking at groups of consecutive token (n-grams). single words (unigrams) with binary encoding if you use a bag of single words the sentence the cat sat on the mat becomes {"cat" "mat" "on" "sat" "the"} the main advantage of this encoding is that you can represent an entire text as a sin.gle vector where each entry is a presence indicator for a given word. for instance using binary encoding (multi-hot) you d encode a text as a vector with as many dimensions as there are words in your vocabulary with 0s almost everywhere and some 1s for dimensions that encode words present in the text. this is what we did when we worked with text data in chapters 4 and 5. let s try this on our task. first let s process our raw text datasets with a textvectorization layer so that they yield multi-hot encoded binary word vectors. our layer will only look at single words (that is to say unigrams). limit the vocabulary to the 20000 most frequent words. encode the output otherwise we d be indexing every word in the training data tokens as multi-hot potentially tens of thousands of terms that only occur once or binary vectors. twice and thus aren t informative. in general 20000 is the right vocabulary size for text classification. text_vectorization = textvectorization( max_tokens=20000 output_mode="multi_hot" ) text_only_train_ds = train_ds.map(lambda x y x) text_vectorization.adapt(text_only_train_ds) binary_1gram_train_ds = train_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) binary_1gram_val_ds = val_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) binary_1gram_test_ds = test_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) prepare a dataset that only yields raw text inputs (no labels). use that dataset to index the dataset vocabulary via the adapt() method. prepare processed versions of our training validation and test dataset. make sure to specify num_parallel_calls to leverage multiple cpu cores. two approaches for representing groups of words sets and sequences you can try to inspect the output of one of these datasets. >>> for inputs targets in binary_1gram_train_ds >>> print("inputs.shape" inputs.shape) >>> print("inputs.dtype" inputs.dtype) >>> print("targets.shape" targets.shape) >>> print("targets.dtype" targets.dtype) >>> print("inputs" inputs) >>> print("targets" targets) >>> break inputs.shape (32 20000) inputs are batches of inputs.dtype  20000-dimensional these vectors consist targets.shape (32) vectors. entirely of ones and zeros. targets.dtype  inputs tf.tensor([1. 1. 1. ... 0. 0. 0.] shape=(20000) dtype=float32) targets tf.tensor(1 shape=() dtype=int32) next let s write a reusable model-building function that we ll use in all of our experi.ments in this section. from tensorflow import keras from tensorflow.keras import layers def get_model(max_tokens=20000 hidden_dim=16) inputs = keras.input(shape=(max_tokens)) x = layers.dense(hidden_dim activation="relu")(inputs) x = layers.dropout(0.5)(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) return model finally let s train and test our model. model = get_model() model.summary() callbacks = [ keras.callbacks.modelcheckpoint("binary_1gram.keras" save_best_only=true) ] model.fit(binary_1gram_train_ds.cache() validation_data=binary_1gram_val_ds.cache() epochs=10 callbacks=callbacks) model = keras.models.load_model("binary_1gram.keras") we call cache() on the datasets to cache them in memory this way we will only do the preprocessing once during the first epoch and we ll reuse the preprocessed texts for the following epochs. this can only be done if the data is small enough to fit in memory. print(f"test acc {model.evaluate(binary_1gram_test_ds).3f}") this gets us to a test accuracy of 89.2% not bad! note that in this case since the data.set is a balanced two-class classification dataset (there are as many positive samples as negative samples) the naive baseline we could reach without training an actual model would only be 50%. meanwhile the best score that can be achieved on this dataset without leveraging external data is around 95% test accuracy. bigrams with binary encoding of course discarding word order is very reductive because even atomic concepts can be expressed via multiple words the term united states conveys a concept that is quite distinct from the meaning of the words states and united taken separately. for this reason you will usually end up re-injecting local order information into your bag-of-words representation by looking at n-grams rather than single words (most commonly bigrams). with bigrams our sentence becomes {"the" "the cat" "cat" "cat sat" "sat" "sat on" "on" "on the" "the mat" "mat"} the textvectorization layer can be configured to return arbitrary n-grams bigrams trigrams etc. just pass an ngrams=n argument as in the following listing. text_vectorization = textvectorization( ngrams=2 max_tokens=20000 output_mode="multi_hot" ) let s test how our model performs when trained on such binary-encoded bags of bigrams. text_vectorization.adapt(text_only_train_ds) binary_2gram_train_ds = train_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) binary_2gram_val_ds = val_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) binary_2gram_test_ds = test_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) model = get_model() model.summary() callbacks = [ keras.callbacks.modelcheckpoint("binary_2gram.keras" save_best_only=true) ] two approaches for representing groups of words sets and sequences model.fit(binary_2gram_train_ds.cache() validation_data=binary_2gram_val_ds.cache() epochs=10 callbacks=callbacks) model = keras.models.load_model("binary_2gram.keras") print(f"test acc {model.evaluate(binary_2gram_test_ds).3f}") we re now getting 90.4% test accuracy a marked improvement! turns out local order is pretty important. bigrams with tf-idf encoding you can also add a bit more information to this representation by counting how many times each word or n-gram occurs that is to say by taking the histogram of the words over the text {"the" 2 "the cat" 1 "cat" 1 "cat sat" 1 "sat" 1 "sat on" 1 "on" 1 "on the" 1 "the mat 1" "mat" 1} if you re doing text classification knowing how many times a word occurs in a sample is critical any sufficiently long movie review may contain the word terrible regard.less of sentiment but a review that contains many instances of the word terrible is likely a negative one. here s how you d count bigram occurrences with the textvectorization layer. text_vectorization = textvectorization( ngrams=2 max_tokens=20000 output_mode="count" ) now of course some words are bound to occur more often than others no matter what the text is about. the words the a is and are will always dominate your word count histograms drowning out other words despite being pretty much useless features in a classification context. how could we address this? you already guessed it via normalization. we could just normalize word counts by subtracting the mean and dividing by the variance (computed across the entire train.ing dataset). that would make sense. except most vectorized sentences consist almost entirely of zeros (our previous example features 12 non-zero entries and 19988 zero entries) a property called sparsity. that s a great property to have as it dramatically reduces compute load and reduces the risk of overfitting. if we subtracted the mean from each feature we d wreck sparsity. thus whatever normalization scheme we use should be divide-only. what then should we use as the denominator? the best prac.tice is to go with something called tf-idf normalization tf-idf stands for term fre.quency inverse document frequency. tf-idf is so common that it s built into the textvectorization layer. all you need to do to start using it is to switch the output_mode argument to "tf_idf". understanding tf-idf normalization the more a given term appears in a document the more important that term is for understanding what the document is about. at the same time the frequency at which the term appears across all documents in your dataset matters too terms that appear in almost every document (like the or a ) aren t particularly informative while terms that appear only in a small subset of all texts (like herzog ) are very dis.tinctive and thus important. tf-idf is a metric that fuses these two ideas. it weights a given term by taking term frequency how many times the term appears in the current document and dividing it by a measure of document frequency which esti.mates how often the term comes up across the dataset. you d compute it as follows def tfidf(term document dataset) term_freq = document.count(term) doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1) return term_freq / doc_freq text_vectorization = textvectorization( ngrams=2 max_tokens=20000 output_mode="tf_idf" ) let s train a new model with this scheme. text_vectorization.adapt(text_only_train_ds) the adapt() call will learn the tf-idf weights in addition to tfidf_2gram_train_ds = train_ds.map( the vocabulary. lambda x y (text_vectorization(x) y) num_parallel_calls=4) tfidf_2gram_val_ds = val_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) tfidf_2gram_test_ds = test_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) model = get_model() model.summary() callbacks = [ keras.callbacks.modelcheckpoint("tfidf_2gram.keras" save_best_only=true) ] model.fit(tfidf_2gram_train_ds.cache() validation_data=tfidf_2gram_val_ds.cache() epochs=10 callbacks=callbacks) model = keras.models.load_model("tfidf_2gram.keras") print(f"test acc {model.evaluate(tfidf_2gram_test_ds).3f}") two approaches for representing groups of words sets and sequences this gets us an 89.8% test accuracy on the imdb classification task it doesn t seem to be particularly helpful in this case. however for many text-classification datasets it would be typical to see a one-percentage-point increase when using tf-idf compared to plain binary encoding. exporting a model that processes raw strings in the preceding examples we did our text standardization splitting and indexing as part of the tf.data pipeline. but if we want to export a standalone model indepen.dent of this pipeline we should make sure that it incorporates its own text prepro.cessing (otherwise you d have to reimplement in the production environment which can be challenging or can lead to subtle discrepancies between the training data and the production data). thankfully this is easy. just create a new model that reuses your textvectorization layer and adds to it the model you just trained one input sample would be one string. apply text inputs = keras.input(shape=(1) dtype="string") preprocessing. processed_inputs = text_vectorization(inputs) outputs = model(processed_inputs) apply the inference_model = keras.model(inputs outputs) previously instantiate the end-to-end model. trained model. the resulting model can process batches of raw strings import tensorflow as tf raw_text_data = tf.convert_to_tensor([ ["that was an excellent movie i loved it."] ]) predictions = inference_model(raw_text_data) print(f"{float(predictions * 100).2f} percent positive") 11.3.3 processing words as a sequence the sequence model approach these past few examples clearly show that word order matters manual engineering of order-based features such as bigrams yields a nice accuracy boost. now remember the history of deep learning is that of a move away from manual feature engineering toward letting models learn their own features from exposure to data alone. what if instead of manually crafting order-based features we exposed the model to raw word sequences and let it figure out such features on its own? this is what sequence models are about. to implement a sequence model you d start by representing your input samples as sequences of integer indices (one integer standing for one word). then you d map each integer to a vector to obtain vector sequences. finally you d feed these sequences of vectors into a stack of layers that could cross-correlate features from adja.cent vectors such as a 1d convnet a rnn or a transformer. for some time around 2016 2017 bidirectional rnns (in particular bidirectional lstms) were considered to be the state of the art for sequence modeling. since you re already familiar with this architecture this is what we ll use in our first sequence model examples. however nowadays sequence modeling is almost universally done with trans.formers which we will cover shortly. oddly one-dimensional convnets were never very popular in nlp even though in my own experience a residual stack of depth-wise-separable 1d convolutions can often achieve comparable performance to a bidi.rectional lstm at a greatly reduced computational cost. a first practical example let s try out a first sequence model in practice. first let s prepare datasets that return integer sequences. from tensorflow.keras import layers max_length = 600 max_tokens = 20000 text_vectorization = layers.textvectorization( max_tokens=max_tokens output_mode="int" output_sequence_length=max_length ) text_vectorization.adapt(text_only_train_ds) int_train_ds = train_ds.map( lambda x y (text_vectorization(x) y)) num_parallel_calls=4) int_val_ds = val_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) int_test_ds = test_ds.map( lambda x y (text_vectorization(x) y) num_parallel_calls=4) in order to keep a manageable input size we ll truncate the inputs after the first 600 words. this is a reasonable choice since the average review length is 233 words and only 5% of reviews are longer than 600 words. next let s make a model. the simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers (each dimension would represent one possible term in the vocabulary). on top of these one-hot vectors we ll add a simple bidirectional lstm. import tensorflow as tf inputs = keras.input(shape=(none) dtype="int64") embedded = tf.one_hot(inputs depth=max_tokens) x = layers.bidirectional(layers.lstm(32))(embedded) x = layers.dropout(0.5)(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop" finally add a loss="binary_crossentropy" classification metrics=["accuracy"]) layer. one input is a sequence of integers. encode the integers into binary 20000.dimensional vectors. add a bidirectional lstm. model.summary() two approaches for representing groups of words sets and sequences now let s train our model. callbacks = [ keras.callbacks.modelcheckpoint("one_hot_bidir_lstm.keras" save_best_only=true) ] model.fit(int_train_ds validation_data=int_val_ds epochs=10 callbacks=callbacks) model = keras.models.load_model("one_hot_bidir_lstm.keras") print(f"test acc {model.evaluate(int_test_ds).3f}") a first observation this model trains very slowly especially compared to the light.weight model of the previous section. this is because our inputs are quite large each input sample is encoded as a matrix of size (600 20000) (600 words per sample 20000 possible words). that s 12000000 floats for a single movie review. our bidirec.tional lstm has a lot of work to do. second the model only gets to 87% test accu.racy it doesn t perform nearly as well as our (very fast) binary unigram model. clearly using one-hot encoding to turn words into vectors which was the simplest thing we could do wasn t a great idea. there s a better way word embeddings. understanding word embeddings crucially when you encode something via one-hot encoding you re making a feature-engineering decision. you re injecting into your model a fundamental assumption about the structure of your feature space. that assumption is that the different tokens you re encoding are all independent from each other indeed one-hot vectors are all orthogo.nal to one another. and in the case of words that assumption is clearly wrong. words form a structured space they share information with each other. the words movie and film are interchangeable in most sentences so the vector that represents movie should not be orthogonal to the vector that represents film they should be the same vector or close enough. to get a bit more abstract the geometric relationship between two word vectors should reflect the semantic relationship between these words. for instance in a reason.able word vector space you would expect synonyms to be embedded into similar word vectors and in general you would expect the geometric distance (such as the cosine distance or l2 distance) between any two word vectors to relate to the semantic dis.tance between the associated words. words that mean different things should lie far away from each other whereas related words should be closer. word embeddings are vector representations of words that achieve exactly this they map human language into a structured geometric space. whereas the vectors obtained through one-hot encoding are binary sparse (mostly made of zeros) and very high-dimensional (the same dimensionality as the number of words in the vocabulary) word embeddings are low-dimensional floating-point vectors (that is dense vectors as opposed to sparse vectors) see figure 11.2. it s common to see word embeddings that are 256-dimensional 512-dimensional or 1024-dimensional when dealing with very large vocabularies. on the other hand one-hot encoding words generally leads to vectors that are 20000-dimensional or greater (capturing a vocabu.lary of 20000 tokens in this case). so word embeddings pack more information into far fewer dimensions. figure 11.2 word representations obtained from one-hot encoding or one-hot word vectors - sparse - high-dimensional word embeddings - dense - lower-dimensional hashing are sparse high-dimensional and hardcoded. word embeddings are dense relatively low-dimensional and - hardcoded - learned from data learned from data. besides being dense representations word embeddings are also structured representa.tions and their structure is learned from data. similar words get embedded in close locations and further specific directions in the embedding space are meaningful. to make this clearer let s look at a concrete example. in figure 11.3 four words are embedded on a 2d plane cat dog wolf and tiger. with the vector representations we chose here some semantic relationships between these words can be encoded as geometric transformations. for instance the same vector allows us to go from cat to tiger and from dog to wolf this vector could be inter.preted as the from pet to wild animal vector. similarly another vector lets us go from dog to cat and from wolf to tiger which could be interpreted as a from canine to feline vector. 1 wolf tiger dog cat figure 11.3 a toy example 0x of a word-embedding space 10 two approaches for representing groups of words sets and sequences in real-world word-embedding spaces common examples of meaningful geometric transformations are gender vectors and plural vectors. for instance by adding a female vector to the vector king we obtain the vector queen. by adding a plu.ral vector we obtain kings. word-embedding spaces typically feature thousands of such interpretable and potentially useful vectors. let s look at how to use such an embedding space in practice. there are two ways to obtain word embeddings learn word embeddings jointly with the main task you care about (such as doc.ument classification or sentiment prediction). in this setup you start with ran.dom word vectors and then learn word vectors in the same way you learn the weights of a neural network. load into your model word embeddings that were precomputed using a differ.ent machine learning task than the one you re trying to solve. these are called pretrained word embeddings. let s review each of these approaches. learning word embeddings with the embedding layer is there some ideal word-embedding space that would perfectly map human language and could be used for any natural language processing task? possibly but we have yet to compute anything of the sort. also there is no such a thing as human language there are many different languages and they aren t isomorphic to one another because a language is the reflection of a specific culture and a specific context. but more pragmatically what makes a good word-embedding space depends heavily on your task the perfect word-embedding space for an english-language movie-review sentiment-analysis model may look different from the perfect embedding space for an english-language legal-document classification model because the importance of cer.tain semantic relationships varies from task to task. it s thus reasonable to learn a new embedding space with every new task. fortu.nately backpropagation makes this easy and keras makes it even easier. it s about learning the weights of a layer the embedding layer. embedding_layer = layers.embedding(input_dim=max_tokens output_dim=256) the embedding layer takes at least two arguments the number of possible tokens and the dimensionality of the embeddings (here 256). the embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. it takes integers as input looks up these integers in an internal dictionary and returns the associated vectors. it s effec.tively a dictionary lookup (see figure 11.4). word index embedding layer corresponding word vector figure 11.4 the embedding layer the embedding layer takes as input a rank-2 tensor of integers of shape (batch_size sequence_length) where each entry is a sequence of integers. the layer then returns a 3d floating-point tensor of shape (batch_size sequence_length embedding_ dimensionality). when you instantiate an embedding layer its weights (its internal dictionary of token vectors) are initially random just as with any other layer. during training these word vectors are gradually adjusted via backpropagation structuring the space into something the downstream model can exploit. once fully trained the embedding space will show a lot of structure a kind of structure specialized for the specific prob.lem for which you re training your model. let s build a model that includes an embedding layer and benchmark it on our task. inputs = keras.input(shape=(none) dtype="int64") embedded = layers.embedding(input_dim=max_tokens output_dim=256)(inputs) x = layers.bidirectional(layers.lstm(32))(embedded) x = layers.dropout(0.5)(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) model.summary() callbacks = [ keras.callbacks.modelcheckpoint("embeddings_bidir_gru.keras" save_best_only=true) ] model.fit(int_train_ds validation_data=int_val_ds epochs=10 callbacks=callbacks) model = keras.models.load_model("embeddings_bidir_gru.keras") print(f"test acc {model.evaluate(int_test_ds).3f}") it trains much faster than the one-hot model (since the lstm only has to process 256-dimensional vectors instead of 20000-dimensional) and its test accuracy is com.parable (87%). however we re still some way off from the results of our basic bigram model. part of the reason why is simply that the model is looking at slightly less data the bigram model processed full reviews while our sequence model truncates sequences after 600 words. understanding padding and masking one thing that s slightly hurting model performance here is that our input sequences are full of zeros. this comes from our use of the output_sequence_length=max_ length option in textvectorization (with max_length equal to 600) sentences lon.ger than 600 tokens are truncated to a length of 600 tokens and sentences shorter than 600 tokens are padded with zeros at the end so that they can be concatenated together with other sequences to form contiguous batches. two approaches for representing groups of words sets and sequences we re using a bidirectional rnn two rnn layers running in parallel with one processing the tokens in their natural order and the other processing the same tokens in reverse. the rnn that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode padding possibly for several hun.dreds of iterations if the original sentence was short. the information stored in the internal state of the rnn will gradually fade out as it gets exposed to these meaning.less inputs. we need some way to tell the rnn that it should skip these iterations. there s an api for that masking. the embedding layer is capable of generating a mask that corresponds to its input data. this mask is a tensor of ones and zeros (or true/false booleans) of shape (batch_size sequence_length) where the entry mask[i t] indicates where time-step t of sample i should be skipped or not (the timestep will be skipped if mask[i t] is 0 or false and processed otherwise). by default this option isn t active you can turn it on by passing mask_zero=true to your embedding layer. you can retrieve the mask with the compute_mask() method >>> embedding_layer = embedding(input_dim=10 output_dim=256 mask_zero=true) >>> some_input = [ ... [4 3 2 1 0 0 0] ... [5 4 3 2 1 0 0] ... [2 1 0 0 0 0 0]] >>> mask = embedding_layer.compute_mask(some_input)  in practice you will almost never have to manage masks by hand. instead keras will automatically pass on the mask to every layer that is able to process it (as a piece of metadata attached to the sequence it represents). this mask will be used by rnn lay.ers to skip masked steps. if your model returns an entire sequence the mask will also be used by the loss function to skip masked steps in the output sequence. let s try retraining our model with masking enabled. inputs = keras.input(shape=(none) dtype="int64") embedded = layers.embedding( input_dim=max_tokens output_dim=256 mask_zero=true)(inputs) x = layers.bidirectional(layers.lstm(32))(embedded) x = layers.dropout(0.5)(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop" loss="binary_crossentropy" metrics=["accuracy"]) model.summary() callbacks = [ keras.callbacks.modelcheckpoint("embeddings_bidir_gru_with_masking.keras" save_best_only=true) ] model.fit(int_train_ds validation_data=int_val_ds epochs=10 callbacks=callbacks) model = keras.models.load_model("embeddings_bidir_gru_with_masking.keras") print(f"test acc {model.evaluate(int_test_ds).3f}") this time we get to 88% test accuracy a small but noticeable improvement. using pretrained word embeddings sometimes you have so little training data available that you can t use your data alone to learn an appropriate task-specific embedding of your vocabulary. in such cases instead of learning word embeddings jointly with the problem you want to solve you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties one that captures generic aspects of language structure. the rationale behind using pretrained word embeddings in natu.ral language processing is much the same as for using pretrained convnets in image classification you don t have enough data available to learn truly powerful features on your own but you expect that the features you need are fairly generic that is com.mon visual features or semantic features. in this case it makes sense to reuse features learned on a different problem. such word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents) using a variety of techniques some involving neural networks others not. the idea of a dense low-dimensional embedding space for words computed in an unsupervised way was ini.tially explored by bengio et al. in the early 2000s1 but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes the word2vec algorithm (https//code.google .com/archive/p/word2vec) developed by tomas mikolov at google in 2013. word2vec dimensions capture specific semantic properties such as gender. there are various precomputed databases of word embeddings that you can down.load and use in a keras embedding layer. word2vec is one of them. another popular one is called global vectors for word representation (glove https//nlp.stanford .edu/projects/glove) which was developed by stanford researchers in 2014. this embedding technique is based on factorizing a matrix of word co-occurrence statis.tics. its developers have made available precomputed embeddings for millions of english tokens obtained from wikipedia data and common crawl data. let s look at how you can get started using glove embeddings in a keras model. the same method is valid for word2vec embeddings or any other word-embedding database. we ll start by downloading the glove files and parse them. we ll then load the word vectors into a keras embedding layer which we ll use to build a new model. 1 yoshua bengio et al. a neural probabilistic language model journal of machine learning research (2003). two approaches for representing groups of words sets and sequences first let s download the glove word embeddings precomputed on the 2014 english wikipedia dataset. it s an 822 mb zip file containing 100-dimensional embed.ding vectors for 400000 words (or non-word tokens). !wget http//nlp.stanford.edu/data/glove.6b.zip !unzip -q glove.6b.zip let s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation. import numpy as np path_to_glove_file = "glove.6b.100d.txt" embeddings_index = {} with open(path_to_glove_file) as f for line in f word coefs = line.split(maxsplit=1) coefs = np.fromstring(coefs "f" sep=" ") embeddings_index[word] = coefs print(f"found {len(embeddings_index)} word vectors.") next let s build an embedding matrix that you can load into an embedding layer. it must be a matrix of shape (max_words embedding_dim) where each entry i contains the embedding_dim-dimensional vector for the word of index i in the reference word index (built during tokenization). retrieve the vocabulary indexed by embedding_dim = 100 our previous textvectorization layer. vocabulary = text_vectorization.get_vocabulary() word_index = dict(zip(vocabulary range(len(vocabulary)))) embedding_matrix = np.zeros((max_tokens embedding_dim)) for word i in word_index.items() if i  1500 sequence model figure 11.11 a simple heuristic for number of samples selecting a text-classification model the ratio between the number of training mean sample length samples and the mean number of words >> import random >>> print(random.choice(text_pairs)) ("soccer is more popular than tennis." "[start] el f tbol es m s popular que el tenis. [end]") let s shuffle them and split them into the usual training validation and test sets import random random.shuffle(text_pairs) num_val_samples = int(0.15 * len(text_pairs)) num_train_samples = len(text_pairs) - 2 * num_val_samples train_pairs = text_pairs[num_train_samples] val_pairs = text_pairs[num_train_samplesnum_train_samples + num_val_samples] test_pairs = text_pairs[num_train_samples + num_val_samples] next let s prepare two separate textvectorization layers one for english and one for spanish. we re going to need to customize the way strings are preprocessed we need to preserve the "[start]" and "[end]" tokens that we ve inserted. by default the characters [ and ] would be stripped but we want to keep them around so we can tell apart the word start and the start token "[start]". punctuation is different from language to language! in the spanish text-vectorization layer if we re going to strip punctuation characters we need to also strip the character . note that for a non-toy translation model we would treat punctuation characters as sep.arate tokens rather than stripping them since we would want to be able to generate cor.rectly punctuated sentences. in our case for simplicity we ll get rid of all punctuation. import tensorflow as tf import string import re strip_chars = string.punctuation + " " strip_chars = strip_chars.replace("[" "") strip_chars = strip_chars.replace("]" "") def custom_standardization(input_string) lowercase = tf.strings.lower(input_string) return tf.strings.regex_replace( lowercase f"[{re.escape(strip_chars)}]" "") prepare a custom string standardization function for the spanish textvectorization layer it preserves [ and ] but strips (as well as all other characters from strings.punctuation). beyond text classification sequence-to-sequence learning vocab_size = 15000 to keep things simple we ll only look at sequence_length = 20 the top 15000 words in each language and we ll restrict sentences to 20 words. source_vectorization = layers.textvectorization( the english max_tokens=vocab_size layer output_mode="int" output_sequence_length=sequence_length ) the spanish layer target_vectorization = layers.textvectorization( max_tokens=vocab_size output_mode="int" output_sequence_length=sequence_length + 1 standardize=custom_standardization ) train_english_texts = [pair for pair in train_pairs] train_spanish_texts = [pair for pair in train_pairs] generate spanish sentences that have one extra token since we ll need to offset the sentence by one step during training. source_vectorization.adapt(train_english_texts) learn the vocabulary target_vectorization.adapt(train_spanish_texts) of each language. finally we can turn our data into a tf.data pipeline. we want it to return a tuple (inputs target) where inputsis a dict with two keys encoder_inputs (the english sentence) and decoder_inputs (the spanish sentence) and target is the spanish sentence offset by one step ahead. batch_size = 64 def format_dataset(eng spa) eng = source_vectorization(eng) the input spanish sentence spa = target_vectorization(spa) doesn t include the last token return ({ to keep inputs and targets at "english" eng the same length. "spanish" spa[ -1] } spa[ 1]) the target spanish sentence is one step ahead. both are still def make_dataset(pairs) the same length (20 words). eng_texts spa_texts = zip(*pairs) eng_texts = list(eng_texts) spa_texts = list(spa_texts) dataset = tf.data.dataset.from_tensor_slices((eng_texts spa_texts)) dataset = dataset.batch(batch_size) dataset = dataset.map(format_dataset num_parallel_calls=4) return dataset.shuffle(2048).prefetch(16).cache() use in-memory caching to speed up train_ds = make_dataset(train_pairs) preprocessing. val_ds = make_dataset(val_pairs) here s what our dataset outputs look like >>> for inputs targets in train_ds.take(1) >>> print(f"inputs['english'].shape {inputs['english'].shape}") >>> print(f"inputs['spanish'].shape {inputs['spanish'].shape}") >>> print(f"targets.shape {targets.shape}") inputs["encoder_inputs"].shape (64 20) inputs["decoder_inputs"].shape (64 20) targets.shape (64 20) the data is now ready time to build some models. we ll start with a recurrent sequence-to-sequence model before moving on to a transformer. 11.5.2 sequence-to-sequence learning with rnns recurrent neural networks dominated sequence-to-sequence learning from 2015 2017 before being overtaken by transformer. they were the basis for many real-world machine-translation systems as mentioned in chapter 10 google translate circa 2017 was powered by a stack of seven large lstm layers. it s still worth learning about this approach today as it provides an easy entry point to understanding sequence-to-sequence models. the simplest naive way to use rnns to turn a sequence into another sequence is to keep the output of the rnn at each time step. in keras it would look like this inputs = keras.input(shape=(sequence_length) dtype="int64") x = layers.embedding(input_dim=vocab_size output_dim=128)(inputs) x = layers.lstm(32 return_sequences=true)(x) outputs = layers.dense(vocab_size activation="softmax")(x) model = keras.model(inputs outputs) however there are two major issues with this approach the target sequence must always be the same length as the source sequence. in practice this is rarely the case. technically this isn t critical as you could always pad either the source sequence or the target sequence to make their lengths match. due to the step-by-step nature of rnns the model will only be looking at tokens 0 n in the source sequence in order to predict token n in the target sequence. this constraint makes this setup unsuitable for most tasks and particularly translation. consider translating the weather is nice today to french that would be il fait beau aujourd hui. you d need to be able to predict il from just the il fait from just the weather etc. which is simply impossible. if you re a human translator you d start by reading the entire source sentence before starting to translate it. this is especially important if you re dealing with languages that have wildly different word ordering like english and japanese. and that s exactly what standard sequence-to-sequence models do. in a proper sequence-to-sequence setup (see figure 11.13) you would first use an rnn (the encoder) to turn the entire source sequence into a single vector (or set of vectors). this could be the last output of the rnn or alternatively its final internal state vectors. then you would use this vector (or vectors) as the initial state of another beyond text classification sequence-to-sequence learning final output vector qu tiempo hace hoy [end] or nal internal state how is the weather today [start] qu tiempo hace hoy initial state for an rnn decoder. rnn (the decoder) which would look at elements 0 n in the target sequence and try to predict step n+1 in the target sequence. let s implement this in keras with gru-based encoders and decoders. the choice of gru rather than lstm makes things a bit simpler since gru only has a single state vector whereas lstm has multiple. let s start with the encoder. don t forget masking from tensorflow import keras it s critical in this setup. from tensorflow.keras import layers the english source sentence goes here. embed_dim = 256 specifying the name of the input enables latent_dim = 1024 us to fit() the model with a dict of inputs. source = keras.input(shape=(none) dtype="int64" name="english") x = layers.embedding(vocab_size embed_dim mask_zero=true)(source) encoded_source = layers.bidirectional( our encoded source layers.gru(latent_dim) merge_mode="sum")(x) sentence is the last output of a bidirectional gru. next let s add the decoder a simple gru layer that takes as its initial state the encoded source sentence. on top of it we add a dense layer that produces for each output step a probability distribution over the spanish vocabulary. the spanish target sentence goes here. don t forget masking. past_target = keras.input(shape=(none) dtype="int64" name="spanish") x=layers.embedding(vocab_sizeembed_dim mask_zero=true)(past_target) decoder_gru = layers.gru(latent_dim return_sequences=true) x=decoder_gru(xinitial_state=encoded_source) predicts the x = layers.dropout(0.5)(x) next token target_next_step = layers.dense(vocab_size activation="softmax")(x) seq2seq_rnn = keras.model([source past_target] target_next_step) the encoded source sentence end-to-end model maps the source serves as the initial state of sentence and the target sentence to the the decoder gru. target sentence one step in the future during training the decoder takes as input the entire target sequence but thanks to the step-by-step nature of rnns it only looks at tokens 0 n in the input to predict token n in the output (which corresponds to the next token in the sequence since the output is intended to be offset by one step). this means we only use information from the past to predict the future as we should otherwise we d be cheating and our model would not work at inference time. let s start training. seq2seq_rnn.compile( optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) seq2seq_rnn.fit(train_ds epochs=15 validation_data=val_ds) we picked accuracy as a crude way to monitor validation-set performance during training. we get to 64% accuracy on average the model predicts the next word in the spanish sentence correctly 64% of the time. however in practice next-token accuracy isn t a great metric for machine translation models in particular because it makes the assumption that the correct target tokens from 0 to n are already known when pre.dicting token n+1. in reality during inference you re generating the target sentence from scratch and you can t rely on previously generated tokens being 100% correct. if you work on a real-world machine translation system you will likely use bleu scores to evaluate your models a metric that looks at entire generated sequences and that seems to correlate well with human perception of translation quality. at last let s use our model for inference. we ll pick a few sentences in the test set and check how our model translates them. we ll start from the seed token "[start]" and feed it into the decoder model together with the encoded english source sen.tence. we ll retrieve a next-token prediction and we ll re-inject it into the decoder repeatedly sampling one new target token at each iteration until we get to "[end]" or reach the maximum sentence length. prepare a dict to convert token index predictions to string tokens. import numpy as np spa_vocab = target_vectorization.get_vocabulary() spa_index_lookup = dict(zip(range(len(spa_vocab)) spa_vocab)) max_decoded_sentence_length = 20 def decode_sequence(input_sentence) seed tokenized_input_sentence = source_vectorization([input_sentence]) token decoded_sentence = "[start]" for i in range(max_decoded_sentence_length) tokenized_target_sentence = target_vectorization([decoded_sentence]) next_token_predictions = seq2seq_rnn.predict( sample the [tokenized_input_sentence tokenized_target_sentence]) next token. sampled_token_index = np.argmax(next_token_predictions[0 i ]) beyond text classification sequence-to-sequence learning sampled_token = spa_index_lookup[sampled_token_index] convert the next decoded_sentence += " " + sampled_token token prediction to a string and append if sampled_token == "[end]" exit condition it to the generated break either hit max sentence. return decoded_sentence length or sample a stop character test_eng_texts = [pair for pair in test_pairs] for _ in range(20) input_sentence = random.choice(test_eng_texts) print("-") print(input_sentence) print(decode_sequence(input_sentence)) note that this inference setup while very simple is rather inefficient since we repro.cess the entire source sentence and the entire generated target sentence every time we sample a new word. in a practical application you d factor the encoder and the decoder as two separate models and your decoder would only run a single step at each token-sampling iteration reusing its previous internal state. here are our translation results. our model works decently well for a toy model though it still makes many basic mistakes. who is in this room? [start] qui n est en esta habitaci n [end] -that doesn't sound too dangerous. [start] eso no es muy dif cil [end] -no one will stop me. [start] nadie me va a hacer [end] -tom is friendly. [start] tom es un buen [unk] [end] there are many ways this toy model could be improved we could use a deep stack of recurrent layers for both the encoder and the decoder (note that for the decoder this makes state management a bit more involved). we could use an lstm instead of a gru. and so on. beyond such tweaks however the rnn approach to sequence-to-sequence learning has a few fundamental limitations the source sequence representation has to be held entirely in the encoder state vector(s) which puts significant limitations on the size and complexity of the sentences you can translate. it s a bit as if a human were translating a sentence entirely from memory without looking twice at the source sentence while pro.ducing the translation. rnns have trouble dealing with very long sequences since they tend to pro.gressively forget about the past by the time you ve reached the 100th token in either sequence little information remains about the start of the sequence. that means rnn-based models can t hold onto long-term context which can be essential for translating long documents. these limitations are what has led the machine learning community to embrace the transformer architecture for sequence-to-sequence problems. let s take a look. 11.5.3 sequence-to-sequence learning with transformer sequence-to-sequence learning is the task where transformer really shines. neural attention enables transformer models to successfully process sequences that are con.siderably longer and more complex than those rnns can handle. as a human translating english to spanish you re not going to read the english sentence one word at a time keep its meaning in memory and then generate the spanish sentence one word at a time. that may work for a five-word sentence but it s unlikely to work for an entire paragraph. instead you ll probably want to go back and forth between the source sentence and your translation in progress and pay attention to different words in the source as you re writing down different parts of your translation. that s exactly what you can achieve with neural attention and transformers. you re already familiar with the transformer encoder which uses self-attention to pro.duce context-aware representations of each token in an input sequence. in a sequence-to-sequence transformer the transformer encoder would naturally play the role of the encoder which reads the source sequence and produces an encoded rep.resentation of it. unlike our previous rnn encoder though the transformer encoder keeps the encoded representation in a sequence format it s a sequence of context-aware embedding vectors. the second half of the model is the transformer decoder. just like the rnn decoder it reads tokens 0 n in the target sequence and tries to predict token n+1. crucially while doing this it uses neural attention to identify which tokens in the encoded source sentence are most closely related to the target token it s currently trying to predict perhaps not unlike what a human translator would do. recall the query-key-value model in a transformer decoder the target sequence serves as an attention query that is used to to pay closer attention to different parts of the source sequence (the source sequence plays the roles of both keys and values). the transformer decoder figure 11.14 shows the full sequence-to-sequence transformer. look at the decoder internals you ll recognize that it looks very similar to the transformer encoder except that an extra attention block is inserted between the self-attention block applied to the target sequence and the dense layers of the exit block. let s implement it. like for the transformerencoder we ll use a layer subclass. before we focus on the call() method where the action happens let s start by defin. ing the class constructor containing the layers we re going to need. beyond text classification sequence-to-sequence learning class transformerdecoder(layers.layer) def __init__(self embed_dim dense_dim num_heads **kwargs) super().__init__(**kwargs) self.embed_dim = embed_dim self.dense_dim = dense_dim self.num_heads = num_heads self.attention_1 = layers.multiheadattention( num_heads=num_heads key_dim=embed_dim) self.attention_2 = layers.multiheadattention( num_heads=num_heads key_dim=embed_dim) self.dense_proj = keras.sequential( [layers.dense(dense_dim activation="relu") layers.dense(embed_dim)] ) self.layernorm_1 = layers.layernormalization() self.layernorm_2 = layers.layernormalization() self.layernorm_3 = layers.layernormalization() self.supports_masking = true def get_config(self) config = super().get_config() config.update({ "embed_dim" self.embed_dim "num_heads" self.num_heads "dense_dim" self.dense_dim }) return config this attribute ensures that the layer will propagate its input mask to its outputs masking in keras is explicitly opt-in. if you pass a mask to a layer that doesn t implement compute_mask() and that doesn t expose this supports_masking attribute that s an error. the call() method is almost a straightforward rendering of the connectivity dia.gram from figure 11.14. but there s an additional detail we need to take into account causal padding. causal padding is absolutely critical to successfully training a sequence-to-sequence transformer. unlike an rnn which looks at its input one step at a time and thus will only have access to steps 0...n to generate output step n (which is token n+1 in the target sequence) the transformerdecoder is order-agnos.tic it looks at the entire target sequence at once. if it were allowed to use its entire input it would simply learn to copy input step n+1 to location n in the output. the model would thus achieve perfect training accuracy but of course when running inference it would be completely useless since input steps beyond n aren t available. the fix is simple we ll mask the upper half of the pairwise attention matrix to pre.vent the model from paying any attention to information from the future only infor.mation from tokens 0...n in the target sequence should be used when generating target token n+1. to do this we ll add a get_causal_attention_mask(self inputs) method to our transformerdecoder to retrieve an attention mask that we can pass to our multiheadattention layers. generate matrix of shape (sequence_length sequence_length) with 1s in one half and 0s in the other. def get_causal_attention_mask(self inputs) input_shape = tf.shape(inputs) batch_size sequence_length = input_shape input_shape i = tf.range(sequence_length)[ tf.newaxis] j = tf.range(sequence_length) mask = tf.cast(i >= j dtype="int32") replicate it along the batch axis to get a matrix of shape (batch_size sequence_length sequence_length). mask = tf.reshape(mask (1 input_shape input_shape)) mult = tf.concat( [tf.expand_dims(batch_size -1) tf.constant([1 1] dtype=tf.int32)] axis=0) return tf.tile(mask mult) beyond text classification sequence-to-sequence learning now we can write down the full call() method implementing the forward pass of the decoder. def call(self inputs encoder_outputs mask=none) causal_mask = self.get_causal_attention_mask(inputs) retrieve if mask is not none the causal padding_mask = tf.cast( mask. mask[ tf.newaxis ] dtype="int32") prepare the input mask (that describes padding locations in the target sequence). padding_mask = tf.minimum(padding_mask causal_mask) merge the attention_output_1 = self.attention_1( two maskspass the causal mask to the query=inputs together. first attention layer which performs self-attention over value=inputs key=inputs the target sequence. attention_mask=causal_mask) attention_output_1 = self.layernorm_1(inputs + attention_output_1) attention_output_2 = self.attention_2( query=attention_output_1 pass the combined mask to the second attention layer which relates the source sequence to value=encoder_outputs key=encoder_outputs the target sequence. attention_mask=padding_mask ) attention_output_2 = self.layernorm_2( attention_output_1 + attention_output_2) proj_output = self.dense_proj(attention_output_2) return self.layernorm_3(attention_output_2 + proj_output) putting it all together a transformer for machine translation the end-to-end transformer is the model we ll be training. it maps the source sequence and the target sequence to the target sequence one step in the future. it straightforwardly combines the pieces we ve built so far positionalembedding layers the transformerencoder and the transformerdecoder. note that both the trans.formerencoder and the transformerdecoder are shape-invariant so you could be stacking many of them to create a more powerful encoder or decoder. in our exam.ple we ll stick to a single instance of each. embed_dim = 256 dense_dim = 2048 encode the num_heads = 8 source sentence. encoder_inputs = keras.input(shape=(none) dtype="int64" name="english") x = positionalembedding(sequence_length vocab_size embed_dim)(encoder_inputs) encoder_outputs = transformerencoder(embed_dim dense_dim num_heads)(x) decoder_inputs = keras.input(shape=(none) dtype="int64" name="spanish") x = positionalembedding(sequence_length vocab_size embed_dim)(decoder_inputs) x = transformerdecoder(embed_dim dense_dim num_heads)(x encoder_outputs) x = layers.dropout(0.5)(x) encode the target sentence and combine it with the encoded source sentence. decoder_outputs = layers.dense(vocab_size activation="softmax")(x) transformer = keras.model([encoder_inputs decoder_inputs] decoder_outputs) predict a word for each output position. we re now ready to train our model we get to 67% accuracy a good deal above the gru-based model. transformer.compile( optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) transformer.fit(train_ds epochs=30 validation_data=val_ds) finally let s try using our model to translate never-seen-before english sentences from the test set. the setup is identical to what we used for the sequence-to-sequence rnn model. import numpy as np spa_vocab = target_vectorization.get_vocabulary() spa_index_lookup = dict(zip(range(len(spa_vocab)) spa_vocab)) max_decoded_sentence_length = 20 def decode_sequence(input_sentence) tokenized_input_sentence = source_vectorization([input_sentence]) decoded_sentence = "[start]" for i in range(max_decoded_sentence_length) tokenized_target_sentence = target_vectorization( [decoded_sentence])[ -1] predictions = transformer( [tokenized_input_sentence tokenized_target_sentence]) sample the sampled_token_index = np.argmax(predictions[0 i ]) next token. sampled_token = spa_index_lookup[sampled_token_index] convert the next token decoded_sentence += " " + sampled_token prediction to if sampled_token == "[end]" exit condition a string and break append it to the generated sentence. return decoded_sentence test_eng_texts = [pair for pair in test_pairs] for _ in range(20) input_sentence = random.choice(test_eng_texts) print("-") print(input_sentence) print(decode_sequence(input_sentence)) subjectively the transformer seems to perform significantly better than the gru-based translation model. it s still a toy model but it s a better toy model. this is a song i learned when i was a kid. [start] esta es una canci n que aprend cuando era chico [end] - while the source sentence wasn t she can play the piano. gendered this translation assumes [start] ella puede tocar piano [end] a male speaker. keep in mind that - translation models will often make i'm not who you think i am. unwarranted assumptions about [start] no soy la persona que t creo que soy [end] their input data which leads to - algorithmic bias. in the worst it may have rained a little last night. cases a model might hallucinate [start] puede que llueve un poco el pasado [end] memorized information that has nothing to do with the data it s currently processing. that concludes this chapter on natural language processing you just went from the very basics to a fully fledged transformer that can translate from english to spanish. teaching machines to make sense of language is the latest superpower you can add to your collection. summary there are two kinds of nlp models bag-of-words models that process sets of words or n-grams without taking into account their order and sequence models that pro.cess word order. a bag-of-words model is made of dense layers while a sequence model could be an rnn a 1d convnet or a transformer. when it comes to text classification the ratio between the number of samples in your training data and the mean number of words per sample can help you determine whether you should use a bag-of-words model or a sequence model. word embeddings are vector spaces where semantic relationships between words are modeled as distance relationships between vectors that represent those words. sequence-to-sequence learning is a generic powerful learning framework that can be applied to solve many nlp problems including machine translation. a sequence.to-sequence model is made of an encoder which processes a source sequence and a decoder which tries to predict future tokens in target sequence by looking at past tokens with the help of the encoder-processed source sequence. neural attention is a way to create context-aware word representations. it s the basis for the transformer architecture. the transformer architecture which consists of a transformerencoder and a transformerdecoder yields excellent results on sequence-to-sequence tasks. the first half the transformerencoder can also be used for text classification or any sort of single-input nlp task. this chapter covers text generation deepdream neural style transfer variational autoencoders generative adversarial networks the potential of artificial intelligence to emulate human thought processes goes beyond passive tasks such as object recognition and mostly reactive tasks such as driving a car. it extends well into creative activities. when i first made the claim that in a not-so-distant future most of the cultural content that we consume will be cre.ated with substantial help from ais i was met with utter disbelief even from long.time machine learning practitioners. that was in 2014. fast-forward a few years and the disbelief had receded at an incredible speed. in the summer of 2015 we were entertained by google s deepdream algorithm turning an image into a psy.chedelic mess of dog eyes and pareidolic artifacts in 2016 we started using smart.phone applications to turn photos into paintings of various styles. in the summer of 2016 an experimental short movie sunspring was directed using a script written by 364 a long short-term memory. maybe you ve recently listened to music that was tenta.tively generated by a neural network. granted the artistic productions we ve seen from ai so far have been fairly low quality. ai isn t anywhere close to rivaling human screenwriters painters and compos.ers. but replacing humans was always beside the point artificial intelligence isn t about replacing our own intelligence with something else it s about bringing into our lives and work more intelligence intelligence of a different kind. in many fields but especially in creative ones ai will be used by humans as a tool to augment their own capabilities more augmented intelligence than artificial intelligence. a large part of artistic creation consists of simple pattern recognition and technical skill. and that s precisely the part of the process that many find less attractive or even dispensable. that s where ai comes in. our perceptual modalities our language and our artwork all have statistical structure. learning this structure is what deep learning algorithms excel at. machine learning models can learn the statistical latent space of images music and stories and they can then sample from this space creating new art.works with characteristics similar to those the model has seen in its training data. naturally such sampling is hardly an act of artistic creation in itself. it s a mere mathe.matical operation the algorithm has no grounding in human life human emotions or our experience of the world instead it learns from an experience that has little in common with ours. it s only our interpretation as human spectators that will give meaning to what the model generates. but in the hands of a skilled artist algorithmic generation can be steered to become meaningful and beautiful. latent space sam.pling can become a brush that empowers the artist augments our creative affor.dances and expands the space of what we can imagine. what s more it can make artistic creation more accessible by eliminating the need for technical skill and prac.tice setting up a new medium of pure expression factoring art apart from craft. iannis xenakis a visionary pioneer of electronic and algorithmic music beauti. fully expressed this same idea in the 1960s in the context of the application of auto. mation technology to music composition1 freed from tedious calculations the composer is able to devote himself to the general problems that the new musical form poses and to explore the nooks and crannies of this form while modifying the values of the input data. for example he may test all instrumental combinations from soloists to chamber orchestras to large orchestras. with the aid of electronic computers the composer becomes a sort of pilot he presses the buttons introduces coordinates and supervises the controls of a cosmic vessel sailing in the space of sound across sonic constellations and galaxies that he could formerly glimpse only as a distant dream. in this chapter we ll explore from various angles the potential of deep learning to augment artistic creation. we ll review sequence data generation (which can be 1 iannis xenakis musiques formelles nouveaux principes formels de composition musicale special issue of la revue musicale nos. 253 254 (1963). used to generate text or music) deepdream and image generation using both vari.ational autoencoders and generative adversarial networks. we ll get your computer to dream up content never seen before and maybe we ll get you to dream too about the fantastic possibilities that lie at the intersection of technology and art. let s get started. 12.1 text generation in this section we ll explore how recurrent neural networks can be used to generate sequence data. we ll use text generation as an example but the exact same tech.niques can be generalized to any kind of sequence data you could apply it to sequences of musical notes in order to generate new music to timeseries of brush.stroke data (perhaps recorded while an artist paints on an ipad) to generate paintings stroke by stroke and so on. sequence data generation is in no way limited to artistic content generation. it has been successfully applied to speech synthesis and to dialogue generation for chatbots. the smart reply feature that google released in 2016 capable of automat.ically generating a selection of quick replies to emails or text messages is powered by similar techniques. 12.1.1 a brief history of generative deep learning for sequence generation in late 2014 few people had ever seen the initials lstm even in the machine learn.ing community. successful applications of sequence data generation with recurrent networks only began to appear in the mainstream in 2016. but these techniques have a fairly long history starting with the development of the lstm algorithm in 1997 (discussed in chapter 10). this new algorithm was used early on to generate text char.acter by character. in 2002 douglas eck then at schmidhuber s lab in switzerland applied lstm to music generation for the first time with promising results. eck is now a researcher at google brain and in 2016 he started a new research group there called magenta focused on applying modern deep learning techniques to produce engaging music. sometimes good ideas take 15 years to get started. in the late 2000s and early 2010s alex graves did important pioneering work on using recurrent networks for sequence data generation. in particular his 2013 work on applying recurrent mixture density networks to generate human-like handwriting using timeseries of pen positions is seen by some as a turning point.2 this specific application of neural networks at that specific moment in time captured for me the notion of machines that dream and was a significant inspiration around the time i started developing keras. graves left a similar commented-out remark hidden in a 2013 latex file uploaded to the preprint server arxiv generating sequential data is 2 alex graves generating sequences with recurrent neural networks arxiv (2013) https//arxiv.org/ abs/1308.0850. the closest computers get to dreaming. several years later we take a lot of these devel.opments for granted but at the time it was difficult to watch graves s demonstrations and not walk away awe-inspired by the possibilities. between 2015 and 2017 recurrent neural networks were successfully used for text and dialogue generation music gener.ation and speech synthesis. then around 2017 2018 the transformer architecture started taking over recur.rent neural networks not just for supervised natural language processing tasks but also for generative sequence models in particular language modeling (word-level text generation). the best-known example of a generative transformer would be gpt-3 a 175 billion parameter text-generation model trained by the startup openai on an astoundingly large text corpus including most digitally available books wikipedia and a large fraction of a crawl of the entire internet. gpt-3 made headlines in 2020 due to its capability to generate plausible-sounding text paragraphs on virtually any topic a prowess that has fed a short-lived hype wave worthy of the most torrid ai summer. 12.1.2 how do you generate sequence data? the universal way to generate sequence data in deep learning is to train a model (usu.ally a transformer or an rnn) to predict the next token or next few tokens in a sequence using the previous tokens as input. for instance given the input the cat is on the the model is trained to predict the target mat the next word. as usual when working with text data tokens are typically words or characters and any network that can model the probability of the next token given the previous ones is called a language model. a language model captures the latent space of language its statistical structure. once you have such a trained language model you can sample from it (generate new sequences) you feed it an initial string of text (called conditioning data) ask it to generate the next character or the next word (you can even generate several tokens at once) add the generated output back to the input data and repeat the process many times (see figure 12.1). this loop allows you to generate sequences of arbitrary length that reflect the structure of the data on which the model was trained sequences that look almost like human-written sentences. probability distribution over sampled next initial text initial text next word word the cat sat on the mat the cat sat on the mat which figure 12.1 the process of word-by-word text generation using a language model 12.1.3 the importance of the sampling strategy when generating text the way you choose the next token is crucially important. a naive approach is greedy sampling consisting of always choosing the most likely next character. but such an approach results in repetitive predictable strings that don t look like coherent language. a more interesting approach makes slightly more sur.prising choices it introduces randomness in the sampling process by sampling from the probability distribution for the next character. this is called stochastic sampling (recall that stochasticity is what we call randomness in this field). in such a setup if a word has probability 0.3 of being next in the sentence according to the model you ll choose it 30% of the time. note that greedy sampling can also be cast as sampling from a probability distribution one where a certain word has probability 1 and all oth.ers have probability 0. sampling probabilistically from the softmax output of the model is neat it allows even unlikely words to be sampled some of the time generating more interesting-looking sentences and sometimes showing creativity by coming up with new realistic-sounding sentences that didn t occur in the training data. but there s one issue with this strategy it doesn t offer a way to control the amount of randomness in the sampling process. why would you want more or less randomness? consider an extreme case pure random sampling where you draw the next word from a uniform probability distribu.tion and every word is equally likely. this scheme has maximum randomness in other words this probability distribution has maximum entropy. naturally it won t produce anything interesting. at the other extreme greedy sampling doesn t produce anything interesting either and has no randomness the corresponding probability distribution has minimum entropy. sampling from the real probability distribu.tion the distribution that is output by the model s softmax function constitutes an intermediate point between these two extremes. but there are many other intermedi.ate points of higher or lower entropy that you may want to explore. less entropy will give the generated sequences a more predictable structure (and thus they will poten.tially be more realistic looking) whereas more entropy will result in more surprising and creative sequences. when sampling from generative models it s always good to explore different amounts of randomness in the generation process. because we humans are the ultimate judges of how interesting the generated data is interesting.ness is highly subjective and there s no telling in advance where the point of optimal entropy lies. in order to control the amount of stochasticity in the sampling process we ll intro.duce a parameter called the softmax temperature which characterizes the entropy of the probability distribution used for sampling it characterizes how surprising or predict.able the choice of the next word will be. given a temperature value a new probability distribution is computed from the original one (the softmax output of the model) by reweighting it in the following way. original_distribution is a 1d numpy array of probability values that must sum to 1. temperature is a factor quantifying the entropy of the output distribution. import numpy as np def reweight_distribution(original_distribution temperature=0.5) distribution = np.log(original_distribution) / temperature distribution = np.exp(distribution) return distribution / np.sum(distribution) returns a reweighted version of the original distribution. the sum of the distribution may no longer be 1 so you divide it by its sum to obtain the new distribution. higher temperatures result in sampling distributions of higher entropy that will gener.ate more surprising and unstructured generated data whereas a lower temperature will result in less randomness and much more predictable generated data (see figure 12.2). temperature = 0.01 temperature = 0.2 temperature = 0.4 12.1.4 implementing text generation with keras let s put these ideas into practice in a keras implementation. the first thing you need is a lot of text data that you can use to learn a language model. you can use any suffi.ciently large text file or set of text files wikipedia the lord of the rings and so on. in this example we ll keep working with the imdb movie review dataset from the last chapter and we ll learn to generate never-read-before movie reviews. as such our language model will be a model of the style and topics of these movie reviews specifi.cally rather than a general model of the english language. preparing the data just like in the previous chapter let s download and uncompress the imdb movie reviews dataset. !wget https//ai.stanford.edu/~amaas/data/sentiment/aclimdb_v1.tar.gz !tar -xf aclimdb_v1.tar.gz you re already familiar with the structure of the data we get a folder named aclimdb containing two subfolders one for negative-sentiment movie reviews and one for positive-sentiment reviews. there s one text file per review. we ll call text_dataset_ from_directory with label_mode=none to create a dataset that reads from these files and yields the text content of each file. strip the  html tag that occurs in many of the reviews. this did not matter much for text classification import tensorflow as tf but we wouldn t want to generate  from tensorflow import keras tags in this example! dataset = keras.utils.text_dataset_from_directory( directory="aclimdb" label_mode=none batch_size=256) dataset = dataset.map(lambda x tf.strings.regex_replace(x "" " ")) now let s use a textvectorization layer to compute the vocabulary we ll be working with. we ll only use the first sequence_length words of each review our textvector.ization layer will cut off anything beyond that when vectorizing a text. from tensorflow.keras.layers import textvectorization we ll only consider the top 15000 most sequence_length = 100 common words anything else will be treated vocab_size = 15000 as the out-of-vocabulary token "[unk]". text_vectorization = textvectorization( max_tokens=vocab_size we want to return integer output_mode="int" word index sequences. output_sequence_length=sequence_length ) we ll work with inputs and targets of length 100 (but since we ll offset the targets by 1 the model will actually see sequences of length 99). text_vectorization.adapt(dataset) let s use the layer to create a language modeling dataset where input samples are vec.torized texts and corresponding targets are the same texts offset by one word. convert a batch of texts (strings) to a batch of integer sequences. def prepare_lm_dataset(text_batch) vectorized_sequences = text_vectorization(text_batch) x = vectorized_sequences[ -1] create inputs by cutting y = vectorized_sequences[ 1] off the last word of the return x y sequences. lm_dataset = dataset.map(prepare_lm_dataset num_parallel_calls=4) create targets by offsetting the sequences by 1. a transformer-based sequence-to-sequence model we ll train a model to predict a probability distribution over the next word in a sen.tence given a number of initial words. when the model is trained we ll feed it with a prompt sample the next word add that word back to the prompt and repeat until we ve generated a short paragraph. like we did for temperature forecasting in chapter 10 we could train a model that takes as input a sequence of n words and simply predicts word n+1. however there are several issues with this setup in the context of sequence generation. first the model would only learn to produce predictions when n words were avail.able but it would be useful to be able to start predicting with fewer than n words. otherwise we d be constrained to only use relatively long prompts (in our implemen.tation n=100 words). we didn t have this need in chapter 10. second many of our training sequences will be mostly overlapping. consider n=4. the text a complete sentence must have at minimum three things a subject verb and an object would be used to generate the following training sequences a complete sentence must complete sentence must have sentence must have at and so on until verb and an object a model that treats each such sequence as an independent sample would have to do a lot of redundant work re-encoding multiple times subsequences that it has largely seen before. in chapter 10 this wasn t much of a problem because we didn t have that many training samples in the first place and we needed to benchmark dense and con.volutional models for which redoing the work every time is the only option. we could try to alleviate this redundancy problem by using strides to sample our sequences skipping a few words between two consecutive samples. but that would reduce our number of training samples while only providing a partial solution. to address these two issues we ll use a sequence-to-sequence model we ll feed sequences of n words (indexed from 0 to n) into our model and we ll predict the sequence offset by one (from 1 to n+1). we ll use causal masking to make sure that for any i the model will only be using words from 0 to i in order to predict the word i+ 1. this means that we re simultaneously training the model to solve n mostly overlapping but different problems predicting the next words given a sequence of 1 max_loss break print(f"... loss value at step {i} {loss.2f}") return image break out if the loss crosses a certain threshold (over-optimizing would create unwanted image artifacts). repeatedly update the image in a way that increases the deepdream loss. finally the outer loop of the deepdream algorithm. first we ll define a list of scales (also called octaves) at which to process the images. we ll process our image over three different such octaves. for each successive octave from the smallest to the largest we ll run 20 gradient ascent steps via gradient_ascent_loop() to maximize the loss we previously defined. between each octave we ll upscale the image by 40% (1.4x) we ll start by processing a small image and then increasingly scale it up (see figure 12.6). we define the parameters of this process in the following code. tweaking these parameters will allow you to achieve new effects! detail detail reinjection reinjection dream upscale dream upscale dream octave 1 octave 2 octave 3 gradient ascent step size number of scales at which to run gradient ascent step = 20. num_octave = 3 size ratio between octave_scale = 1.4 successive scales iterations = 30 max_loss = 15. number of gradient we ll stop the gradient ascent process for ascent steps per scale a scale if the loss gets higher than this. we re also going to need a couple of utility functions to load and save images. import numpy as np util function to open resize and format pictures into def preprocess_image(image_path) appropriate arrays img = keras.utils.load_img(image_path) img = keras.utils.img_to_array(img) img = np.expand_dims(img axis=0) img = keras.applications.inception_v3.preprocess_input(img) return img util function to convert a numpy array into a valid image def deprocess_image(img) img = img.reshape((img.shape img.shape 3)) img /= 2.0 undo inception v3 convert to uint8 img += 0.5 preprocessing. and clip to the valid img *= 255. range [0 255]. img = np.clip(img 0 255).astype("uint8") return img this is the outer loop. to avoid losing a lot of image detail after each successive scale-up (resulting in increasingly blurry or pixelated images) we can use a simple trick after each scale-up we ll re-inject the lost details back into the image which is possible because we know what the original image should look like at the larger scale. given a small image size s and a larger image size l we can compute the difference between the original image resized to size l and the original resized to size s this difference quantifies the details lost when going from s to l. load the test image. iterate over the different octaves. scale up the dream image. original_img = preprocess_image(base_image_path) original_shape = original_img.shape successive_shapes = [original_shape] for i in range(1 num_octave) compute the target shape of the image at different octaves. shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape]) successive_shapes.append(shape) successive_shapes = successive_shapes[-1] shrunk_original_img = tf.image.resize(original_img successive_shapes) img = tf.identity(original_img) for i shape in enumerate(successive_shapes) print(f"processing octave {i} with shape {shape}") img = tf.image.resize(img shape) img = gradient_ascent_loop( make a copy of the image (we need to keep the original around). run gradient ascent altering the dream. img iterations=iterations learning_rate=step max_loss=max_loss ) upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img shape) same_size_original = tf.image.resize(original_img shape) lost_detail = same_size_original - upscaled_shrunk_original_img img += lost_detail shrunk_original_img = tf.image.resize(original_img shape) keras.utils.save_img("dream.png" deprocess_image(img.numpy())) compute the high-quality version of the original image at this size. scale up the smaller version of the original image it will be pixellated. save the final result. re-inject lost detail into the dream. the difference between the two is the detail that was lost when scaling up. note because the original inception v3 network was trained to recognize con.cepts in images of size 299 ! 299 and given that the process involves scaling the images down by a reasonable factor the deepdream implementation produces much better results on images that are somewhere between 300 ! 300 and 400 ! 400. regardless you can run the same code on images of any size and any ratio. on a gpu it only takes a few seconds to run the whole thing. figure 12.7 shows the result of our dream configuration on the test image. i strongly suggest that you explore what you can do by adjusting which layers you use in your loss. layers that are lower in the network contain more-local less-abstract representations and lead to dream patterns that look more geometric. layers that are higher up lead to more-recognizable visual patterns based on the most common objects found in imagenet such as dog eyes bird feathers and so on. you can use random gen.eration of the parameters in the layer_settings dictionary to quickly explore many different layer combinations. figure 12.8 shows a range of results obtained on an image of a delicious homemade pastry using different layer configurations. 12.2.2 wrapping up deepdream consists of running a convnet in reverse to generate inputs based on the representations learned by the network. the results produced are fun and somewhat similar to the visual artifacts induced in humans by the disruption of the visual cortex via psychedelics. note that the process isn t specific to image models or even to convnets. it can be done for speech music and more. 12.3 neural style transfer in addition to deepdream another major development in deep-learning-driven image modification is neural style transfer introduced by leon gatys et al. in the sum.mer of 2015.4 the neural style transfer algorithm has undergone many refinements and spawned many variations since its original introduction and it has made its way into many smartphone photo apps. for simplicity this section focuses on the formula.tion described in the original paper. neural style transfer consists of applying the style of a reference image to a target image while conserving the content of the target image. figure 12.9 shows an example. content target style reference combination image in this context style essentially means textures colors and visual patterns in the image at various spatial scales and the content is the higher-level macrostructure of the image. for instance blue-and-yellow circular brushstrokes are considered to be the style in figure 12.9 (using starry night by vincent van gogh) and the buildings in the t bingen photograph are considered to be the content. the idea of style transfer which is tightly related to that of texture generation has had a long history in the image-processing community prior to the development of neural style transfer in 2015. but as it turns out the deep-learning-based implementa.tions of style transfer offer results unparalleled by what had been previously achieved with classical computer vision techniques and they triggered an amazing renaissance in creative applications of computer vision. 4 leon a. gatys alexander s. ecker and matthias bethge a neural algorithm of artistic style arxiv (2015) https//arxiv.org/abs/1508.06576. the key notion behind implementing style transfer is the same idea that s central to all deep learning algorithms you define a loss function to specify what you want to achieve and you minimize this loss. we know what we want to achieve conserving the content of the original image while adopting the style of the reference image. if we were able to mathematically define content and style then an appropriate loss function to minimize would be the following loss = (distance(style(reference_image) - style(combination_image)) + distance(content(original_image) - content(combination_image))) here distance is a norm function such as the l2 norm content is a function that takes an image and computes a representation of its content and style is a function that takes an image and computes a representation of its style. minimizing this loss causes style(combination_image) to be close to style(reference_image) and content(combination_image) is close to content(original_image) thus achieving style transfer as we defined it. a fundamental observation made by gatys et al. was that deep convolutional neu. ral networks offer a way to mathematically define the style and content functions. let s see how. 12.3.1 the content loss as you already know activations from earlier layers in a network contain local informa.tion about the image whereas activations from higher layers contain increasingly global abstract information. formulated in a different way the activations of the dif.ferent layers of a convnet provide a decomposition of the contents of an image over different spatial scales. therefore you d expect the content of an image which is more global and abstract to be captured by the representations of the upper layers in a convnet. a good candidate for content loss is thus the l2 norm between the activations of an upper layer in a pretrained convnet computed over the target image and the acti.vations of the same layer computed over the generated image. this guarantees that as seen from the upper layer the generated image will look similar to the original target image. assuming that what the upper layers of a convnet see is really the content of their input images this works as a way to preserve image content. 12.3.2 the style loss the content loss only uses a single upper layer but the style loss as defined by gatys et al. uses multiple layers of a convnet you try to capture the appearance of the style-reference image at all spatial scales extracted by the convnet not just a single scale. for the style loss gatys et al. use the gram matrix of a layer s activations the inner product of the feature maps of a given layer. this inner product can be understood as representing a map of the correlations between the layer s features. these feature correlations capture the statistics of the patterns of a particular spatial scale which empirically correspond to the appearance of the textures found at this scale. hence the style loss aims to preserve similar internal correlations within the activa.tions of different layers across the style-reference image and the generated image. in turn this guarantees that the textures found at different spatial scales look similar across the style-reference image and the generated image. in short you can use a pretrained convnet to define a loss that will do the following preserve content by maintaining similar high-level layer activations between the original image and the generated image. the convnet should see both the original image and the generated image as containing the same things. preserve style by maintaining similar correlations within activations for both low-level layers and high-level layers. feature correlations capture textures the gen.erated image and the style-reference image should share the same textures at different spatial scales. now let s look at a keras implementation of the original 2015 neural style transfer algorithm. as you ll see it shares many similarities with the deepdream implementa.tion we developed in the previous section. 12.3.3 neural style transfer in keras neural style transfer can be implemented using any pretrained convnet. here we ll use the vgg19 network used by gatys et al. vgg19 is a simple variant of the vgg16 network introduced in chapter 5 with three more convolutional layers. here s the general process set up a network that computes vgg19 layer activations for the style-reference image the base image and the generated image at the same time. use the layer activations computed over these three images to define the loss function described earlier which we ll minimize in order to achieve style transfer. set up a gradient-descent process to minimize this loss function. let s start by defining the paths to the style-reference image and the base image. to make sure that the processed images are a similar size (widely different sizes make style transfer more difficult) we ll later resize them all to a shared height of 400 px. from tensorflow import keras path to the image we want to transform base_image_path = keras.utils.get_file( "sf.jpg" origin="https//img-datasets.s3.amazonaws.com/sf.jpg") style_reference_image_path = keras.utils.get_file( path to the "starry_night.jpg" style image origin="https//img-datasets.s3.amazonaws.com/starry_night.jpg") original_width original_height = keras.utils.load_img(base_image_path).size img_height = 400 dimensions of img_width = round(original_width * img_height / original_height) the generated picture our content image is shown in figure 12.10 and figure 12.11 shows our style image. we also need some auxiliary functions for loading preprocessing and postprocessing the images that go in and out of the vgg19 convnet. util function to open resize import numpy as np and format pictures into appropriate arrays def preprocess_image(image_path) img = keras.utils.load_img( image_path target_size=(img_height img_width)) img = keras.utils.img_to_array(img) img = np.expand_dims(img axis=0) img = keras.applications.vgg19.preprocess_input(img) return img util function to convert a numpy array into a valid image def deprocess_image(img) img = img.reshape((img_height img_width 3)) img[ 0] += 103.939 zero-centering by removing the mean pixel value img[ 1] += 116.779 from imagenet. this reverses a transformation done by vgg19.preprocess_input. img[ 2] += 123.68 img = img[ -1] converts images from 'bgr' to 'rgb'. img = np.clip(img 0 255).astype("uint8") this is also part of the reversal of return img vgg19.preprocess_input. let s set up the vgg19 network. like in the deepdream example we ll use the pre.trained convnet to create a feature exactor model that returns the activations of inter.mediate layers all layers in the model this time. build a vgg19 model loaded with pretrained imagenet weights. model = keras.applications.vgg19.vgg19(weights="imagenet" include_top=false) outputs_dict = dict([(layer.name layer.output) for layer in model.layers]) feature_extractor = keras.model(inputs=model.inputs outputs=outputs_dict) model that returns the activation values for every target layer (as a dict) let s define the content loss which will make sure the top layer of the vgg19 convnet has a similar view of the style image and the combination image. def content_loss(base_img combination_img) return tf.reduce_sum(tf.square(combination_img - base_img)) next is the style loss. it uses an auxiliary function to compute the gram matrix of an input matrix a map of the correlations found in the original feature matrix. def gram_matrix(x) x = tf.transpose(x (2 0 1)) features = tf.reshape(x (tf.shape(x) -1)) gram = tf.matmul(features tf.transpose(features)) return gram def style_loss(style_img combination_img) s = gram_matrix(style_img) c = gram_matrix(combination_img) channels = 3 size = img_height * img_width return tf.reduce_sum(tf.square(s - c)) / (4.0 * (channels ** 2) * (size ** 2)) to these two loss components you add a third the total variation loss which operates on the pixels of the generated combination image. it encourages spatial continuity in the generated image thus avoiding overly pixelated results. you can interpret it as a regularization loss. def total_variation_loss(x) a = tf.square( x[ img_height -1 img_width -1 ] -x[ 1 img_width -1 ] ) b = tf.square( x[ img_height -1 img_width -1 ] -x[ img_height -1 1 ] ) return tf.reduce_sum(tf.pow(a + b 1.25)) the loss that you minimize is a weighted average of these three losses. to compute the content loss you use only one upper layer the block5_conv2 layer whereas for the style loss you use a list of layers that spans both low-level and high-level layers. you add the total variation loss at the end. depending on the style-reference image and content image you re using you ll likely want to tune the content_weight coefficient (the contribution of the content loss to the total loss). a higher content_weight means the target content will be more recognizable in the generated image. style_layer_names = [ list of layers to use "block1_conv1" for the style loss "block2_conv1" "block3_conv1" "block4_conv1" the layer to use for "block5_conv1" the content loss ] contribution weight of the total content_layer_name = "block5_conv2" variation loss total_variation_weight = 1e-6 contribution weight of the content loss initialize the loss to 0. add the style loss. style_weight = 1e-6 contribution weight content_weight = 2.5e-8 of the style loss def compute_loss(combination_image base_image style_reference_image) input_tensor = tf.concat( [base_image style_reference_image combination_image] axis=0) features = feature_extractor(input_tensor) loss = tf.zeros(shape=()) layer_features = features[content_layer_name] base_image_features = layer_features[0 ] combination_features = layer_features[2 ] loss = loss + content_weight * content_loss( base_image_features combination_features ) for layer_name in style_layer_names layer_features = features[layer_name] add the content loss. style_reference_features = layer_features[1 ] combination_features = layer_features[2 ] style_loss_value = style_loss( style_reference_features combination_features) loss += (style_weight / len(style_layer_names)) * style_loss_value loss += total_variation_weight * total_variation_loss(combination_image) return loss add the total variation loss. finally let s set up the gradient-descent process. in the original gatys et al. paper opti.mization is performed using the l-bfgs algorithm but that s not available in tensor-flow so we ll just do mini-batch gradient descent with the sgd optimizer instead. we ll leverage an optimizer feature you haven t seen before a learning-rate schedule. we ll use it to gradually decrease the learning rate from a very high value (100) to a much smaller final value (about 20). that way we ll make fast progress in the early stages of training and then proceed more cautiously as we get closer to the loss minimum. we make the training step fast by compiling it as a tf.function. import tensorflow as tf @tf.function def compute_loss_and_grads( combination_image base_image style_reference_image) with tf.gradienttape() as tape loss = compute_loss( combination_image base_image style_reference_image) grads = tape.gradient(loss combination_image) we ll start with a return loss grads learning rate of 100 and decrease it by 4% optimizer = keras.optimizers.sgd( every 100 steps. keras.optimizers.schedules.exponentialdecay( initial_learning_rate=100.0 decay_steps=100 decay_rate=0.96 ) ) base_image = preprocess_image(base_image_path) style_reference_image = preprocess_image(style_reference_image_path) combination_image = tf.variable(preprocess_image(base_image_path)) use a variable to store the iterations = 4000 combination image since we ll be for i in range(1 iterations + 1) updating it during training. loss grads = compute_loss_and_grads( combination_image base_image style_reference_image ) optimizer.apply_gradients([(grads combination_image)]) if i % 100 == 0 print(f"iteration {i} loss={loss.2f}") img = deprocess_image(combination_image.numpy()) fname = f"combination_image_at_iteration_{i}.png" keras.utils.save_img(fname img) save the combination image at regular intervals. update the combination image in a direction that reduces the style transfer loss. figure 12.12 shows what you get. keep in mind that what this technique achieves is merely a form of image retexturing or texture transfer. it works best with style-reference images that are strongly textured and highly self-similar and with content targets that don t require high levels of detail in order to be recognizable. it typically can t achieve fairly abstract feats such as transferring the style of one portrait to another. the algo.rithm is closer to classical signal processing than to ai so don t expect it to work like magic! additionally note that this style-transfer algorithm is slow to run. but the transforma.tion operated by the setup is simple enough that it can be learned by a small fast generating images with variational autoencoders feedforward convnet as well as long as you have appropriate training data available. fast style transfer can thus be achieved by first spending a lot of compute cycles to generate input-output training examples for a fixed style-reference image using the method outlined here and then training a simple convnet to learn this style-specific transformation. once that s done stylizing a given image is instantaneous it s just a forward pass of this small convnet. 12.3.4 wrapping up style transfer consists of creating a new image that preserves the contents of a target image while also capturing the style of a reference image. content can be captured by the high-level activations of a convnet. style can be captured by the internal correlations of the activations of different layers of a convnet. hence deep learning allows style transfer to be formulated as an optimization process using a loss defined with a pretrained convnet. starting from this basic idea many variants and refinements are possible. 12.4 generating images with variational autoencoders the most popular and successful application of creative ai today is image generation learning latent visual spaces and sampling from them to create entirely new pictures interpolated from real ones pictures of imaginary people imaginary places imagi.nary cats and dogs and so on. in this section and the next we ll review some high-level concepts pertaining to image generation alongside implementation details relative to the two main tech.niques in this domain variational autoencoders (vaes) and generative adversarial networks (gans). note that the techniques i ll present here aren t specific to images you could develop latent spaces of sound music or even text using gans and vaes but in practice the most interesting results have been obtained with pictures and that s what we ll focus on here. 12.4.1 sampling from latent spaces of images the key idea of image generation is to develop a low-dimensional latent space of repre.sentations (which like everything else in deep learning is a vector space) where any point can be mapped to a valid image an image that looks like the real thing. the module capable of realizing this mapping taking as input a latent point and output.ting an image (a grid of pixels) is called a generator (in the case of gans) or a decoder (in the case of vaes). once such a latent space has been learned you can sample points from it and by mapping them back to image space generate images that have never been seen before (see figure 12.13). these new images are the in-betweens of the training images. gans and vaes are two different strategies for learning such latent spaces of image representations each with its own characteristics. vaes are great for learning vector image latent space of images (a vector space) figure 12.13 learning a latent vector space of images and using it to sample new images latent spaces that are well structured where specific directions encode a meaningful axis of variation in the data (see figure 12.14). gans generate images that can poten.tially be highly realistic but the latent space they come from may not have as much structure and continuity. generating images with variational autoencoders 12.4.2 concept vectors for image editing we already hinted at the idea of a concept vector when we covered word embeddings in chapter 11. the idea is still the same given a latent space of representations or an embedding space certain directions in the space may encode interesting axes of vari.ation in the original data. in a latent space of images of faces for instance there may be a smile vector such that if latent point z is the embedded representation of a certain face then latent point z+s is the embedded representation of the same face smiling. once you ve identified such a vector it then becomes possible to edit images by pro.jecting them into the latent space moving their representation in a meaningful way and then decoding them back to image space. there are concept vectors for essen.tially any independent dimension of variation in image space in the case of faces you may discover vectors for adding sunglasses to a face removing glasses turning a male face into a female face and so on. figure 12.15 is an example of a smile vector a concept vector discovered by tom white from the victoria university school of design in new zealand using vaes trained on a dataset of faces of celebrities (the celeba dataset). 12.4.3 variational autoencoders variational autoencoders simultaneously discovered by kingma and welling in december 20135 and rezende mohamed and wierstra in january 20146 are a kind of generative model that s especially appropriate for the task of image editing via con.cept vectors. they re a modern take on autoencoders (a type of network that aims to encode an input to a low-dimensional latent space and then decode it back) that mixes ideas from deep learning with bayesian inference. 5 diederik p. kingma and max welling auto-encoding variational bayes arxiv (2013) https//arxiv.org/ abs/1312.6114. 6 danilo jimenez rezende shakir mohamed and daan wierstra stochastic backpropagation and approxi.mate inference in deep generative models arxiv (2014) https//arxiv.org/abs/1401.4082. a classical image autoencoder takes an image maps it to a latent vector space via an encoder module and then decodes it back to an output with the same dimensions as the original image via a decoder module (see figure 12.16). it s then trained by using as target data the same images as the input images meaning the autoencoder learns to reconstruct the original inputs. by imposing various constraints on the code (the output of the encoder) you can get the autoencoder to learn more-or less-interesting latent representations of the data. most commonly you ll constrain the code to be low-dimensional and sparse (mostly zeros) in which case the encoder acts as a way to compress the input data into fewer bits of information. input representation input figure 12.16 an autoencoder mapping an input x to a compressed representation and then decoding it back as x' in practice such classical autoencoders don t lead to particularly useful or nicely structured latent spaces. they re not much good at compression either. for these rea.sons they have largely fallen out of fashion. vaes however augment autoencoders with a little bit of statistical magic that forces them to learn continuous highly struc.tured latent spaces. they have turned out to be a powerful tool for image generation. a vae instead of compressing its input image into a fixed code in the latent space turns the image into the parameters of a statistical distribution a mean and a variance. essentially this means we re assuming the input image has been generated by a statistical process and that the randomness of this process should be taken into account during encoding and decoding. the vae then uses the mean and variance parameters to randomly sample one element of the distribution and decodes that element back to the original input (see figure 12.17). the stochasticity of this pro.cess improves robustness and forces the latent space to encode meaningful repre.sentations everywhere every point sampled in the latent space is decoded to a valid output. in technical terms here s how a vae works 1 an encoder module turns the input sample input_img into two parameters in a latent space of representations z_mean and z_log_variance. 2 you randomly sample a point z from the latent normal distribution that s assumed to generate the input image via z = z_mean + exp(z_log_variance) * epsilon where epsilon is a random tensor of small values. 3 a decoder module maps this point in the latent space back to the original input image. generating images with variational autoencoders distribution over latent sampled from the distribution figure 12.17 a vae maps an image to two vectors z_mean and z_log_sigma which define a probability distribution over the latent space used to sample a latent point to decode. because epsilon is random the process ensures that every point that s close to the latent location where you encoded input_img (z-mean) can be decoded to something similar to input_img thus forcing the latent space to be continuously meaningful. any two close points in the latent space will decode to highly similar images. continuity combined with the low dimensionality of the latent space forces every direction in the latent space to encode a meaningful axis of variation of the data making the latent space very structured and thus highly suitable to manipulation via concept vectors. the parameters of a vae are trained via two loss functions a reconstruction loss that forces the decoded samples to match the initial inputs and a regularization loss that helps learn well-rounded latent distributions and reduces overfitting to the training data. schematically the process looks like this draws a latent and variance parameters encodes the input into mean point using a small random epsilon z_mean z_log_variance = encoder(input_img) z = z_mean + exp(z_log_variance) * epsilon reconstructed_img = decoder(z) decodes z model = model(input_img reconstructed_img) back to an image instantiates the autoencoder model which maps an input image to its reconstruction you can then train the model using the reconstruction loss and the regularization loss. for the regularization loss we typically use an expression (the kullback leibler diver.gence) meant to nudge the distribution of the encoder output toward a well-rounded normal distribution centered around 0. this provides the encoder with a sensible assumption about the structure of the latent space it s modeling. now let s see what implementing a vae looks like in practice! 12.4.4 implementing a vae with keras we re going to be implementing a vae that can generate mnist digits. it s going to have three parts an encoder network that turns a real image into a mean and a variance in the latent space a sampling layer that takes such a mean and variance and uses them to sample a random point from the latent space a decoder network that turns points from the latent space back into images the following listing shows the encoder network we ll use mapping images to the parameters of a probability distribution over the latent space. it s a simple convnet that maps the input image x to two vectors z_mean and z_log_var. one important detail is that we use strides for downsampling feature maps instead of max pooling. the last time we did this was in the image segmentation example in chapter 9. recall that in general strides are preferable to max pooling for any model that cares about information location that is to say where stuff is in the image and this one does since it will have to produce an image encoding that can be used to reconstruct a valid image. from tensorflow import keras dimensionality of from tensorflow.keras import layers the latent space a 2d plane latent_dim = 2 encoder_inputs = keras.input(shape=(28 28 1)) x = layers.conv2d( 32 3 activation="relu" strides=2 padding="same")(encoder_inputs) x = layers.conv2d(64 3 activation="relu" strides=2 padding="same")(x) x = layers.flatten()(x) the input image ends up x = layers.dense(16 activation="relu")(x) being encoded into these z_mean = layers.dense(latent_dim name="z_mean")(x) two parameters. z_log_var = layers.dense(latent_dim name="z_log_var")(x) encoder = keras.model(encoder_inputs [z_mean z_log_var] name="encoder") its summary looks like this >>> encoder.summary() model "encoder" layer (type) output shape param # connected to ================================================================================================== input_1 (inputlayer) [(none 28 28 1)] 0 conv2d (conv2d) (none 14 14 32) 320 input_1 conv2d_1 (conv2d) (none 7 7 64) 18496 conv2d generating images with variational autoencoders flatten (flatten) (none 3136) 0 conv2d_1 dense (dense) (none 16) 50192 flatten z_mean (dense) (none 2) 34 dense z_log_var (dense) (none 2) 34 dense ================================================================================================== total params 69076 trainable params 69076 non-trainable params 0 next is the code for using z_mean and z_log_var the parameters of the statistical dis.tribution assumed to have produced input_img to generate a latent space point z. import tensorflow as tf class sampler(layers.layer) draw a batch of def call(self z_mean z_log_var) random normal batch_size = tf.shape(z_mean) vectors. apply the vae z_size = tf.shape(z_mean) sampling epsilon = tf.random.normal(shape=(batch_size z_size)) formula. return z_mean + tf.exp(0.5 * z_log_var) * epsilon the following listing shows the decoder implementation. we reshape the vector z to the dimensions of an image and then use a few convolution layers to obtain a final image output that has the same dimensions as the original input_img. input where produce the same number of coefficients that we we ll feed z had at the level of the flatten layer in the encoder. revert the conv2d layers of the encoder. latent_inputs = keras.input(shape=(latent_dim)) x = layers.dense(7 * 7 * 64 activation="relu")(latent_inputs) x = layers.reshape((7 7 64))(x) revert the flatten layer of the encoder. x = layers.conv2dtranspose(64 3 activation="relu" strides=2 padding="same")(x) x = layers.conv2dtranspose(32 3 activation="relu" strides=2 padding="same")(x) decoder_outputs = layers.conv2d(1 3 activation="sigmoid" padding="same")(x) decoder = keras.model(latent_inputs decoder_outputs name="decoder") the output ends up with shape (28 28 1). its summary looks like this >>> decoder.summary() model "decoder" layer (type) output shape param # ================================================================= input_2 (inputlayer) [(none 2)] 0 dense_1 (dense) (none 3136) 9408 reshape (reshape) (none 7 7 64) 0 conv2d_transpose (conv2dtran (none 14 14 64) 36928 conv2d_transpose_1 (conv2dtr (none 28 28 32) 18464 conv2d_2 (conv2d) (none 28 28 1) 289 ================================================================= total params 65089 trainable params 65089 non-trainable params 0 now let s create the vae model itself. this is your first example of a model that isn t doing supervised learning (an autoencoder is an example of self-supervised learning because it uses its inputs as targets). whenever you depart from classic supervised learning it s common to subclass the model class and implement a custom train_ step() to specify the new training logic a workflow you learned about in chapter 7. that s what we ll do here. class vae(keras.model) def __init__(self encoder decoder **kwargs) super().__init__(**kwargs) self.encoder = encoder self.decoder = decoder self.sampler = sampler() we use these metrics to keep track of the loss averages over each epoch. self.total_loss_tracker = keras.metrics.mean(name="total_loss") self.reconstruction_loss_tracker = keras.metrics.mean( name="reconstruction_loss") self.kl_loss_tracker = keras.metrics.mean(name="kl_loss") @property def metrics(self) return [self.total_loss_tracker self.reconstruction_loss_tracker self.kl_loss_tracker] def train_step(self data) with tf.gradienttape() as tape z_mean z_log_var = self.encoder(data) z = self.sampler(z_mean z_log_var) reconstruction = decoder(z) reconstruction_loss = tf.reduce_mean( tf.reduce_sum( we list the metrics in the metrics property to enable the model to reset them after each epoch (or between multiple calls to fit()/evaluate()). we sum the reconstruction loss over the spatial dimensions (axes 1 and 2) and take its mean over the batch dimension. add the regularization term (kullback leibler divergence). keras.losses.binary_crossentropy(data reconstruction) axis=(1 2) ) ) kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)) total_loss = reconstruction_loss + tf.reduce_mean(kl_loss) generating images with variational autoencoders grads = tape.gradient(total_loss self.trainable_weights) self.optimizer.apply_gradients(zip(grads self.trainable_weights)) self.total_loss_tracker.update_state(total_loss) self.reconstruction_loss_tracker.update_state(reconstruction_loss) self.kl_loss_tracker.update_state(kl_loss) return { "total_loss" self.total_loss_tracker.result() "reconstruction_loss" self.reconstruction_loss_tracker.result() "kl_loss" self.kl_loss_tracker.result() } finally we re ready to instantiate and train the model on mnist digits. because the loss is taken care of in the custom layer we don t specify an external loss at compile time (loss=none) which in turn means we won t pass target data during training (as you can see we only pass x_train to the model in fit()). we train on all mnist import numpy as np digits so we concatenate the training and test (x_train _) (x_test _) = keras.datasets.mnist.load_data() samples. mnist_digits = np.concatenate([x_train x_test] axis=0) mnist_digits = np.expand_dims(mnist_digits -1).astype("float32") / 255 vae = vae(encoder decoder) vae.compile(optimizer=keras.optimizers.adam() run_eagerly=true) vae.fit(mnist_digits epochs=30 batch_size=128) note that we don t pass targets note that we don t pass a loss in fit() since train_step() argument in compile() since the loss doesn t expect any. is already part of the train_step(). once the model is trained we can use the decoder network to turn arbitrary latent space vectors into images. import matplotlib.pyplot as plt we ll display a grid of 30 ! 30 n = 30 digits (900 digits total). digit_size = 28 figure = np.zeros((digit_size * n digit_size * n)) grid_x = np.linspace(-1 1 n) sample points grid_y = np.linspace(-1 1 n)[-1] linearly on a 2d grid. for i yi in enumerate(grid_y) iterate over for j xi in enumerate(grid_x) grid locations. z_sample = np.array([[xi yi]]) for each location x_decoded = vae.decoder.predict(z_sample) sample a digit and add it to our figure. digit = x_decoded.reshape(digit_size digit_size) figure[ i * digit_size (i + 1) * digit_size j * digit_size (j + 1) * digit_size ] = digit plt.figure(figsize=(15 15)) start_range = digit_size // 2 end_range = n * digit_size + start_range pixel_range = np.arange(start_range end_range digit_size) sample_range_x = np.round(grid_x 1) sample_range_y = np.round(grid_y 1) plt.xticks(pixel_range sample_range_x) plt.yticks(pixel_range sample_range_y) plt.xlabel("z") plt.ylabel("z") plt.axis("off") plt.imshow(figure cmap="greys_r") the grid of sampled digits (see figure 12.18) shows a completely continuous distribu.tion of the different digit classes with one digit morphing into another as you follow a path through latent space. specific directions in this space have a meaning for exam.ple there are directions for five-ness one-ness and so on. introduction to generative adversarial networks in the next section we ll cover in detail the other major tool for generating artificial images generative adversarial networks (gans). 12.4.5 wrapping up image generation with deep learning is done by learning latent spaces that cap.ture statistical information about a dataset of images. by sampling and decod.ing points from the latent space you can generate never-before-seen images. there are two major tools to do this vaes and gans. vaes result in highly structured continuous latent representations. for this rea.son they work well for doing all sorts of image editing in latent space face swapping turning a frowning face into a smiling face and so on. they also work nicely for doing latent-space-based animations such as animating a walk along a cross section of the latent space or showing a starting image slowly morphing into different images in a continuous way. gans enable the generation of realistic single-frame images but may not induce latent spaces with solid structure and high continuity. most successful practical applications i have seen with images rely on vaes but gans have enjoyed enduring popularity in the world of academic research. you ll find out how they work and how to implement one in the next section. 12.5 introduction to generative adversarial networks generative adversarial networks (gans) introduced in 2014 by goodfellow et al.7 are an alternative to vaes for learning latent spaces of images. they enable the genera.tion of fairly realistic synthetic images by forcing the generated images to be statisti.cally almost indistinguishable from real ones. an intuitive way to understand gans is to imagine a forger trying to create a fake picasso painting. at first the forger is pretty bad at the task. he mixes some of his fakes with authentic picassos and shows them all to an art dealer. the art dealer makes an authenticity assessment for each painting and gives the forger feedback about what makes a picasso look like a picasso. the forger goes back to his studio to prepare some new fakes. as time goes on the forger becomes increasingly competent at imitating the style of picasso and the art dealer becomes increasingly expert at spotting fakes. in the end they have on their hands some excellent fake picassos. that s what a gan is a forger network and an expert network each being trained to best the other. as such a gan is made of two parts generator network takes as input a random vector (a random point in the latent space) and decodes it into a synthetic image discriminator network (or adversary) takes as input an image (real or synthetic) and predicts whether the image came from the training set or was created by the generator network 7 ian goodfellow et al. generative adversarial networks arxiv (2014) https//arxiv.org/abs/1406.2661. the generator network is trained to be able to fool the discriminator network and thus it evolves toward generating increasingly realistic images as training goes on arti.ficial images that look indistinguishable from real ones to the extent that it s impossi.ble for the discriminator network to tell the two apart (see figure 12.19). meanwhile the discriminator is constantly adapting to the gradually improving capabilities of the generator setting a high bar of realism for the generated images. once training is over the generator is capable of turning any point in its input space into a believable image. unlike vaes this latent space has fewer explicit guarantees of meaningful structure in particular it isn t continuous. mix of real and fake images remarkably a gan is a system where the optimization minimum isn t fixed unlike in any other training setup you ve encountered in this book. normally gradient descent consists of rolling down hills in a static loss landscape. but with a gan every step taken down the hill changes the entire landscape a little. it s a dynamic system where the optimization process is seeking not a minimum but an equilibrium between two forces. for this reason gans are notoriously difficult to train getting a gan to work requires lots of careful tuning of the model architecture and training parameters. 12.5.1 a schematic gan implementation in this section we ll explain how to implement a gan in keras in its barest form. gans are advanced so diving deeply into the technical details of architectures like that of the stylegan2 that generated the images in figure 12.20 would be out of scope for this book. the specific implementation we ll use in this demonstration is a deep convolutional gan (dcgan) a very basic gan where the generator and discriminator are deep convnets. introduction to generative adversarial networks we ll train our gan on images from the large-scale celebfaces attributes dataset (known as celeba) a dataset of 200000 faces of celebrities (http//mmlab.ie.cuhk .edu.hk/projects/celeba.html) to speed up training we ll resize the images to 64 ! 64 so we ll be learning to generate 64 ! 64 images of human faces. schematically the gan looks like this a generator network maps vectors of shape (latent_dim) to images of shape (6464 3). a discriminator network maps images of shape (64 64 3) to a binary score estimating the probability that the image is real. a gan network chains the generator and the discriminator together gan(x) = discriminator(generator(x)). thus this gan network maps latent space vec.tors to the discriminator s assessment of the realism of these latent vectors as decoded by the generator. we train the discriminator using examples of real and fake images along with real / fake labels just as we train any regular image-classification model. to train the generator we use the gradients of the generator s weights with regard to the loss of the gan model. this means that at every step we move the weights of the generator in a direction that makes the discriminator more likely to classify as real the images decoded by the generator. in other words we train the generator to fool the discriminator. 12.5.2 a bag of tricks the process of training gans and tuning gan implementations is notoriously diffi.cult. there are a number of known tricks you should keep in mind. like most things in deep learning it s more alchemy than science these tricks are heuristics not theory-backed guidelines. they re supported by a level of intuitive understanding of the phe.nomenon at hand and they re known to work well empirically although not necessar.ily in every context. here are a few of the tricks used in the implementation of the gan generator and discriminator in this section. it isn t an exhaustive list of gan-related tips you ll find many more across the gan literature we use strides instead of pooling for downsampling feature maps in the dis.criminator just like we did in our vae encoder. we sample points from the latent space using a normal distribution (gaussian dis.tribution) not a uniform distribution. stochasticity is good for inducing robustness. because gan training results in a dynamic equilibrium gans are likely to get stuck in all sorts of ways. introduc.ing randomness during training helps prevent this. we introduce randomness by adding random noise to the labels for the discriminator. sparse gradients can hinder gan training. in deep learning sparsity is often a desirable property but not in gans. two things can induce gradient spar.sity max pooling operations and relu activations. instead of max pooling we recommend using strided convolutions for downsampling and we recom.mend using a leakyrelu layer instead of a relu activation. it s similar to relu but it relaxes sparsity constraints by allowing small negative activation values. in generated images it s common to see checkerboard artifacts caused by unequal coverage of the pixel space in the generator (see figure 12.21). to fix this we use a kernel size that s divisible by the stride size whenever we use a strided conv2dtranspose or conv2d in both the generator and the discriminator. 12.5.3 getting our hands on the celeba dataset you can download the dataset manually from the website http//mmlab.ie.cuhk.edu .hk/projects/celeba.html. if you re using colab you can run the following to down.load the data from google drive and uncompress it. introduction to generative adversarial networks create a working directory. !mkdir celeba_gan !gdown --id 1o7m1010ejjle5qxlzim9fpjs7oj6e684 -o celeba_gan/data.zip !unzip -qq celeba_gan/data.zip -d celeba_gan download the compressed data uncompress using gdown (available by default the data. in colab install it otherwise). once you ve got the uncompressed images in a directory you can use image_data.set_from_directory to turn it into a dataset. since we just need the images there are no labels we ll specify label_mode=none. from tensorflow import keras dataset = keras.utils_dataset_from_directory( "celeba_gan" only the images will be label_mode=none returned no labels. image_size=(64 64) batch_size=32 we will resize the images to 64 ! 64 by using a smart smart_resize=true) combination of cropping and resizing to preserve aspect ratio. we don t want face proportions to get distorted! finally let s rescale the images to the [0-1] range. dataset = dataset.map(lambda x x / 255.) you can use the following code to display a sample image. import matplotlib.pyplot as plt for x in dataset plt.axis("off") plt.imshow((x.numpy() * 255).astype("int32")) break 12.5.4 the discriminator first we ll develop a discriminator model that takes as input a candidate image (real or synthetic) and classifies it into one of two classes generated image or real image that comes from the training set. one of the many issues that commonly arise with gans is that the generator gets stuck with generated images that look like noise. a possible solution is to use dropout in the discriminator so that s what we will do here. from tensorflow.keras import layers discriminator = keras.sequential( [ keras.input(shape=(64 64 3)) layers.conv2d(64 kernel_size=4 strides=2 padding="same") layers.leakyrelu(alpha=0.2) layers.conv2d(128 kernel_size=4 strides=2 padding="same") layers.leakyrelu(alpha=0.2) layers.conv2d(128 kernel_size=4 strides=2 padding="same") layers.leakyrelu(alpha=0.2) one dropout layer layers.flatten() an important trick! layers.dropout(0.2) layers.dense(1 activation="sigmoid") ] name="discriminator" ) here s the discriminator model summary >>> discriminator.summary() model "discriminator" layer (type) output shape param # ================================================================= conv2d (conv2d) (none 32 32 64) 3136 leaky_re_lu (leakyrelu) (none 32 32 64) 0 conv2d_1 (conv2d) (none 16 16 128) 131200 leaky_re_lu_1 (leakyrelu) (none 16 16 128) 0 conv2d_2 (conv2d) (none 8 8 128) 262272 leaky_re_lu_2 (leakyrelu) (none 8 8 128) 0 flatten (flatten) (none 8192) 0 dropout (dropout) (none 8192) 0 dense (dense) (none 1) 8193 ================================================================= total params 404801 trainable params 404801 non-trainable params 0 introduction to generative adversarial networks 12.5.5 the generator next let s develop a generator model that turns a vector (from the latent space during training it will be sampled at random) into a candidate image. latent_dim = 128 the latent space will be made of 128. generator = keras.sequential( dimensional vectors. revert the flatten layer of the encoder. revert the conv2d layers of the encoder. the output ends up with shape (28 28 1). ) produce the same number of coefficients we had at the level of the flatten layer in the encoder. [ keras.input(shape=(latent_dim)) layers.dense(8 * 8 * 128) layers.reshape((8 8 128)) layers.conv2dtranspose(128 kernel_size=4 strides=2 padding="same") layers.leakyrelu(alpha=0.2) layers.conv2dtranspose(256 kernel_size=4 strides=2 padding="same") layers.leakyrelu(alpha=0.2) layers.conv2dtranspose(512 kernel_size=4 strides=2 padding="same") layers.leakyrelu(alpha=0.2) layers.conv2d(3 kernel_size=5 padding="same" activation="sigmoid") ] we use leakyrelu name="generator" as our activation. this is the generator model summary >>> generator.summary() model "generator" layer (type) output shape param # ================================================================= dense_1 (dense) (none 8192) 1056768 reshape (reshape) (none 8 8 128) 0 conv2d_transpose (conv2dtran (none 16 16 128) 262272 leaky_re_lu_3 (leakyrelu) (none 16 16 128) 0 conv2d_transpose_1 (conv2dtr (none 32 32 256) 524544 leaky_re_lu_4 (leakyrelu) (none 32 32 256) 0 conv2d_transpose_2 (conv2dtr (none 64 64 512) 2097664 leaky_re_lu_5 (leakyrelu) (none 64 64 512) 0 conv2d_3 (conv2d) (none 64 64 3) 38403 ================================================================= total params 3979651 trainable params 3979651 non-trainable params 0 12.5.6 the adversarial network finally we ll set up the gan which chains the generator and the discriminator. when trained this model will move the generator in a direction that improves its ability to fool the discriminator. this model turns latent-space points into a classifi.cation decision fake or real and it s meant to be trained with labels that are always these are real images. so training gan will update the weights of generator in a way that makes discriminator more likely to predict real when looking at fake images. to recapitulate this is what the training loop looks like schematically. for each epoch you do the following 1 draw random points in the latent space (random noise). 2 generate images with generator using this random noise. 3 mix the generated images with real ones. 4 train discriminator using these mixed images with corresponding targets either real (for the real images) or fake (for the generated images). 5 draw new random points in the latent space. 6 train generator using these random vectors with targets that all say these are real images. this updates the weights of the generator to move them toward getting the discriminator to predict these are real images for generated images this trains the generator to fool the discriminator. let s implement it. like in our vae example we ll use a model subclass with a cus.tom train_step(). note that we ll use two optimizers (one for the generator and one for the discriminator) so we will also override compile() to allow for passing two optimizers. import tensorflow as tf class gan(keras.model) def __init__(self discriminator generator latent_dim) super().__init__() self.discriminator = discriminator self.generator = generator self.latent_dim = latent_dim self.d_loss_metric = keras.metrics.mean(name="d_loss") self.g_loss_metric = keras.metrics.mean(name="g_loss") def compile(self d_optimizer g_optimizer loss_fn) super(gan self).compile() self.d_optimizer = d_optimizer self.g_optimizer = g_optimizer self.loss_fn = loss_fn @property def metrics(self) sets up metrics to track the two losses over each training epoch return [self.d_loss_metric self.g_loss_metric] introduction to generative adversarial networks 409 def train_step(self real_images) decodes them to fake images combines them with real images trains the discriminator samples random points in the latent space trains the generator batch_size = tf.shape(real_images) samples random points random_latent_vectors = tf.random.normal( in the latent space shape=(batch_size self.latent_dim)) generated_images = self.generator(random_latent_vectors) combined_images = tf.concat([generated_images real_images] axis=0) labels = tf.concat( assembles labels [tf.ones((batch_size 1)) tf.zeros((batch_size 1))] discriminating real from fake images axis=0 ) labels += 0.05 * tf.random.uniform(tf.shape(labels)) adds random noise to the with tf.gradienttape() as tape labels an predictions = self.discriminator(combined_images) important trick! d_loss = self.loss_fn(labels predictions) grads = tape.gradient(d_loss self.discriminator.trainable_weights) self.d_optimizer.apply_gradients( zip(grads self.discriminator.trainable_weights) ) random_latent_vectors = tf.random.normal( assembles labels that shape=(batch_size self.latent_dim)) say these are all real images (it s a lie!) misleading_labels = tf.zeros((batch_size 1)) with tf.gradienttape() as tape predictions = self.discriminator( self.generator(random_latent_vectors)) g_loss = self.loss_fn(misleading_labels predictions) grads = tape.gradient(g_loss self.generator.trainable_weights) self.g_optimizer.apply_gradients( zip(grads self.generator.trainable_weights)) self.d_loss_metric.update_state(d_loss) self.g_loss_metric.update_state(g_loss) return {"d_loss" self.d_loss_metric.result() "g_loss" self.g_loss_metric.result()} before we start training let s also set up a callback to monitor our results it will use the generator to create and save a number of fake images at the end of each epoch. class ganmonitor(keras.callbacks.callback) def __init__(self num_img=3 latent_dim=128) self.num_img = num_img self.latent_dim = latent_dim def on_epoch_end(self epoch logs=none) random_latent_vectors = tf.random.normal( shape=(self.num_img self.latent_dim)) generated_images = self.model.generator(random_latent_vectors) generated_images *= 255 generated_images.numpy() for i in range(self.num_img) img = keras.utils.array_to_img(generated_images[i]) img.save(f"generated_img_{epoch03d}_{i}.png") finally we can start training. epochs = 100 gan = gan(discriminator=discriminator generator=generator latent_dim=latent_dim) you ll start getting interesting results after epoch 20. gan.compile( d_optimizer=keras.optimizers.adam(learning_rate=0.0001) g_optimizer=keras.optimizers.adam(learning_rate=0.0001) loss_fn=keras.losses.binarycrossentropy() ) gan.fit( dataset epochs=epochs callbacks=[ganmonitor(num_img=10 latent_dim=latent_dim)] ) when training you may see the adversarial loss begin to increase considerably while the discriminative loss tends to zero the discriminator may end up dominating the generator. if that s the case try reducing the discriminator learning rate and increase the dropout rate of the discriminator. figure 12.22 some generated images around epoch 30 12.5.7 wrapping up a gan consists of a generator network coupled with a discriminator network. the discriminator is trained to differentiate between the output of the genera.tor and real images from a training dataset and the generator is trained to fool the discriminator. remarkably the generator never sees images from the training set directly the information it has about the data comes from the discriminator. gans are difficult to train because training a gan is a dynamic process rather than a simple gradient descent process with a fixed loss landscape. getting a gan to train correctly requires using a number of heuristic tricks as well as extensive tuning. gans can potentially produce highly realistic images. but unlike vaes the latent space they learn doesn t have a neat continuous structure and thus may not be suited for certain practical applications such as image editing via latent-space concept vectors. these few techniques cover only the basics of this fast-expanding field. there s a lot more to discover out there generative deep learning is deserving of an entire book of its own. summary you can use a sequence-to-sequence model to generate sequence data one step at a time. this is applicable to text generation but also to note-by-note music generation or any other type of timeseries data. deepdream works by maximizing convnet layer activations through gradient ascent in input space. in the style-transfer algorithm a content image and a style image are combined together via gradient descent to produce an image with the high-level features of the content image and the local characteristics of the style image. vaes and gans are models that learn a latent space of images and can then dream up entirely new images by sampling from the latent space. concept vectors in the latent space can even be used for image editing. this chapter covers hyperparameter tuning model ensembling mixed-precision training training keras models on multiple gpus or on a tpu you ve come far since the beginning of this book. you can now train image classifi.cation models image segmentation models models for classification or regression on vector data timeseries forecasting models text-classification models sequence.to-sequence models and even generative models for text and images. you ve got all the bases covered. however your models so far have all been trained at a small scale on small datasets with a single gpu and they generally haven t reached the best achiev.able performance on each dataset we looked at. this book is after all an introduc.tory book. if you are to go out in the real world and achieve state-of-the-art results on brand new problems there s still a bit of a chasm that you ll need to cross. this penultimate chapter is about bridging that gap and giving you the best practices you ll need as you go from machine learning student to fully fledged 412 getting the most out of your models machine learning engineer. we ll review essential techniques for systematically improv.ing model performance hyperparameter tuning and model ensembling. then we ll look at how you can speed up and scale up model training with multi-gpu and tpu training mixed precision and leveraging remote computing resources in the cloud. 13.1 getting the most out of your models blindly trying out different architecture configurations works well enough if you just need something that works okay. in this section we ll go beyond works okay to works great and wins machine learning competitions via a set of must-know tech.niques for building state-of-the-art deep learning models. 13.1.1 hyperparameter optimization when building a deep learning model you have to make many seemingly arbitrary decisions how many layers should you stack? how many units or filters should go in each layer? should you use relu as activation or a different function? should you use batchnormalization after a given layer? how much dropout should you use? and so on. these architecture-level parameters are called hyperparameters to distinguish them from the parameters of a model which are trained via backpropagation. in practice experienced machine learning engineers and researchers build intu.ition over time as to what works and what doesn t when it comes to these choices they develop hyperparameter-tuning skills. but there are no formal rules. if you want to get to the very limit of what can be achieved on a given task you can t be content with such arbitrary choices. your initial decisions are almost always suboptimal even if you have very good intuition. you can refine your choices by tweaking them by hand and retraining the model repeatedly that s what machine learning engineers and researchers spend most of their time doing. but it shouldn t be your job as a human to fiddle with hyperparameters all day that is better left to a machine. thus you need to explore the space of possible decisions automatically systemati.cally in a principled way. you need to search the architecture space and find the best-performing architectures empirically. that s what the field of automatic hyperparame.ter optimization is about it s an entire field of research and an important one. the process of optimizing hyperparameters typically looks like this 1 choose a set of hyperparameters (automatically). 2 build the corresponding model. 3 fit it to your training data and measure performance on the validation data. 4 choose the next set of hyperparameters to try (automatically). 5 repeat. 6 eventually measure performance on your test data. the key to this process is the algorithm that analyzes the relationship between vali.dation performance and various hyperparameter values to choose the next set of hyperparameters to evaluate. many different techniques are possible bayesian optimi.zation genetic algorithms simple random search and so on. training the weights of a model is relatively easy you compute a loss function on a mini-batch of data and then use backpropagation to move the weights in the right direction. updating hyperparameters on the other hand presents unique challenges. consider these points the hyperparameter space is typically made up of discrete decisions and thus isn t continuous or differentiable. hence you typically can t do gradient descent in hyperparameter space. instead you must rely on gradient-free optimization techniques which naturally are far less efficient than gradient descent. computing the feedback signal of this optimization process (does this set of hyperparameters lead to a high-performing model on this task?) can be extremely expensive it requires creating and training a new model from scratch on your dataset. the feedback signal may be noisy if a training run performs 0.2% better is that because of a better model configuration or because you got lucky with the ini.tial weight values? thankfully there s a tool that makes hyperparameter tuning simpler kerastuner. let s check it out. using kerastuner let s start by installing kerastuner !pip install keras-tuner -q kerastuner lets you replace hard-coded hyperparameter values such as units=32 with a range of possible choices such as int(name="units" min_value=16 max_value=64 step=16). this set of choices in a given model is called the search space of the hyperparameter tuning process. to specify a search space define a model-building function (see the next listing). it takes an hp argument from which you can sample hyperparameter ranges and it returns a compiled keras model. sample hyperparameter values from the from tensorflow import keras hp object. after sampling these values from tensorflow.keras import layers (such as the "units" variable here) are just regular python constants. def build_model(hp) units = hp.int(name="units" min_value=16 max_value=64 step=16) model = keras.sequential([ different kinds of hyperpa. layers.dense(units activation="relu") rameters are available int layers.dense(10 activation="softmax") float boolean choice. ]) optimizer = hp.choice(name="optimizer" values=["rmsprop" "adam"]) model.compile( getting the most out of your models optimizer=optimizer loss="sparse_categorical_crossentropy" metrics=["accuracy"]) return model the function returns a compiled model. if you want to adopt a more modular and configurable approach to model-building you can also subclass the hypermodel class and define a build method as follows. import kerastuner as kt class simplemlp(kt.hypermodel) def __init__(self num_classes) self.num_classes = num_classes def build(self hp) units = hp.int(name="units" min_value=16 max_value=64 step=16) the build() model = keras.sequential([ method is layers.dense(units activation="relu") identical to layers.dense(self.num_classes activation="softmax") our prior ]) build_model() optimizer = hp.choice(name="optimizer" values=["rmsprop" "adam"]) standalone model.compile( function. thanks to the object. optimizer=optimizer oriented approach we can loss="sparse_categorical_crossentropy" configure model constants metrics=["accuracy"]) as constructor arguments return model (instead of hardcoding them in the model-building hypermodel = simplemlp(num_classes=10) function). the next step is to define a tuner. schematically you can think of a tuner as a for loop that will repeatedly pick a set of hyperparameter values call the model-building function with these values to create a model train the model and record its metrics kerastuner has several built-in tuners available randomsearch bayesianoptimiza.tion and hyperband. let s try bayesianoptimization a tuner that attempts to make smart predictions for which new hyperparameter values are likely to perform best given the outcomes of previous choices specify the model-build.ing function (or hyper-model instance). tuner = kt.bayesianoptimization( build_model objective="val_accuracy" max_trials=100 specify the metric that the tuner will seek to optimize. always specify validation metrics since the goal of the search process is to find models that generalize! maximum number of different model configurations ( trials ) to try before ending the search. executions_per_trial=2 to reduce metrics variance you can train the directory="mnist_kt_test" same model multiple times and average the results. overwrite=true ) where to store search logs executions_per_trial is how many training rounds (executions) to run for each model configuration (trial). whether to overwrite data in directory to start a new search. set this to true if you ve modified the model-building function or to false to resume a previously started search with the same model-building function. you can display an overview of the search space via search_space_summary() >>> tuner.search_space_summary() search space summary default search space size 2 units (int) {"default" none "conditions" [] "min_value" 128 "max_value" 1024 "step" 128 "sampling" none} optimizer (choice) {"default" "rmsprop" "conditions" [] "values" ["rmsprop" "adam"] "ordered" false} objective maximization and minimization for built-in metrics (like accuracy in our case) the direction of the metric (accuracy should be maximized but a loss should be minimized) is inferred by kerastuner. however for a custom metric you should specify it yourself like this the metric s name as objective = kt.objective( found in epoch logs name="val_accuracy" direction="max") the metric s tuner = kt.bayesianoptimization( desired direction build_model "min" or "max" objective=objective ... ) finally let s launch the search. don t forget to pass validation data and make sure not to use your test set as validation data otherwise you d quickly start overfitting to your test data and you wouldn t be able to trust your test metrics anymore (x_train y_train) (x_test y_test) = keras.datasets.mnist.load_data() x_train = x_train.reshape((-1 28 * 28)).astype("float32") / 255 x_test = x_test.reshape((-1 28 * 28)).astype("float32") / 255 x_train_full = x_train[] reserve these for later. y_train_full = y_train[] getting the most out of your models set these aside as a validation set. num_val_samples = 10000 x_train x_val = x_train[-num_val_samples] x_train[-num_val_samples] y_train y_val = y_train[-num_val_samples] y_train[-num_val_samples] callbacks = [ keras.callbacks.earlystopping(monitor="val_loss" patience=5) ] tuner.search( x_train y_train batch_size=128 epochs=100 validation_data=(x_val y_val) callbacks=callbacks verbose=2 ) this takes the same arguments as fit() (it simply passes them down to fit() for each new model). use a large number of epochs (you don t know in advance how many epochs each model will need) and use an earlystopping callback to stop training when you start overfitting. the preceding example will run in just a few minutes since we re only looking at a few possible choices and we re training on mnist. however with a typical search space and dataset you ll often find yourself letting the hyperparameter search run overnight or even over several days. if your search process crashes you can always restart it just specify overwrite=false in the tuner so that it can resume from the trial logs stored on disk. once the search is complete you can query the best hyperparameter configura.tions which you can use to create high-performing models that you can then retrain. returns a list of hyperparameter top_n = 4 objects which you can pass to the model-building function best_hps = tuner.get_best_hyperparameters(top_n) usually when retraining these models you may want to include the validation data as part of the training data since you won t be making any further hyperparameter changes and thus you will no longer be evaluating performance on the validation data. in our example we d train these final models on the totality of the original mnist training data without reserving a validation set. before we can train on the full training data though there s one last parameter we need to settle the optimal number of epochs to train for. typically you ll want to train the new models for longer than you did during the search using an aggressive patience value in the earlystopping callback saves time during the search but it may lead to under-fit models. just use the validation set to find the best epoch def get_best_epoch(hp) model = build_model(hp) callbacks=[ note the very high patience keras.callbacks.earlystopping( value. monitor="val_loss" mode="min" patience=10) ] history = model.fit( x_train y_train validation_data=(x_val y_val) epochs=100 batch_size=128 callbacks=callbacks) val_loss_per_epoch = history.history["val_loss"] best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1 print(f"best epoch {best_epoch}") return best_epoch finally train on the full dataset for just a bit longer than this epoch count since you re training on more data 20% more in this case def get_best_trained_model(hp) best_epoch = get_best_epoch(hp) model.fit( x_train_full y_train_full batch_size=128 epochs=int(best_epoch * 1.2)) return model best_models = [] for hp in best_hps model = get_best_trained_model(hp) model.evaluate(x_test y_test) best_models.append(model) note that if you re not worried about slightly underperforming there s a shortcut you can take just use the tuner to reload the top-performing models with the best weights saved during the hyperparameter search without retraining new models from scratch best_models = tuner.get_best_models(top_n) note one important issue to think about when doing automatic hyper-parameter optimization at scale is validation-set overfitting. because you re updating hyperparameters based on a signal that is computed using your vali.dation data you re effectively training them on the validation data and thus they will quickly overfit to the validation data. always keep this in mind. the art of crafting the right search space overall hyperparameter optimization is a powerful technique that is an absolute requirement for getting to state-of-the-art models on any task or to win machine learn.ing competitions. think about it once upon a time people handcrafted the features that went into shallow machine learning models. that was very much suboptimal. now deep learning automates the task of hierarchical feature engineering features are learned using a feedback signal not hand-tuned and that s the way it should be. in the same way you shouldn t handcraft your model architectures you should opti.mize them in a principled way. however doing hyperparameter tuning is not a replacement for being familiar with model architecture best practices. search spaces grow combinatorially with the getting the most out of your models number of choices so it would be far too expensive to turn everything into a hyper-parameter and let the tuner sort it out. you need to be smart about designing the right search space. hyperparameter tuning is automation not magic you use it to automate experiments that you would otherwise have run by hand but you still need to handpick experiment configurations that have the potential to yield good metrics. the good news is that by leveraging hyperparameter tuning the configuration decisions you have to make graduate from micro-decisions (what number of units do i pick for this layer?) to higher-level architecture decisions (should i use residual con.nections throughout this model?). and while micro-decisions are specific to a certain model and a certain dataset higher-level decisions generalize better across different tasks and datasets. for instance pretty much every image classification problem can be solved via the same sort of search-space template. following this logic kerastuner attempts to provide premade search spaces that are relevant to broad categories of problems such as image classification. just add data run the search and get a pretty good model. you can try the hypermodels kt.appli.cations.hyperxception and kt.applications.hyperresnet which are effectively tunable versions of keras applications models. the future of hyperparameter tuning automated machine learning currently most of your job as a deep learning engineer consists of munging data with python scripts and then tuning the architecture and hyperparameters of a deep net.work at length to get a working model or even to get a state-of-the-art model if you are that ambitious. needless to say that isn t an optimal setup. but automation can help and it won t stop merely at hyperparameter tuning. searching over a set of possible learning rates or possible layer sizes is just the first step. we can also be far more ambitious and attempt to generate the model architecture itself from scratch with as few constraints as possible such as via reinforcement learn.ing or genetic algorithms. in the future entire end-to-end machine learning pipelines will be automatically generated rather than be handcrafted by engineer-artisans. this is called automated machine learning or automl. you can already leverage libraries like autokeras (https//github.com/keras-team/autokeras) to solve basic machine learning problems with very little involvement on your part. today automl is still in its early days and it doesn t scale to large problems. but when automl becomes mature enough for widespread adoption the jobs of machine learning engineers won t disappear rather engineers will move up the value-creation chain. they will begin to put much more effort into data curation crafting complex loss functions that truly reflect business goals as well as understanding how their models impact the digital ecosystems in which they re deployed (such as the users who consume the model s predictions and generate the model s training data). these are problems that only the largest companies can afford to consider at present. always look at the big picture focus on understanding the fundamentals and keep in mind that the highly specialized tedium will eventually be automated away. see it as a gift greater productivity for your workflows and not as a threat to your own rele.vance. it shouldn t be your job to tune knobs endlessly. 13.1.2 model ensembling another powerful technique for obtaining the best possible results on a task is model ensembling. ensembling consists of pooling together the predictions of a set of differ.ent models to produce better predictions. if you look at machine learning competi.tions in particular on kaggle you ll see that the winners use very large ensembles of models that inevitably beat any single model no matter how good. ensembling relies on the assumption that different well-performing models trained independently are likely to be good for different reasons each model looks at slightly different aspects of the data to make its predictions getting part of the truth but not all of it. you may be familiar with the ancient parable of the blind men and the elephant a group of blind men come across an elephant for the first time and try to understand what the elephant is by touching it. each man touches a different part of the elephant s body just one part such as the trunk or a leg. then the men describe to each other what an elephant is it s like a snake like a pillar or a tree and so on. the blind men are essentially machine learning models trying to understand the manifold of the training data each from their own perspective using their own assumptions (provided by the unique architecture of the model and the unique ran.dom weight initialization). each of them gets part of the truth of the data but not the whole truth. by pooling their perspectives together you can get a far more accurate description of the data. the elephant is a combination of parts not any single blind man gets it quite right but interviewed together they can tell a fairly accurate story. let s use classification as an example. the easiest way to pool the predictions of a set of classifiers (to ensemble the classifiers) is to average their predictions at inference time preds_a = model_a.predict(x_val) preds_b = model_b.predict(x_val) preds_c = model_c.predict(x_val) preds_d = model_d.predict(x_val) use four different models to compute initial predictions. final_preds = 0.25 *(preds_a+preds_b+preds_c+preds_d) this new prediction array should be more accurate than any of the initial ones. however this will work only if the classifiers are more or less equally good. if one of them is significantly worse than the others the final predictions may not be as good as the best classifier of the group. a smarter way to ensemble classifiers is to do a weighted average where the weights are learned on the validation data typically the better classifiers are given a higher weight and the worse classifiers are given a lower weight. to search for a good set of ensembling weights you can use random search or a simple optimization algo.rithm such as the nelder-mead algorithm preds_a = model_a.predict(x_val) preds_b = model_b.predict(x_val) preds_c = model_c.predict(x_val) scaling-up model training preds_d = model_d.predict(x_val) final_preds = 0.5 *preds_a+ 0.25 *preds_b+ 0.1 *preds_c+ 0.15 *preds_d these weights (0.5 0.25 0.1 0.15) are assumed to be learned empirically. there are many possible variants you can do an average of an exponential of the pre.dictions for instance. in general a simple weighted average with weights optimized on the validation data provides a very strong baseline. the key to making ensembling work is the diversity of the set of classifiers. diversity is strength. if all the blind men only touched the elephant s trunk they would agree that elephants are like snakes and they would forever stay ignorant of the truth of the elephant. diversity is what makes ensembling work. in machine learning terms if all of your models are biased in the same way your ensemble will retain this same bias. if your models are biased in different ways the biases will cancel each other out and the ensemble will be more robust and more accurate. for this reason you should ensemble models that are as good as possible while being as different as possible. this typically means using very different architectures or even different brands of machine learning approaches. one thing that is largely not worth doing is ensembling the same network trained several times independently from dif.ferent random initializations. if the only difference between your models is their ran.dom initialization and the order in which they were exposed to the training data then your ensemble will be low-diversity and will provide only a tiny improvement over any single model. one thing i have found to work well in practice but that doesn t generalize to every problem domain is using an ensemble of tree-based methods (such as random forests or gradient-boosted trees) and deep neural networks. in 2014 andrey kolev and i took fourth place in the higgs boson decay detection challenge on kaggle (www.kaggle.com/c/higgs-boson) using an ensemble of various tree models and deep neural networks. remarkably one of the models in the ensemble originated from a different method than the others (it was a regularized greedy forest) and it had a sig.nificantly worse score than the others. unsurprisingly it was assigned a small weight in the ensemble. but to our surprise it turned out to improve the overall ensemble by a large factor because it was so different from every other model it provided informa.tion that the other models didn t have access to. that s precisely the point of ensem.bling. it s not so much about how good your best model is it s about the diversity of your set of candidate models. 13.2 scaling-up model training recall the loop of progress concept we introduced in chapter 7 the quality of your ideas is a function of how many refinement cycles they ve been through (see figure 13.1). and the speed at which you can iterate on an idea is a function of how fast you can set up an experiment how fast you can run that experiment and finally how well you can analyze the resulting data. as you develop your expertise with the keras api how fast you can code up your deep learning experiments will cease to be the bottleneck of this progress cycle. the next bottleneck will become the speed at which you can train your models. fast training infrastructure means that you can get your results back in 10 15 minutes and hence that you can go through dozens of iterations every day. faster training directly improves the quality of your deep learning solutions. in this section you ll learn about three ways you can train your models faster mixed-precision training which you can use even with a single gpu training on multiple gpus training on tpus let s go. 13.2.1 speeding up training on gpu with mixed precision what if i told you there s a simple technique you can use to speed up the training of almost any model by up to 3x basically for free? it seems too good to but true and yet such a trick does exist. that s mixed-precision training. to understand how it works we first need to take a look at the notion of precision in computer science. understanding floating-point precision precision is to numbers what resolution is to images. because computers can only pro.cess ones and zeros any number seen by a computer has to be encoded as a binary string. for instance you may be familiar with uint8 integers which are integers encoded on eight bits 00000000 represents 0 in uint8 and 11111111 represents 255. to represent integers beyond 255 you d need to add more bits eight isn t enough. most integers are stored on 32 bits with which you can represent signed integers rang.ing from 2147483648 to 2147483647. floating-point numbers are the same. in mathematics real numbers form a con.tinuous axis there s an infinite number of points in between any two numbers. you can always zoom in on the axis of reals. in computer science this isn t true there s a finite number of intermediate points between 3 and 4 for instance. how many? well it depends on the precision you re working with the number of bits you re using to store a number. you can only zoom up to a certain resolution. scaling-up model training there are three of levels of precision you d typically use half precision or float16 where numbers are stored on 16 bits single precision or float32 where numbers are stored on 32 bits double precision or float64 where numbers are stored on 64 bits a note on floating-point encoding a counterintuitive fact about floating-point numbers is that representable numbers are not uniformly distributed. larger numbers have lower precision there are the same number of representable values between 2** n and 2 ** (n +1) as there are between 1 and 2 for any n. that s because floating-point numbers are encoded in three parts the sign the sig.nificant value (called the "mantissa") and the exponent in the form {sign} * (2 ** ({exponent} - 127)) * 1.{mantissa} for example here s how you would encode the closest float32 value approximating pi 1 bit 8 bits 23 bits sign exponent mantissa +1 128 5707963705062866 value = +1 * (2 ** (128 - 127)) * 1.5707963705062866 value = 3.1415927410125732 the number pi encoded in single precision via a sign bit an integer exponent and an integer mantissa for this reason the numerical error incurred when converting a number to its floating-point representation can vary wildly depending on the exact value considered and the error tends to get larger for numbers with a large absolute value. the way to think about the resolution of floating-point numbers is in terms of the smallest distance between two arbitrary numbers that you ll be able to safely process. in single precision that s around 1e-7. in double precision that s around 1e-16. and in half precision it s only 1e-3. every model you ve seen in this book so far used single-precision numbers it stored its state as float32 weight variables and ran its computations on float32 inputs. that s enough precision to run the forward and backwards pass of a model without losing any information particularly when it comes to small gradient updates (recall that the typical learning rate is 1e-3 and it s pretty common to see weight updates on the order of 1e-6). you could also use float64 though that would be wasteful operations like matrix multiplication or addition are much more expensive in double precision so you d be doing twice as much work for no clear benefits. but you could not do the same with float16 weights and computation the gradient descent process wouldn t run smoothly since you couldn t represent small gradient updates of around 1e-5 or 1e-6. you can however use a hybrid approach that s what mixed precision is about. the idea is to leverage 16-bit computations in places where precision isn t an issue and to work with 32-bit values in other places to maintain numerical stability. modern gpus and tpus feature specialized hardware that can run 16-bit operations much faster and use less memory than equivalent 32-bits operations. by using these lower-precision operations whenever possible you can speed up training on those devices by a signifi.cant factor. meanwhile by maintaining the precision-sensitive parts of the model in single precision you can get these benefits without meaningfully impacting model quality. and those benefits are considerable on modern nvidia gpus mixed precision can speed up training by up to 3x. it s also beneficial when training on a tpu (a sub.ject we ll get to in a bit) where it can speed up training by up to 60%. beware of dtype defaults single precision is the default floating-point type throughout keras and tensorflow any tensor or variable you create will be in float32 unless you specify otherwise. for numpy arrays however the default is float64! converting a default numpy array to a tensorflow tensor will result in a float64 ten. sor which may not be what you want >>> import tensorflow as tf >>> import numpy as np >>> np_array = np.zeros((2 2)) >>> tf_tensor = tf.convert_to_tensor(np_array) >>> tf_tensor.dtype tf.float64 remember to be explicit about data types when converting numpy arrays >>> np_array = np.zeros((2 2)) >>> tf_tensor = tf.convert_to_tensor(np_array dtype="float32") >>> tf_tensor.dtype tf.float32 specify the dtype explicitly. note that when you call the keras fit() method with numpy data it will do this con.version for you. scaling-up model training mixed-precision training in practice when training on a gpu you can turn on mixed precision like this from tensorflow import keras keras.mixed_precision.set_global_policy("mixed_float16") typically most of the forward pass of the model will be done in float16 (with the exception of numerically unstable operations like softmax) while the weights of the model will be stored and updated in float32. keras layers have a variable_dtype and a compute_dtype attribute. by default both of these are set to float32. when you turn on mixed precision the compute_dtype of most layers switches to float16 and those layers will cast their inputs to float16 and will perform their computations in float16 (using half-precision copies of the weights). however since their variable_dtype is still float32 their weights will be able to receive accurate float32 updates from the optimizer as opposed to half-precision updates. note that some operations may be numerically unstable in float16 (in particular softmax and crossentropy). if you need to opt out of mixed precision for a specific layer just pass the argument dtype="float32" to the constructor of this layer. 13.2.2 multi-gpu training while gpus are getting more powerful every year deep learning models are getting increasingly larger requiring ever more computational resources. training on a single gpu puts a hard bound on how fast you can move. the solution? you could simply add more gpus and start doing multi-gpu distributed training. there are two ways to distribute computation across multiple devices data parallel.ism and model parallelism. with data parallelism a single model is replicated on multiple devices or multiple machines. each of the model replicas processes different batches of data and then they merge their results. with model parallelism different parts of a single model run on different devices processing a single batch of data together at the same time. this works best with models that have a naturally parallel architecture such as models that feature multiple branches. in practice model parallelism is only used for models that are too large to fit on any single device it isn t used as a way to speed up training of regular models but as a way to train larger models. we won t cover model parallelism in these pages instead we ll focus on what you ll be using most of the time data parallelism. let s take a look at how it works. getting your hands on two or more gpus first you need to get access to several gpus. as of now google colab only lets you use a single gpu so you will need to do one of two things acquire 2 4 gpus mount them on a single machine (it will require a beefy power supply) and install cuda drivers cudnn etc. for most people this isn t the best option. rent a multi-gpu virtual machine (vm) on google cloud azure or aws. you ll be able to use vm images with preinstalled drivers and software and you ll have very little setup overhead. this is likely the best option for anyone who isn t training models 24/7. we won t cover the details of how to spin up multi-gpu cloud vms because such instruc.tions would be relatively short-lived and this information is readily available online. and if you don t want to deal with the overhead of managing your own vm instances you can use tensorflow cloud (https//github.com/tensorflow/cloud) a package that my team and i have recently released it enables you to start training on multiple gpus by just adding one line of code at the start of a colab notebook. if you re looking for a seamless transition from debugging your model in colab to train.ing it as fast as possible on as many gpus as you want check it out. single-host multi-device synchronous training once you re able to importtensorflow on a machine with multiple gpus you re sec.onds away from training a distributed model. it works like this create a distribution strategy object. mirroredstrategy should be your go-to solution. strategy = tf.distribute.mirroredstrategy() print(f"number of devices {strategy.num_replicas_in_sync}") with strategy.scope() model = get_compiled_model() model.fit( train_dataset epochs=100 validation_data=val_dataset callbacks=callbacks) use it to open a strategy scope. everything that creates variables should be under the strategy scope. in general this is only model construction and compile(). train the model on all available devices. these few lines implement the most common training setup single-host multi-device synchronous training also known in tensorflow as the mirrored distribution strategy. single host means that the different gpus considered are all on a single machine (as opposed to a cluster of many machines each with its own gpu communicating over a network). synchronous training means that the state of the per-gpu model replicas stays the same at all times there are variants of distributed training where this isn t the case. when you open a mirroredstrategy scope and build your model within it the mirroredstrategy object will create one model copy (replica) on each available gpu. then each step of training unfolds in the following way (see figure 13.2) 1 a batch of data (called global batch) is drawn from the dataset. 2 it gets split into four different sub-batches (called local batches). for instance if the global batch has 512 samples each of the four local batches will have 128 samples. because you want local batches to be large enough to keep the gpu busy the global batch size typically needs to be very large. scaling-up model training 3 each of the four replicas processes one local batch independently on its own device they run a forward pass and then a backward pass. each replica outputs a weight delta describing by how much to update each weight variable in the model given the gradient of the previous weights with respect to the loss of the model on the local batch. 4 the weight deltas originating from local gradients are efficiently merged across the four replicas to obtain a global delta which is applied to all replicas. because this is done at the end of every step the replicas always stay in sync their weights are always equal. local figure 13.2 one step of mirroredstrategy training each model replica computes local weight updates which are then merged and used to update the state of all replicas. tf.data performance tips when doing distributed training always provide your data as a tf.data.dataset object to guarantee best performance. (passing your data as numpy arrays also works since those get converted to dataset objects by fit()). you should also make sure you leverage data prefetching before passing the dataset to fit() call dataset.prefetch(buffer_size). if you aren t sure what buffer size to pick try the dataset.prefetch(tf.data.autotune) option which will pick a buffer size for you. in an ideal world training on n gpus would result in a speedup of factor n. in prac.tice however distribution introduces some overhead in particular merging the weight deltas originating from different devices takes some time. the effective speedup you get is a function of the number of gpus used with two gpus the speedup stays close to 2x. with four the speedup is around 3.8x. with eight it s around 7.3x. this assumes that you re using a large enough global batch size to keep each gpu uti.lized at full capacity. if your batch size is too small the local batch size won t be enough to keep your gpus busy. 13.2.3 tpu training beyond just gpus there is a trend in the deep learning world toward moving work.flows to increasingly specialized hardware designed specifically for deep learning workflows (such single-purpose chips are known as asics application-specific inte.grated circuits). various companies big and small are working on new chips but today the most prominent effort along these lines is google s tensor processing unit (tpu) which is available on google cloud and via google colab. training on a tpu does involve jumping through some hoops but it s worth the extra work tpus are really really fast. training on a tpu v2 will typically be 15x faster than training an nvidia p100 gpu. for most models tpu training ends up being 3x more cost-effective than gpu training on average. using a tpu via google colab you can actually use an 8-core tpu for free in colab. in the colab menu under the runtime tab in the change runtime type option you ll notice that you have access to a tpu runtime in addition to the gpu runtime. when you re using the gpu runtime your models have direct access to the gpu without you needing to do anything special. this isn t true for the tpu runtime there s an extra step you need to take before you can start building a model you need to connect to the tpu cluster. it works like this import tensorflow as tf tpu = tf.distribute.cluster_resolver.tpuclusterresolver.connect() print("device" tpu.master()) you don t have to worry too much about what this does it s just a little incantation that connects your notebook runtime to the device. open sesame. much like in the case of multi-gpu training using the tpu requires you to open a distribution strategy scope in this case a tpustrategy scope. tpustrategy follows the same distribution template as mirroredstrategy the model is replicated once per tpu core and the replicas are kept in sync. here s a simple example. from tensorflow import keras from tensorflow.keras import layers strategy = tf.distribute.tpustrategy(tpu) print(f"number of replicas {strategy.num_replicas_in_sync}") def build_model(input_size) inputs = keras.input((input_size input_size 3)) x = keras.applications.resnet.preprocess_input(inputs) x = keras.applications.resnet.resnet50( weights=none include_top=false pooling="max")(x) scaling-up model training 429 outputs = layers.dense(10 activation="softmax")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop" loss="sparse_categorical_crossentropy" metrics=["accuracy"]) return model with strategy.scope() model = build_model(input_size=32) we re almost ready to start training. but there s something a bit curious about tpus in colab it s a two-vm setup meaning that the vm that hosts your notebook runtime isn t the same vm that the tpu lives in. because of this you won t be able to train from files stored on the local disk (that is to say on the disk linked to the vm that hosts the notebook). the tpu runtime can t read from there. you have two options for data loading train from data that lives in the memory of the vm (not on disk). if your data is in a numpy array this is what you re already doing. store the data in a google cloud storage (gcs) bucket and create a dataset that reads the data directly from the bucket without downloading locally. the tpu runtime can read data from gcs. this is your only option for datasets that are too large to live entirely in memory. in our case let s train from numpy arrays in memory the cifar10 dataset (x_train y_train) (x_test y_test) = keras.datasets.cifar10.load_data() model.fit(x_train y_train batch_size=1024) note that tpu training much like multi-gpu training requires large batch sizes to make sure the device stays well-utilized. you ll notice that the first epoch takes a while to start that s because your model is getting compiled to something that the tpu can execute. once that step is done the training itself is blazing fast. beware of i/o bottlenecks because tpus can process batches of data extremely quickly the speed at which you can read data from gcs can easily become a bottleneck. if your dataset is small enough you should keep it in the memory of the vm. you can do so by calling dataset.cache() on your dataset. that way the data will only be read from gcs once. if your dataset is too large to fit in memory make sure to store it as tfrecord files an efficient binary storage format that can be loaded very quickly. on keras.io you ll find a code example demonstrating how to format your data as tfrecord files (https//keras.io/examples/keras_recipes/creating_tfrecords/). leveraging step fusing to improve tpu utilization because a tpu has a lot of compute power available you need to train with very large batches to keep the tpu cores busy. for small models the batch size required can get extraordinarily large upwards of 10000 samples per batch. when working with enor.mous batches you should make sure to increase your optimizer learning rate accord.ingly you re going to be making fewer updates to your weights but each update will be more accurate (since the gradients are computed using more data points) so you should move the weights by a greater magnitude with each update. there is however a simple trick you can leverage to keep reasonably sized batches while maintaining full tpu utilization step fusing. the idea is to run multiple steps of training during each tpu execution step. basically do more work in between two round trips from the vm memory to the tpu. to do this simply specify the steps_ per_execution argument in compile() for instance steps_per_execution=8 to run eight steps of training during each tpu execution. for small models that are underutilizing the tpu this can result in a dramatic speedup. summary you can leverage hyperparameter tuning and kerastuner to automate the tedium out of finding the best model configuration. but be mindful of validation-set overfitting! an ensemble of diverse models can often significantly improve the quality of your predictions. you can speed up model training on gpu by turning on mixed precision you ll generally get a nice speed boost at virtually no cost. to further scale your workflows you can use the tf.distribute.mirrored-strategy api to train models on multiple gpus. you can even train on google s tpus (available on colab) by using the tpu-strategy api. if your model is small make sure to leverage step fusing (via the compile( steps_per_execution=n) argument) in order to fully utilize the tpu cores. this chapter covers important takeaways from this book the limitations of deep learning possible future directions for deep learning machine learning and ai resources for further learning and applying your skills in practice you ve almost reached the end of this book. this last chapter will summarize and review core concepts while also expanding your horizons beyond what you ve learned so far. becoming an effective ai practitioner is a journey and finishing this book is merely your first step on it. i want to make sure you realize this and are properly equipped to take the next steps of this journey on your own. we ll start with a bird s-eye view of what you should take away from this book. this should refresh your memory regarding some of the concepts you ve learned. next i ll present an overview of some key limitations of deep learning. to use a tool appropriately you should not only understand what it can do but also be aware of what it can t do. finally i ll offer some speculative thoughts about the future evolution of deep learning machine learning and ai. this should be 431 especially interesting to you if you d like to get into fundamental research. the chap.ter ends with a short list of resources and strategies for further learning about machine learning and staying up to date with new advances. 14.1 key concepts in review this section briefly synthesizes key takeaways from this book. if you ever need a quick refresher to help you recall what you ve learned you can read these few pages. 14.1.1 various approaches to ai first of all deep learning isn t synonymous with ai or even with machine learning artificial intelligence (ai) is an ancient broad field that can generally be under.stood as all attempts to automate human cognitive processes. this can range from the very basic such as an excel spreadsheet to the very advanced like a humanoid robot that can walk and talk. machine learning is a specific subfield of ai that aims at automatically developing programs (called models) purely from exposure to training data. this process of turning data into a program is called learning. although machine learning has been around for a long time it only started to take off in the 1990s before becoming the dominant form of ai in the 2000s. deep learning is one of many branches of machine learning where the models are long chains of geometric transformations applied one after the other. these operations are structured into modules called layers deep learning mod.els are typically stacks of layers or more generally graphs of layers. these lay.ers are parameterized by weights which are the parameters learned during training. the knowledge of a model is stored in its weights and the process of learning consists of finding good values for these weights values that mini.mize a loss function. because the chain of geometric transformations considered is differentiable updating the weights to minimize the loss function is done effi.ciently via gradient descent. even though deep learning is just one among many approaches to machine learn.ing it isn t on an equal footing with the others. deep learning is a breakout success. here s why. 14.1.2 what makes deep learning special within the field of machine learning in the span of only a few years deep learning has achieved tremendous breakthroughs across a wide range of tasks that have been historically perceived as extremely difficult for computers especially in the area of machine perception extracting useful infor.mation from images videos sound and more. given sufficient training data (in par.ticular training data appropriately labeled by humans) deep learning makes it possible to extract from perceptual data almost anything a human could. hence it s sometimes said that deep learning has solved perception although that s true only for a fairly narrow definition of perception. due to its unprecedented technical successes deep learning has singlehandedly brought about the third and by far the largest ai summer a period of intense interest investment and hype in the field of ai. as this book is being written we re in the mid.dle of it. whether this period will end in the near future and what happens after it ends are topics of debate. one thing is certain in stark contrast with previous ai sum.mers deep learning has provided enormous business value to both large and small technology companies enabling human-level speech recognition smart assistants human-level image classification vastly improved machine translation and more. the hype may (and likely will) recede but the sustained economic and technological impact of deep learning will remain. in that sense deep learning could be analogous to the internet it may be overly hyped up for a few years but in the longer term it will still be a major revolution that will transform our economy and our lives. i m particularly optimistic about deep learning because even if we were to make no further technological progress in the next decade deploying existing algorithms to every applicable problem would be a game changer for most industries. deep learning is nothing short of a revolution and progress is currently happening at an incredibly fast rate due to an exponential investment in resources and headcount. from where i stand the future looks bright although short-term expectations are somewhat overoptimistic deploying deep learning to the full extent of its potential will likely take multiple decades. 14.1.3 how to think about deep learning the most surprising thing about deep learning is how simple it is. ten years ago no one expected that we would achieve such amazing results on machine-perception problems by using simple parametric models trained with gradient descent. now it turns out that all you need is sufficiently large parametric models trained with gradi.ent descent on sufficiently many examples. as feynman once said about the universe it s not complicated it s just a lot of it. 1 in deep learning everything is a vector that is to say everything is a point in a geo.metric space. model inputs (text images and so on) and targets are first vectorized turned into an initial input vector space and target vector space. each layer in a deep learning model operates one simple geometric transformation on the data that goes through it. together the chain of layers in the model forms one complex geometric transformation broken down into a series of simple ones. this complex transforma.tion attempts to map the input space to the target space one point at a time. this transformation is parameterized by the weights of the layers which are iteratively updated based on how well the model is currently performing. a key characteristic of this geometric transformation is that it must be differentiable which is required in order 1 richard feynman interview the world from another point of view yorkshire television 1972. for us to be able to learn its parameters via gradient descent. intuitively this means the geometric morphing from inputs to outputs must be smooth and continuous a significant constraint. the entire process of applying this complex geometric transformation to the input data can be visualized in 3d by imagining a person trying to uncrumple a paper ball the crumpled paper ball is the manifold of the input data that the model starts with. each movement operated by the person on the paper ball is similar to a simple geometric transformation operated by one layer. the full uncrumpling ges.ture sequence is the complex transformation of the entire model. deep learning models are mathematical machines for uncrumpling complicated manifolds of high-dimensional data. that s the magic of deep learning turning meaning into vectors then into geo.metric spaces and then incrementally learning complex geometric transformations that map one space to another. all you need are spaces of sufficiently high dimension.ality in order to capture the full scope of the relationships found in the original data. the whole process hinges on a single core idea that meaning is derived from the pair.wise relationship between things (between words in a language between pixels in an image and so on) and that these relationships can be captured by a distance function. but note that whether the brain also implements meaning via geometric spaces is an entirely separate question. vector spaces are efficient to work with from a computational standpoint but different data structures for intelligence can easily be envisioned in particular graphs. neural networks initially emerged from the idea of using graphs as a way to encode meaning which is why they re named neural networks the surrounding field of research used to be called connectionism. nowadays the name neural network exists purely for historical reasons it s an extremely misleading name because they re nei.ther neural nor networks. in particular neural networks have hardly anything to do with the brain. a more appropriate name would have been layered representations learn.ing or hierarchical representations learning or maybe even deep differentiable models or chained geometric transforms to emphasize the fact that continuous geometric space manipulation is at their core. 14.1.4 key enabling technologies the technological revolution that s currently unfolding didn t start with any single breakthrough invention. rather like any other revolution it s the product of a vast accumulation of enabling factors gradual at first and then sudden. in the case of deep learning we can point out the following key factors incremental algorithmic innovations these first began appearing slowly over the span of two decades (starting with backpropagation) and then were devel.oped increasingly faster as more research effort was poured into deep learn.ing after 2012. the availability of large amounts of perceptual data this was a requirement in order to realize that sufficiently large models trained on sufficiently large data are all we need. this is in turn a byproduct of the rise of the consumer inter-net and moore s law applied to storage media. the availability of fast highly parallel computation hardware at a low price especially the gpus produced by nvidia first gaming gpus and then chips designed from the ground up for deep learning. early on nvidia ceo jensen huang took note of the deep learning boom and decided to bet the company s future on it which paid off in a big way. a complex stack of software layers that makes this computational power available to humans the cuda language frameworks like tensorflow that do auto.matic differentiation and keras which makes deep learning accessible to most people. in the future deep learning will not be used only by specialists researchers graduate students and engineers with an academic profile it will be a tool in the toolbox of every developer much like web technology today. everyone needs to build intelligent apps just as every business today needs a website every product will need to intelli.gently make sense of user-generated data. bringing about this future will require us to build tools that make deep learning radically easy to use and accessible to anyone with basic coding abilities. keras has been the first major step in that direction. 14.1.5 the universal machine learning workflow having access to an extremely powerful tool for creating models that map any input space to any target space is great but the difficult part of the machine learning work.flow is often everything that comes before designing and training such models (and for production models what comes after as well). understanding the problem domain so as to be able to determine what to attempt to predict given what data and how to measure success is a prerequisite for any successful application of machine learning and it isn t something that advanced tools like keras and tensorflow can help you with. as a reminder here s a quick summary of the typical machine learning workflow as described in chapter 6 1 define the problem what data is available and what are you trying to predict? will you need to collect more data or hire people to manually label a dataset? 2 identify a way to reliably measure success on your goal. for simple tasks this may be prediction accuracy but in many cases it will require sophisticated domain-specific metrics. 3 prepare the validation process that you ll use to evaluate your models. in partic.ular you should define a training set a validation set and a test set. the valida.tion- and test-set labels shouldn t leak into the training data for instance with temporal prediction the validation and test data should be posterior to the training data. 4 vectorize the data by turning it into vectors and preprocessing it in a way that makes it more easily approachable by a neural network (normalization and so on). 5 develop a first model that beats a trivial common-sense baseline thus demon.strating that machine learning can work on your problem. this may not always be the case! 6 gradually refine your model architecture by tuning hyperparameters and add.ing regularization. make changes based on performance on the validation data only not the test data or the training data. remember that you should get your model to overfit (thus identifying a model capacity level that s greater than you need) and only then begin to add regularization or downsize your model. beware of validation-set overfitting when tuning hyperparameters the fact that your hyperparameters may end up being overspecialized to the validation set. avoiding this is the purpose of having a separate test set. 7 deploy your final model in production as a web api as part of a javascript or c++ application on an embedded device etc. keep monitoring its perfor.mance on real-world data and use your findings to refine the next iteration of the model! 14.1.6 key network architectures the four families of network architectures that you should be familiar with are densely connected networks convolutional networks recurrent networks and transformers. each type of model is meant for a specific input modality. a network architecture encodes assumptions about the structure of the data a hypothesis space within which the search for a good model will proceed. whether a given architecture will work on a given problem depends entirely on the match between the structure of the data and the assumptions of the network architecture. these different network types can easily be combined to achieve larger multi-modal models much as you combine lego bricks. in a way deep learning layers are lego bricks for information processing. here s a quick overview of the mapping between input modalities and appropriate network architectures vector data densely connected models (dense layers). image data 2d convnets. sequence data rnns for timeseries or transformers for discrete sequences (such as sequences of words). 1d convnets can also be used for translation-invariant continuous sequence data such as birdsong waveforms. video data either 3d convnets (if you need to capture motion effects) or a combination of a frame-level 2d convnet for feature extraction followed by a sequence-processing model. volumetric data 3d convnets. now let s quickly review the specificities of each network architecture. densely connected networks a densely connected network is a stack of dense layers meant to process vector data (where each sample is a vector of numerical or categorical attributes). such networks assume no specific structure in the input features they re called densely connected because the units of a dense layer are connected to every other unit. the layer attempts to map relationships between any two input features this is unlike a 2d con.volution layer for instance which only looks at local relationships. densely connected networks are most commonly used for categorical data (for example where the input features are lists of attributes) such as the boston housing price dataset used in chapter 4. they re also used as the final classification or regres.sion stage of most networks. for instance the convnets covered in chapter 8 typically end with one or two dense layers and so do the recurrent networks in chapter 10. remember to perform binary classification end your stack of layers with a dense layer with a single unit and a sigmoid activation and use binary_crossentropy as the loss. your targets should be either 0 or 1 from tensorflow import keras from tensorflow.keras import layers inputs = keras.input(shape=(num_input_features)) x = layers.dense(32activation="relu")(inputs) x = layers.dense(32activation="relu")(x) outputs = layers.dense(1activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop"loss="binary_crossentropy") to perform single-label categorical classification (where each sample has exactly one class no more) end your stack of layers with a dense layer with a number of units equal to the number of classes and a softmax activation. if your targets are one-hot encoded use categorical_crossentropy as the loss if they re integers use sparse_categorical_ crossentropy inputs = keras.input(shape=(num_input_features)) x = layers.dense(32activation="relu")(inputs) x = layers.dense(32activation="relu")(x) outputs = layers.dense(num_classes activation="softmax")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop"loss="categorical_crossentropy") to perform multilabel categorical classification (where each sample can have several classes) end your stack of layers with a dense layer with a number of units equal to the number of classes and a sigmoid activation and use binary_crossentropy as the loss. your targets should be multi-hot encoded inputs = keras.input(shape=(num_input_features)) x = layers.dense(32activation="relu")(inputs) x = layers.dense(32activation="relu")(x) outputs = layers.dense(num_classes activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop"loss="binary_crossentropy") to perform regression toward a vector of continuous values end your stack of layers with a dense layer with a number of units equal to the number of values you re trying to predict (often a single one such as the price of a house) and no activation. various losses can be used for regression most commonly mean_squared_error (mse) inputs = keras.input(shape=(num_input_features)) x = layers.dense(32activation="relu")(inputs) x = layers.dense(32activation="relu")(x) outputs layers.dense(num_values)(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop"loss="mse") convnets convolution layers look at spatially local patterns by applying the same geometric transformation to different spatial locations (patches) in an input tensor. this results in representations that are translation invariant making convolution layers highly data efficient and modular. this idea is applicable to spaces of any dimensionality 1d (continuous sequences) 2d (images) 3d (volumes) and so on. you can use the conv1d layer to process sequences the conv2d layer to process images and the conv3d layers to process volumes. as a leaner more efficient alternative to convolution layers you can also use depthwise separable convolution layers such as separableconv2d. convnets or convolutional networks consist of stacks of convolution and max-pooling layers. the pooling layers let you spatially downsample the data which is required to keep feature maps to a reasonable size as the number of features grows and to allow sub.sequent convolution layers to see a greater spatial extent of the inputs. convnets are often ended with either a flatten operation or a global pooling layer turning spatial fea.ture maps into vectors followed by dense layers to achieve classification or regression. here s a typical image-classification network (categorical classification in this case) leveraging separableconv2d layers inputs = keras.input(shape=(height width channels)) x = layers.separableconv2d(32 3activation="relu")(inputs) x = layers.separableconv2d(64 3activation="relu")(x) x = layers.maxpooling2d(2)(x) x = layers.separableconv2d(64 3activation="relu")(x) x = layers.separableconv2d(128 3activation="relu")(x) x = layers.maxpooling2d(2)(x) x = layers.separableconv2d(64 3activation="relu")(x) x = layers.separableconv2d(128 3activation="relu")(x) x = layers.globalaveragepooling2d()(x) x = layers.dense(32activation="relu")(x) outputs = layers.dense(num_classes activation="softmax")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop"loss="categorical_crossentropy") when building a very deep convnet it s common to add batch normalization layers as well as residual connections two architecture patterns that help gradient information flow smoothly through the network. rnns recurrent neural networks (rnns) work by processing sequences of inputs one timestep at a time and maintaining a state throughout (a state is typically a vector or set of vec.tors). they should be used preferentially over 1d convnets in the case of sequences where patterns of interest aren t invariant by temporal translation (for instance timeseries data where the recent past is more important than the distant past). three rnn layers are available in keras simplernn gru and lstm. for most practi.cal purposes you should use either gru or lstm. lstm is the more powerful of the two but is also more expensive you can think of gru as a simpler cheaper alternative to it. in order to stack multiple rnn layers on top of each other each layer prior to the last layer in the stack should return the full sequence of its outputs (each input time-step will correspond to an output timestep). if you aren t stacking any further rnn layers it s common to return only the last output which contains information about the entire sequence. following is a single rnn layer for binary classification of vector sequences inputs = keras.input(shape=(num_timesteps num_features)) x = layers.lstm(32)(inputs) outputs = layers.dense(num_classes activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop"loss="binary_crossentropy") and this is a stacked rnn for binary classification of vector sequences inputs = keras.input(shape=(num_timesteps num_features)) x = layers.lstm(32return_sequences=true)(inputs) x = layers.lstm(32return_sequences=true)(x) x = layers.lstm(32)(x) outputs = layers.dense(num_classes activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop"loss="binary_crossentropy") transformers a transformer looks at a set of vectors (such as word vectors) and leverages neural attention to transform each vector into a representation that is aware of the context pro.vided by the other vectors in the set. when the set in question is an ordered sequence you can also leverage positional encoding to create transformers that can take into account both global context and word order capable of processing long text para.graphs much more effectively than rnns or 1d convnets. transformers can be used for any set-processing or sequence-processing task including text classification but they excel especially at sequence-to-sequence learning such as translating paragraphs in a source language into a target language. a sequence-to-sequence transformer is made up of two parts a transformerencoder that turns an input vector sequence into a context-aware order-aware output vector sequence a transformerdecoder that takes the output of the transformerencoder as well as a target sequence and predicts what should come next in the target sequence if you re only processing a single sequence (or set) of vectors you d be only using the transformerencoder. following is a sequence-to-sequence transformer for mapping a source sequence to a target sequence (this setup could be used for machine translation or question answering for instance) source sequence encoder_inputs = keras.input(shape=(sequence_length) dtype="int64") x = positionalembedding( target sequence_length vocab_size embed_dim)(encoder_inputs) sequence encoder_outputs = transformerencoder(embed_dim dense_dim num_heads)(x) so far decoder_inputs = keras.input(shape=(none) dtype="int64") x = positionalembedding( sequence_length vocab_size embed_dim)(decoder_inputs) x = transformerdecoder(embed_dim dense_dim num_heads)(x encoder_outputs) decoder_outputs = layers.dense(vocab_size activation="softmax")(x) transformer = keras.model([encoder_inputs decoder_inputs] decoder_outputs) transformer.compile(optimizer="rmsprop" loss="categorical_crossentropy") target sequence one step in the future and this is a lone transformerencoder for binary classification of integer sequences inputs = keras.input(shape=(sequence_length) dtype="int64") x = positionalembedding(sequence_length vocab_size embed_dim)(inputs) x = transformerencoder(embed_dim dense_dim num_heads)(x) x = layers.globalmaxpooling1d()(x) outputs = layers.dense(1 activation="sigmoid")(x) model = keras.model(inputs outputs) model.compile(optimizer="rmsprop" loss="binary_crossentropy") full implementations of the transformerencoder the transformerdecoder and the positionalembedding layer are provided in chapter 11. 14.1.7 the space of possibilities what will you build with these techniques? remember building deep learning models is like playing with lego bricks layers can be plugged together to map essentially anything to anything given that you have appropriate training data available and that the mapping is achievable via a continuous geometric transformation of reasonable complexity. the space of possibilities is infinite. this section offers a few examples to inspire you to think beyond the basic classification and regression tasks that have tra.ditionally been the bread and butter of machine learning. i ve sorted my suggested applications by input and output modalities in the follow.ing list. note that quite a few of them stretch the limits of what is possible although a model could be trained on all of these tasks in some cases such a model probably wouldn t generalize far from its training data. sections 14.2 through 14.4 will address how these limitations could be lifted in the future mapping vector data to vector data predictive healthcare mapping patient medical records to predictions of patient outcomes behavioral targeting mapping a set of website attributes with data on how long a user will spend on the website product quality control mapping a set of attributes relative to an instance of a manufactured product with the probability that the product will fail by next year mapping image data to vector data medical assistant mapping slides of medical images to a prediction about the presence of a tumor self-driving vehicle mapping car dashcam video frames to steering wheel angle commands and gas and braking commands board game ai mapping go or chess boards to the next player move diet helper mapping pictures of a dish to its calorie count age prediction mapping selfies to the age of the person mapping timeseries data to vector data weather prediction mapping timeseries of weather data in a grid of locations to the temperature in a specific place one week later brain-computer interfaces mapping timeseries of magnetoencephalogram (meg) data to computer commands behavioral targeting mapping timeseries of user interactions on a website to the probability that a user will buy something mapping text to text machine translation mapping a paragraph in one language to a translated version in a different language smart reply mapping emails to possible one-line replies question answering mapping general-knowledge questions to answers summarization mapping a long article to a short summary of the article mapping images to text text transcription mapping images that contain a text element to the corre.sponding text string captioning mapping images to short captions describing the contents of the images mapping text to images conditioned image generation mapping a short text description to images matching the description logo generation/selection mapping the name and description of a company to a logo suggestion mapping images to images super-resolution mapping downsized images to higher-resolution versions of the same images visual depth sensing mapping images of indoor environments to maps of depth predictions mapping images and text to text visual qa mapping images and natural language questions about the con.tents of images to natural language answers mapping video and text to text video qa mapping short videos and natural language questions about the contents of videos to natural language answers almost anything is possible but not quite anything. you ll see in the next section what we can t do with deep learning. 14.2 the limitations of deep learning the space of applications that can be implemented with deep learning is infinite. and yet many applications remain completely out of reach for current deep learning tech.niques even given vast amounts of human-annotated data. say for instance that you could assemble a dataset of hundreds of thousands even millions of english-language descriptions of the features of a software product written by a product manager as well as the corresponding source code developed by a team of engineers to meet these requirements. even with this data you could not train a deep learning model to read a product description and generate the appropriate codebase. that s just one example among many. in general anything that requires reasoning like programming or apply.ing the scientific method long-term planning and algorithmic data manipulation is out of reach for deep learning models no matter how much data you throw at them. even learning a simple sorting algorithm with a deep neural network is tremendously difficult. this is because a deep learning model is just a chain of simple continuous geometric transformations mapping one vector space into another. all it can do is map one data manifold x into another manifold y assuming the existence of a learnable continu.ous transform from x to y. a deep learning model can be interpreted as a kind of program but inversely most programs can t be expressed as deep learning models. for most tasks either there exists no corresponding neural network of reasonable size that solves the task or even if one exists it may not be learnable the corresponding geometric transform may be far too complex or there may not be appropriate data available to learn it. scaling up current deep learning techniques by stacking more layers and using more training data can only superficially palliate some of these issues. it won t solve the more fundamental problems that deep learning models are limited in what they can represent and that most of the programs you may wish to learn can t be expressed as a continuous geometric morphing of a data manifold. the limitations of deep learning 14.2.1 the risk of anthropomorphizing machine learning models one real risk with contemporary ai is misinterpreting what deep learning models do and overestimating their abilities. a fundamental feature of humans is our theory of mind our tendency to project intentions beliefs and knowledge on the things around us. drawing a smiley face on a rock suddenly makes it happy in our minds. applied to deep learning this means that for instance when we re able to some.what successfully train a model to generate captions to describe pictures we re led to believe that the model understands the contents of the pictures and the cap.tions it generates. then we re surprised when any slight departure from the sort of images present in the training data causes the model to generate completely absurd captions (see figure 14.1). figure 14.1 failure of an image-captioning system based on deep learning in particular this is highlighted by adversarial examples which are samples fed to a deep learning network that are designed to trick the model into misclassifying them. you re already aware that for instance it s possible to do gradient ascent in input space to generate inputs that maximize the activation of some convnet filter this is the basis of the filter-visualization technique introduced in chapter 9 as well as the deepdream algorithm from chapter 12. similarly through gradient ascent you can slightly modify an image in order to maximize the class prediction for a given class. by taking a picture of a panda and adding to it a gibbon gradient we can get a neural network to classify the panda as a gibbon (see figure 14.2). this evidences both the brittleness of these models and the deep difference between their input-to-output mapping and our human perception. in short deep learning models don t have any understanding of their input at least not in a human sense. our own understanding of images sounds and language is grounded in our sensorimotor experience as humans. machine learning models have no access to such experiences and thus can t understand their inputs in a human-relatable way. by annotating large numbers of training examples to feed into our models we get them to learn a geometric transform that maps data to human concepts on a specific set of examples but this mapping is a simplistic sketch of the f(x) f(x) gibbon class gradient original model in our minds the one developed from our experience as embodied agents. it s like a dim image in a mirror (see figure 14.3). the models you create will take any shortcut available to fit their training data. for instance image models tend to rely more on local textures than on a global understanding of the input images a model trained on a dataset that features both leopards and sofas is likely to classify a leopard-pattern sofa as an actual leopard. labeled data embodied abstract concepts exemplifying machine learning real world human experience in human mind these concepts model may not always doesn t match the matches the transfer well to human mental model training data the real world it came from figure 14.3 current machine learning models like a dim image in a mirror the limitations of deep learning as a machine learning practitioner always be mindful of this and never fall into the trap of believing that neural networks understand the tasks they perform they don t at least not in a way that would make sense to us. they were trained on a different far narrower task than the one we wanted to teach them that of mapping training inputs to training targets point by point. show them anything that deviates from their train.ing data and they will break in absurd ways. 14.2.2 automatons vs. intelligent agents there are fundamental differences between the straightforward geometric morphing from input to output that deep learning models do and the way humans think and learn. it isn t just the fact that humans learn by themselves from embodied experience instead of being presented with explicit training examples. the human brain is an entirely different beast compared to a differentiable parametric function. let s zoom out a little bit and ask what s the purpose of intelligence? why did it arise in the first place? we can only speculate but we can make fairly informed specu.lations. we can start by looking at brains the organ that produces intelligence. brains are an evolutionary adaption a mechanism developed incrementally over hun.dreds of millions of years via random trial-and-error guided by natural selection that dramatically expanded the ability of organisms to adapt to their environment. brains originally appeared more than half a billion years ago as a way to store and execute behav.ioral programs. behavioral programs are just sets of instructions that make an organ.ism reactive to its environment if this happens then do that. they link the organism s sensory inputs to its motor controls. in the beginning brains would have served to hardcode behavioral programs (as neural connectivity patterns) which would allow an organism to react appropriately to its sensory input. this is the way insect brains still work flies ants c. elegans (see figure 14.4) etc. because the origi.nal source code of these programs was dna which would get decoded as neural connectivity patterns evolution was suddenly able to search over behavior space in a largely unbounded way a major evolutionary shift. evolution was the programmer and brains were computers carefully executing the code evolution gave them. because neural connectivity is a very general computing substrate the sensorimotor space of all brain-enabled species could suddenly start undergoing a dramatic expansion. eyes ears mandibles 4 legs 24 legs as long as you have a brain evolution will kindly figure out for you behavioral programs that make good use of these. brains can handle any modality or combination of modali.ties you throw at them. now mind you these early brains weren t exactly intelligent per se. they were very much automatons they would merely execute behavioral programs hardcoded in the organism s dna. they could only be described as intelligent in the same sense that a thermostat is intelligent. or a list-sorting program. or . . . a trained deep neural net.work (of the artificial kind). this is an important distinction so let s look at it care.fully what s the difference between automatons and actual intelligent agents? 14.2.3 local generalization vs. extreme generalization seventeenth century french philosopher and scientist ren descartes wrote in 1637 an illuminating comment that perfectly captures this distinction long before the rise of ai and in fact before the first mechanical computer (which his colleague pascal would create five years later). descartes tells us in reference to automatons even though such machines might do some things as well as we do them or perhaps even better they would inevitably fail in others which would reveal they were acting not through understanding but only from the disposition of their organs. ren descartes discourse on the method (1637) there it is. intelligence is characterized by understanding and understanding is evi.denced by generalization the ability to handle whatever novel situation may arise. how do you tell the difference between a student that has memorized the past three years of exam questions but has no understanding of the subject and a student that the limitations of deep learning actually understands the material? you give them a brand new problem. an automa.ton is static crafted to accomplish specific things in a specific context if this then that while an intelligent agent can adapt on the fly to novel unexpected situations. when an automaton is exposed to something that doesn t match what it is pro.grammed to do (whether we re talking about human-written programs evolution-generated programs or the implicit programming process of fitting a model on a training data set) it will fail. meanwhile intelligent agents like humans will use their understanding to find a way forward. humans are capable of far more than mapping immediate stimuli to immediate responses as a deep net or an insect would. we maintain complex abstract models of our current situation of ourselves and of other people and we can use these mod.els to anticipate different possible futures and perform long-term planning. you can merge together known concepts to represent something you ve never experienced before like imagining what you d do if you won the lottery or picturing how your friend would react if you discreetly replaced her keys with exact copies made of elastic rubber. this ability to handle novelty and what-ifs to expand our mental model space far beyond what we can experience directly to leverage abstraction and reasoning is the defining characteristic of human cognition. i call it extreme generalization an ability to adapt to novel never-before-experienced situations using little data or even no new data at all. this capability is key to the intelligence displayed by humans and advanced animals. this stands in sharp contrast with what automaton-like systems do. a very rigid automaton wouldn t feature any generalization at all it would be incapable of han.dling anything that it wasn t precisely told about in advance. a python dict or a basic question-answering program implemented as hardcoded if-then-else statements would fall into this category. deep nets do slightly better they can successfully process inputs that deviate a bit from what they re familiar with which is precisely what makes them useful. our cats vs. dogs model from chapter 8 could classify cat or dog pictures it had not seen before as long as they were close enough to what it was trained on. however deep nets are limited to what i call local generalization (see fig.ure 14.5) the mapping from inputs to outputs performed by a deep net quickly stops making sense as inputs start deviating from what the net saw at training time. deep nets can only generalize to known unknowns to factors of variation that were anticipated during model development and that are extensively featured in the training data such as different camera angles or lighting conditions for pet pic.tures. that s because deep nets generalize via interpolation on a manifold (remem.ber chapter 5) any factor of variation in their input space needs to be captured by the manifold they learn. that s why basic data augmentation is so helpful in improv.ing deep net generalization. unlike humans these models have no ability to impro.vise in the face of situations for which little or no data is available (like winning the lottery or being handed rubber keys) that only share abstract commonalities with past situations. lower-intelligence system lower information conversion ratio higher-intelligence system higher information conversion ratio consider for instance the problem of learning the appropriate launch parameters to get a rocket to land on the moon. if you used a deep net for this task and trained it using supervised learning or reinforcement learning you d have to feed it tens of thousands or even millions of launch trials you d need to expose it to a dense sampling of the input space in order for it to learn a reliable mapping from input space to out.put space. in contrast as humans we can use our power of abstraction to come up with physical models rocket science and derive an exact solution that will land the rocket on the moon in one or a few trials. similarly if you developed a deep net con.trolling a human body and you wanted it to learn to safely navigate a city without getting hit by cars the net would have to die many thousands of times in various situa.tions until it could infer that cars are dangerous and develop appropriate avoidance behaviors. dropped into a new city the net would have to relearn most of what it knows. on the other hand humans are able to learn safe behaviors without having to die even once again thanks to our power of abstract modeling of novel situations. 14.2.4 the purpose of intelligence this distinction between highly adaptable intelligent agents and rigid automatons leads us back to brain evolution. why did brains originally a mere medium for natu.ral evolution to develop behavioral automatons eventually turn intelligent? like every significant evolutionary milestone it happened because natural selection con.straints encouraged it to happen. brains are responsible for behavior generation. if the set of situations an organism had to face was mostly static and known in advance behavior generation would be an easy problem evolution would just figure out the correct behaviors via random trial and error and hardcode them into the organism s dna. this first stage of brain evo.lution brains as automatons would already be optimal. however crucially as organism complexity and alongside it environmental complexity kept increas.ing the situations that animals had to deal with became much more dynamic and the limitations of deep learning more unpredictable. a day in your life if you look closely is unlike any day you ve ever experienced and unlike any day ever experienced by any of your evolutionary ancestors. you need to be able to face unknown and surprising situations constantly. there is no way for evolution to find and hardcode as dna the sequence of behaviors you ve been executing to successfully navigate your day since you woke up a few hours ago. it has to be generated on the fly every day. the brain as a good behavior-generation engine simply adapted to fit this need. it optimized for adaptability and generality rather than merely optimizing for fitness to a fixed set of situations. this shift likely occurred multiple times throughout evolu.tionary history resulting in highly intelligent animals in very distant evolutionary branches apes octopuses ravens and more. intelligence is an answer to challenges presented by complex dynamic ecosystems. that s the nature of intelligence it is the ability to efficiently leverage the informa.tion at your disposal in order to produce successful behavior in the face of an uncer.tain ever-changing future. what descartes calls understanding is the key to this remarkable capability the power to mine your past experience to develop modular reusable abstractions that can be quickly repurposed to handle novel situations and achieve extreme generalization. 14.2.5 climbing the spectrum of generalization as a crude caricature you could summarize the evolutionary history of biological intelligence as a slow climb up the spectrum of generalization. it started with automaton-like brains that could only perform local generalization. over time evolution started producing organisms capable of increasingly broader generalization that could thrive in ever-more complex and variable environments. eventually in the past few millions of years an instant in evolutionary terms certain hominin species started trending toward an implementation of biological intelligence capable of extreme generaliza.tion precipitating the start of the anthropocene and forever changing the history of life on earth. the progress of ai over the past 70 years bears striking similarities to this evolu.tion. early ai systems were pure automatons like the eliza chat program from the 1960s or shrdlu2 a 1970 ai capable of manipulating simple objects from natural language commands. in the 1990s and 2000s we saw the rise of machine learning sys.tems capable of local generalization which could deal with some level of uncertainty and novelty. in the 2010s deep learning further expanded the local-generalization power of these systems by enabling engineers to leverage much larger datasets and much more expressive models. today we may be on the cusp of the next evolutionary step. there is increasing interest in systems that could achieve broad generalization which i define as the ability 2 terry winograd procedures as a representation for data in a computer program for understanding natural language (1971). to deal with unknown unknowns within a single broad domain of tasks (including situa.tions the system was not trained to handle and that its creators could not have antici.pated). for instance a self-driving car capable of safely dealing with any situation you throw at it or a domestic robot that could pass the woz test of intelligence enter.ing a random kitchen and making a cup of coffee.3 by combining deep learning and painstakingly handcrafted abstract models of the world we re already making visible progress toward these goals. however for the time being ai remains limited to cognitive automation the intel.ligence label in artificial intelligence is a category error. it would be more accurate to call our field artificial cognition with cognitive automation and artificial intelli.gence being two nearly independent subfields within it. in this subdivision artifi.cial intelligence would be a greenfield where almost everything remains to be discovered. now i don t mean to diminish the achievements of deep learning. cognitive auto.mation is incredibly useful and the way deep learning models are capable of automat.ing tasks from exposure to data alone represents an especially powerful form of cognitive automation far more practical and versatile than explicit programming. doing this well is a game-changer for essentially every industry. but it s still a long way from human (or animal) intelligence. our models so far can only perform local gen.eralization they map space x to space y via a smooth geometric transform learned from a dense sampling of x-to-y data points and any disruption within spaces x or y invalidates this mapping. they can only generalize to new situations that stay similar to past data whereas human cognition is capable of extreme generalization quickly adapting to radically novel situations and planning for long-term future situations. 14.3 setting the course toward greater generality in ai to lift some of the limitations we have discussed and create ai that can compete with human brains we need to move away from straightforward input-to-output mappings and on to reasoning and abstraction. in the following couple of sections we ll take a look at what the road ahead may look like. 14.3.1 on the importance of setting the right objective the shortcut rule biological intelligence was the answer to a question asked by nature. likewise if we want to develop true artificial intelligence first we need to be asking the right questions. an effect you see constantly in systems design is the shortcut rule if you focus on optimizing one success metric you will achieve your goal but at the expense of every.thing in the system that wasn t covered by your success metric. you end up taking every available shortcut toward the goal. your creations are shaped by the incentives you give yourself. 3 fast company wozniak could a computer make a cup of coffee? (march 2010) http//mng.bz/pjmp. setting the course toward greater generality in ai you see this often in machine learning competitions. in 2009 netflix ran a challenge that promised a $1 million prize to the team that achieved the highest score on a movie recommendation task. it ended up never using the system created by the winning team because it was way too complex and compute-intensive. the winners had optimized for prediction accuracy alone what they were incentivized to achieve at the expense of every other desirable characteristic of the system inference cost maintainability and explainability. the shortcut rule holds true in most kaggle competitions as well the models produced by kaggle winners can rarely if ever be used in production. the shortcut rule has been everywhere in ai over the past few decades. in the 1970s psychologist and computer science pioneer allen newell concerned that his field wasn t making any meaningful progress toward a proper theory of cognition proposed a new grand goal for ai chess-playing. the rationale was that playing chess in humans seemed to involve perhaps even require capabilities such as percep.tion reasoning and analysis memory study from books and so on. surely if we could build a chess-playing machine it would have to feature these attributes as well. right? over two decades later the dream came true in 1997 ibm s deep blue beat gary kasparov the best chess player in the world. researchers had then to contend with the fact that creating a chess-champion ai had taught them little about human intelli.gence. the alpha beta algorithm at the heart of deep blue wasn t a model of the human brain and couldn t generalize to tasks other than similar board games. it turned out it was easier to build an ai that could only play chess than to build an arti.ficial mind so that s the shortcut researchers took. so far the driving success metric of the field of ai has been to solve specific tasks from chess to go from mnist classification to imagenet from atari arcade games to starcraft and dota 2. consequently the history of the field has been defined by a series of successes where we figured out how to solve these tasks with.out featuring any intelligence. if that sounds like a surprising statement keep in mind that human-like intelli.gence isn t characterized by skill at any particular task rather it is the ability to adapt to novelty to efficiently acquire new skills and master never-seen-before tasks. by fix.ing the task you make it possible to provide an arbitrarily precise description of what needs to be done either via hardcoding human-provided knowledge or by supplying humongous amounts of data. you make it possible for engineers to buy more skill for their ai by just adding data or adding hardcoded knowledge without increasing the generalization power of the ai (see figure 14.6). if you have near-infinite training data even a very crude algorithm like nearest-neighbor search can play video games with superhuman skill. likewise if you have a near-infinite amount of human-written if-then-else statements. that is until you make a small change to the rules of the game the kind a human could adapt to instantly that will require the non-intelligent system to be retrained or rebuilt from scratch. in short by fixing the task you remove the need to handle uncertainty and novelty and since the nature of intelligence is the ability to handle uncertainty and novelty you re effectively removing the need for intelligence. and because it s always easier to find a non-intelligent solution to a specific task than to solve the general problem of intelligence that s the shortcut you will take 100% of the time. humans can use their general intelligence to acquire skills at any new task but in reverse there is no path from a collection of task-specific skills to general intelligence. 14.3.2 a new target to make artificial intelligence actually intelligent and give it the ability to deal with the incredible variability and ever-changing nature of the real world we first need to move away from seeking to achieve task-specific skill and instead start targeting generalization power itself. we need new metrics of progress that will help us develop increasingly intelligent systems. metrics that will point in the right direction and that will give us an actionable feedback signal. as long as we set our goal to be create a model that solves task x the shortcut rule will apply and we ll end up with a model that does x period. in my view intelligence can be precisely quantified as an efficiency ratio the conver.sion ratio between the amount of relevant information you have available about the world (which could be either past experience or innate prior knowledge) and your future operating area the set of novel situations where you will be able to produce appropriate behav.ior (you can view this as your skillset). a more intelligent agent will be able to handle a broader set of future tasks and situations using a smaller amount of past experience. to measure such a ratio you just need to fix the information available to your system its experience and its prior knowledge and measure its performance on a set of ref.erence situations or tasks that are known to be sufficiently different from what the sys.tem has had access to. trying to maximize this ratio should lead you toward intelligence. crucially to avoid cheating you re going to need to make sure you test the system only on tasks it wasn t programmed or trained to handle in fact you need tasks that the creators of the system could not have anticipated. setting the course toward greater generality in ai in 2018 and 2019 i developed a benchmark dataset called the abstraction and rea.soning corpus (arc)4 that seeks to capture this definition of intelligence. arc is meant to be approachable by both machines and humans and it looks very similar to human iq tests such as raven s progressive matrices. at test time you ll see a series of tasks. each task is explained via three or four examples that take the form of an input grid and a corresponding output grid (see figure 14.7). you ll then be given a brand new input grid and you ll have three tries to produce the correct output grid before mov.ing on to the next task. compared to iq tests two things are unique about arc. first arc seeks to measure generalization power by only testing you on tasks you ve never seen before. that means that arc is a game you can t practice for at least in theory the tasks you will get tested on will have their own unique logic that you will have to understand on the fly. you can t just memorize specific strategies from past tasks. in addition arc tries to control for the prior knowledge that you bring to the test. you never approach a new problem entirely from scratch you bring to it preexisting skills and information. arc makes the assumption that all test takers should start from the set of knowledge priors called core knowledge priors that represent the knowledge systems that humans are born with. unlike an iq test arc tasks will never involve acquired knowledge like english sentences for instance. unsurprisingly deep-learning-based methods (including models trained on extremely large amounts of external data like gpt-3) have proven entirely unable to solve arc tasks because these tasks are non-interpolative and thus are a poor fit for curve-fitting. meanwhile average humans have no issue solving these tasks on the first try without any practice. when you see a situation like this where humans as young as 4 fran ois chollet on the measure of intelligence (2019) https//arxiv.org/abs/1911.01547. five are able to naturally perform something that seems to be completely out of reach for modern ai technology that s a clear signal that something interesting is going on that we re missing something. what would it take to solve arc? hopefully this challenge will get you thinking. that s the entire point of arc to give you a goal of a different kind that will nudge you in a new direction hopefully a productive direction. now let s take a quick look at the key ingredients you re going to need if you want to answer the call. 14.4 implementing intelligence the missing ingredients so far you ve learned that there s a lot more to intelligence than the sort of latent manifold interpolation that deep learning does. but what then do we need to start building real intelligence? what are the core pieces that are currently eluding us? 14.4.1 intelligence as sensitivity to abstract analogies intelligence is the ability to use your past experience (and innate prior knowledge) to face novel unexpected future situations. now if the future you had to face was truly novel sharing no common ground with anything you ve seen before you d be unable to react to it no matter how intelligent you were. intelligence works because nothing is ever truly without precedent. when we encounter something new we re able to make sense of it by drawing analogies to our past experience by articulating it in terms of the abstract concepts we ve collected over time. a person from the 17th century seeing a jet plane for the first time might describe it as a large loud metal bird that doesn t flap its wings. a car? that s a horse.less carriage. if you re trying to teach physics to a grade schooler you can explain how electricity is like water in a pipe or how space-time is like a rubber sheet getting dis.torted by heavy objects. besides such clear-cut explicit analogies we re constantly making smaller implicit analogies every second with every thought. analogies are how we navigate life. going to a new supermarket? you ll find your way by relating it to similar stores you ve been to. talking to someone new? they ll remind you of a few people you ve met before. even seemingly random patterns like the shape of clouds instantly evoke in us vivid images an elephant a ship a fish. these analogies aren t just in our minds either physical reality itself is full of isomor. phisms. electromagnetism is analogous to gravity. animals are all structurally similar to each other due to shared origins. silica crystals are similar to ice crystals. and so on. i call this the kaleidoscope hypothesis our experience of the world seems to feature incredible complexity and never-ending novelty but everything in this sea of complex.ity is similar to everything else. the number of unique atoms of meaning that you need to describe the universe you live in is relatively small and everything around you is a recombination of these atoms. a few seeds endless variation much like what goes on inside a kaleidoscope where a few glass beads are reflected by a system of mirrors to produce rich seemingly ever-changing patterns (see figure 14.8). implementing intelligence the missing ingredients generalization power intelligence is the ability to mine your experience to identify these atoms of meaning that can seemingly be reused across many different situations. once extracted they re called abstractions. whenever you encounter a new situation you make sense of it via your accumulated collection of abstractions. how do you identify reusable atoms of meaning? simply by noticing when two things are similar by noticing analogies. if something is repeated twice then both instances must have a single origin like in a kaleidoscope. abstraction is the engine of intelligence and analogy-making is the engine that produces abstraction. in short intelligence is literally sensitivity to abstract analogies and that s in fact all there is to it. if you have a high sensitivity to analogies you will extract powerful abstractions from little experience and you will be able to use these abstractions to operate in a maximally large area of future experience space. you will be maximally efficient in converting past experience into the ability to handle future novelty. 14.4.2 the two poles of abstraction if intelligence is sensitivity to analogies then developing artificial intelligence should start with spelling out a step-by-step algorithm for analogy-making. analogy-making starts with comparing things to one other. crucially there are two distinct ways to compare things from which arise two different kinds of abstraction two modes of thinking each better suited to a different kind of problem. together these two poles of abstrac.tion form the basis for all of our thoughts. the first way to relate things to each other is similarity comparison which gives rise to value-centric analogies. the second way is exact structural match which gives rise to program-centric analogies (or structure-centric analogies). in both cases you start from instances of a thing and you merge together related instances to produce an abstraction that captures the common elements of the underlying instances. what varies is how you tell that two instances are related and how you merge instances into abstractions. let s take a close look at each type. value-centric analogy let s say you come across a number of different beetles in your backyard belonging to multiple species. you ll notice similarities between them. some will be more similar to one another and some will be less similar the notion of similarity is implicitly a smooth continuous distance function that defines a latent manifold where your instances live. once you ve seen enough beetles you can start clustering more similar instances together and merging them into a set of prototypes that captures the shared visual features of each cluster (see figure 14.9). this prototype is abstract it doesn t look like any specific instance you ve seen though it encodes properties that are com.mon across all of them. when you encounter a new beetle you won t need to compare it to every single beetle you ve seen before in order to know what to do with it. you can simply compare it to your handful of prototypes so as to find the closest prototype the beetle s category and use it to make useful predictions is the beetle likely to bite you? will it eat your apples? instances in the wild similarity clustering abstract prototypes does this sound familiar? it s pretty much a description of what unsupervised machine learning (such as the k-means clustering algorithm) does. in general all of modern machine learning unsupervised or not works by learning latent manifolds that describe a space of instances encoded via prototypes. (remember the convnet features you visu.alized in chapter 9? they were visual prototypes.) value-centric analogy is the kind of analogy-making that enables deep learning models to perform local generalization. it s also what many of your own cognitive abilities run on. as a human you per.form value-centric analogies all the time. it s the type of abstraction that underlies pattern recognition perception and intuition. if you can do a task without thinking about it implementing intelligence the missing ingredients you re relying heavily on value-centric analogies. if you re watching a movie and you start subconsciously categorizing the different characters into types that s value-centric abstraction. program-centric analogy crucially there s more to cognition than the kind of immediate approximative intuitive categorization that value-centric analogy enables. there s another type of abstraction-generation mechanism that s slower exact deliberate program-centric (or structure-centric) analogy. in software engineering you often write different functions or classes that seem to have a lot in common. when you notice these redundancies you start asking could there be a more abstract function that performs the same job that could be reused twice? could there be an abstract base class that both of my classes could inherit from? the definition of abstraction you re using here corresponds to program-centric anal.ogy. you re not trying to compare your classes and functions by how similar they look the way you d compare two human faces via an implicit distance function. rather you re interested in whether there are parts of them that have exactly the same structure. you re looking for what is called a subgraph isomorphism (see figure 14.10) programs can be represented as graphs of operators and you re trying to find subgraphs (pro.gram subsets) that are exactly shared across your different programs. instance instance instance instance ls = obj.as_list() my_list = get_data() ls_sum = 0 ls_entries = 0 for n in ls if n is not none ls_sum += n ls_entries += 1 avg = ls sum / ls entries print('avg' avg) total = 0 num_elems = 0 for e in my_list if e is not none total += e num_elems += 1 mean = total / num_elems update_mean(mean) shared abstraction shared abstraction def compute_mean(ls) total = 0 num_elems = 0 for e in ls if e is not none total += e num_elems += 1 return total / num_elems figure 14.10 program-centric analogy identifies and isolates isomorphic substructures across different instances this kind of analogy-making via exact structural match within different discrete struc.tures isn t at all exclusive to specialized fields like computer science or mathematics you re constantly using it without noticing. it underlies reasoning planning and the general concept of rigor (as opposed to intuition). any time you re thinking about objects connected to each other by a discrete network of relationships (rather than a continuous similarity function) you re leveraging program-centric analogies. cognition as a combination of both kinds of abstraction let s compare these two poles of abstraction side by side (see table 14.1). table 14.1 the two poles of abstraction value-centric abstraction program centric abstraction relates things by distance relates things by exact structural match continuous grounded in geometry discrete grounded in topology produces abstractions by averaging instances into prototypes produces abstractions by isolating isomorphic substructures across instances underlies perception and intuition underlies reasoning and planning immediate fuzzy approximative slow exact rigorous requires a lot of experience to produce reliable results experience-efficient can operate on as few as two instances everything we do everything we think is a combination of these two types of abstrac.tion. you d be hard-pressed to find tasks that only involve one of the two. even a seem.ingly pure perception task like recognizing objects in a scene involves a fair amount of implicit reasoning about the relationships between the objects you re look.ing at. and even a seemingly pure reasoning task like finding the proof of a mathe.matical theorem involves a good amount of intuition. when a mathematician puts their pen to the paper they ve already got a fuzzy vision of the direction in which they re going. the discrete reasoning steps they take to get to the destination are guided by high-level intuition. these two poles are complementary and it s their interleaving that enables extreme generalization. no mind could be complete without both of them. 14.4.3 the missing half of the picture by this point you should start seeing what s missing from modern deep learning it s very good at encoding value-centric abstraction but it has basically no ability to generate program-centric abstraction. human-like intelligence is a tight interleaving of both types so we re literally missing half of what we need arguably the most important half. now here s a caveat. so far i ve presented each type of abstraction as entirely sep.arate from the other opposite even. in practice however they re more of a spec.trum to an extent you could do reasoning by embedding discrete programs in continuous manifolds just like you may fit a polynomial function through any set of discrete points as long as you have enough coefficients. and inversely you could use discrete programs to emulate continuous distance functions after all when you re the future of deep learning doing linear algebra on a computer you re working with continuous spaces entirely via discrete programs that operate on ones and zeros. however there are clearly types of problems that are better suited to one or the other. try to train a deep learning model to sort a list of five numbers for instance. with the right architecture it s not impossible but it s an exercise in frustration. you ll need a massive amount of training data to make it happen and even then the model will still make occasional mistakes when presented with new numbers. and if you want to start sorting lists of 10 numbers instead you ll need to completely retrain the model on even more data. meanwhile writing a sorting algorithm in python takes just a few lines and the resulting program once validated on a couple more exam.ples will work every time on lists of any size. that s pretty strong generalization going from a couple of demonstration examples and test examples to a program that can successfully process literally any list of numbers. in reverse perception problems are a terrible fit for discrete reasoning processes. try to write a pure-python program to classify mnist digits without using any machine learning technique you re in for a ride. you ll find yourself painstakingly coding functions that can detect the number of closed loops in a digit the coordi.nates of the center of mass of a digit and so on. after thousands of lines of code you might achieve . . . 90% test accuracy. in this case fitting a parametric model is much simpler it can better utilize the large amount of data that s available and it achieves much more robust results. if you have lots of data and you re faced with a problem where the manifold hypothesis applies go with deep learning. for this reason it s unlikely that we ll see the rise of an approach that would reduce reasoning problems to manifold interpolation or that would reduce percep.tion problems to discrete reasoning. the way forward in ai is to develop a unified framework that incorporates both types of abstract analogy-making. let s examine what that might look like. 14.5 the future of deep learning given what we know of how deep nets work their limitations and what they re cur.rently missing can we predict where things are headed in the medium term? follow.ing are some purely personal thoughts. note that i don t have a crystal ball so a lot of what i anticipate may fail to become reality. i m sharing these predictions not because i expect them to be proven completely right in the future but because they re inter.esting and actionable in the present. at a high level these are the main directions in which i see promise models closer to general-purpose computer programs built on top of far richer primitives than the current differentiable layers. this is how we ll get to reasoning and abstraction the lack of which is the fundamental weakness of current models. a fusion between deep learning and discrete search over program spaces with the former providing perception and intuition capabilities and the latter providing reason.ing and planning capabilities. greater systematic reuse of previously learned features and architectures such as meta-learning systems using reusable and modular program subroutines. additionally note that these considerations aren t specific to the sort of supervised learning that has been the bread and butter of deep learning so far rather they re applicable to any form of machine learning including unsupervised self-supervised and reinforcement learning. it isn t fundamentally important where your labels come from or what your training loop looks like these different branches of machine learn.ing are different facets of the same construct. let s dive in. 14.5.1 models as programs as noted in the previous section a necessary transformational development that we can expect in the field of machine learning is a move away from models that perform purely pattern recognition and can only achieve local generalization toward models capa.ble of abstraction and reasoning that can achieve extreme generalization. current ai pro.grams that are capable of basic forms of reasoning are all hardcoded by human programmers for instance software that relies on search algorithms graph manipula.tion and formal logic. that may be about to change thanks to program synthesis a field that is very niche today but i expect to take off in a big way over the next few decades. program synthesis consists of automatically generating simple programs by using a search algorithm (possi.bly genetic search as in genetic programming) to explore a large space of possible pro.grams (see figure 14.11). the search stops when a program is found that matches the required specifications often provided as a set of input-output pairs. this is highly rem.iniscent of machine learning given training data provided as input-output pairs we find a program that matches inputs to outputs and can generalize to new inputs. the differ.ence is that instead of learning parameter values in a hardcoded program (a neural net.work) we generate source code via a discrete search process (see table 14.2). input [3 51 27] output [1 23 57] program specification input [8 52 91 13] output [1 25 89 13] vocabulary of building blocks += if == for -else search process *+= -= *= candidate programs which are then tested against the specification. the search continues until a valid program is found. the future of deep learning table 14.2 machine learning vs. program synthesis machine learning program synthesis model differentiable parametric function model graph of operators from a programming language engine gradient descent engine discrete search (such as genetic search) requires a lot of data to produce reliable results data-efficient can work with a couple of training examples program synthesis is how we re going to add program-centric abstraction capabilities to our ai systems. it s the missing piece of the puzzle. i mentioned earlier that deep learning techniques were entirely unusable on arc a reasoning-focused intelligence test. meanwhile very crude program-synthesis approaches are already producing very promising results on this benchmark. 14.5.2 blending together deep learning and program synthesis of course deep learning isn t going anywhere. program synthesis isn t its replace.ment it is its complement. it s the hemisphere that has been so far missing from our artificial brains. we re going to be leveraging both in combination. there are two major ways this will take place 1 developing systems that integrate both deep learning modules and discrete algorithmic modules 2 using deep learning to make the program search process itself more efficient let s review each of these possible avenues. integrating deep learning modules and algorithmic modules into hybrid systems today the most powerful ai systems are hybrid they leverage both deep learning models and handcrafted symbol-manipulation programs. in deepmind s alphago for example most of the intelligence on display is designed and hardcoded by human programmers (such as monte carlo tree search). learning from data happens only in specialized submodules (value networks and policy networks). or consider autono.mous vehicles a self-driving car is able to handle a large variety of situations because it maintains a model of the world around it a literal 3d model full of assumptions hardcoded by human engineers. this model is constantly updated via deep learning perception modules that interface it with the surroundings of the car. for both of these systems alphago and self-driving vehicles the combination of human-created discrete programs and learned continuous models is what unlocks a level of performance that would be impossible with either approach in isolation such as an end-to-end deep net or a piece of software without ml elements. so far the dis.crete algorithmic elements of such hybrid systems are painstakingly hardcoded by human engineers. but in the future such systems may be fully learned with no human involvement. what will this look like? consider a well-known type of network rnns. it s import.ant to note that rnns have slightly fewer limitations than feedforward networks. that s because rnns are a bit more than mere geometric transformations they re geometric transformations repeatedly applied inside a for loop. the temporal for loop is itself hardcoded by human developers it s a built-in assumption of the network. nat.urally rnns are still extremely limited in what they can represent primarily because each step they perform is a differentiable geometric transformation and they carry information from step to step via points in a continuous geometric space (state vectors). now imagine a neural network that s augmented in a similar way with programming primitives but instead of a single hardcoded for loop with hard-coded continuous-space memory the network includes a large set of programming primitives that the model is free to manipulate to expand its processing function such as if branches while statements variable creation disk storage for long-term mem.ory sorting operators advanced data structures (such as lists graphs and hash tables) and many more. the space of programs that such a network could represent would be far broader than what can be represented with current deep learning mod.els and some of these programs could achieve superior generalization power. impor.tantly such programs will not be differentiable end-to-end though specific modules will remain differentiable and thus will need to be generated via a combination of dis.crete program search and gradient descent. we ll move away from having on one hand hardcoded algorithmic intelligence (handcrafted software) and on the other hand learned geometric intelligence (deep learning). instead we ll have a blend of formal algorithmic modules that provide rea.soning and abstraction capabilities and geometric modules that provide informal intuition and pattern-recognition capabilities (see figure 14.12). the entire system will be learned with little or no human involvement. this should dramatically expand the scope of problems that can be solved with machine learning the space of pro.grams that we can generate automatically given appropriate training data. systems like alphago or even rnns can be seen as a prehistoric ancestor of such hybrid algorithmic-geometric models. the future of deep learning using deep learning to guide program search today program synthesis faces a major obstacle it s tremendously inefficient. to cari.cature program synthesis works by trying every possible program in a search space until it finds one that matches the specification provided. as the complexity of the program specification increases or as the vocabulary of primitives used to write pro.grams expands the program search process runs into what s known as combinatorial explosion where the set of possible programs to consider grows very fast in fact much faster than merely exponentially fast. as a result today program synthesis can only be used to generate very short programs. you re not going to be generating a new os for your computer anytime soon. to move forward we re going to need to make program synthesis efficient by bringing it closer to the way humans write software. when you open your editor to code up a script you re not thinking about every possible program you could poten.tially write. you only have in mind a handful of possible approaches you can use your understanding of the problem and your past experience to drastically cut through the space of possible options to consider. deep learning can help program synthesis do the same although each specific program we d like to generate might be a fundamentally discrete object that performs non-interpolative data manipulation evidence so far indicates that the space of all useful programs may look a lot like a continuous manifold. that means that a deep learning model that has been trained on millions of successful program-generation episodes might start to develop solid intuition about the path through program space that the search process should take to go from a specification to the corresponding program just like a software engineer might have immediate intuition about the overall archi.tecture of the script they re about to write about the intermediate functions and classes they should use as stepping stones on the way to the goal. remember that human reasoning is heavily guided by value-centric abstraction that is to say by pattern recognition and intuition. program synthesis should be too. i expect the general approach of guiding program search via learned heuristics to see increasing research interest over the next ten to twenty years. 14.5.3 lifelong learning and modular subroutine reuse if models become more complex and are built on top of richer algorithmic primitives this increased complexity will require higher reuse between tasks rather than training a new model from scratch every time we have a new task or a new dataset. many datasets don t contain enough information for us to develop a new complex model from scratch and it will be necessary to use information from previously encountered datasets (much as you don t learn english from scratch every time you open a new book that would be impossible). training models from scratch on every new task is also inefficient due to the large overlap between the current tasks and previously encountered tasks. a remarkable observation has been made repeatedly in recent years training the same model to do several loosely connected tasks at the same time results in a model that s better at each task. for instance training the same neural machine-translation model to perform both english-to-german translation and french-to-italian transla.tion will result in a model that s better at each language pair. similarly training an image-classification model jointly with an image-segmentation model sharing the same convolutional base results in a model that s better at both tasks. this is fairly intuitive there s always some information overlap between seemingly disconnected tasks and a joint model has access to a greater amount of information about each individual task than a model trained on that specific task only. currently when it comes to model reuse across tasks we use pretrained weights for models that perform common functions such as visual feature extraction. you saw this in action in chapter 9. in the future i expect a generalized version of this to be commonplace we ll use not only previously learned features (submodel weights) but also model architectures and training procedures. as models become more like pro.grams we ll begin to reuse program subroutines like the functions and classes found in human programming languages. think of the process of software development today once an engineer solves a spe.cific problem (http queries in python for instance) they package it as an abstract reusable library. engineers who face a similar problem in the future will be able to search for existing libraries download one and use it in their own project. in a similar way in the future meta-learning systems will be able to assemble new programs by sift.ing through a global library of high-level reusable blocks. when the system finds itself developing similar program subroutines for several different tasks it can come up with an abstract reusable version of the subroutine and store it in the global library (see figure 14.13). these subroutines can be either geometric (deep learning modules the future of deep learning with pretrained representations) or algorithmic (closer to the libraries that contem.porary software engineers manipulate). 14.5.4 the long-term vision in short here s my long-term vision for machine learning models will be more like programs and will have capabilities that go far beyond the continuous geometric transformations of the input data we currently work with. these programs will arguably be much closer to the abstract mental mod.els that humans maintain about their surroundings and themselves and they will be capable of stronger generalization due to their rich algorithmic nature. in particular models will blend algorithmic modules providing formal reasoning search and abstraction capabilities with geometric modules providing informal intuition and pattern-recognition capabilities. this will achieve a blend of value-centric and program-centric abstraction. alphago or self-driving cars (sys.tems that required a lot of manual software engineering and human-made design decisions) provide an early example of what such a blend of symbolic and geometric ai could look like. such models will be grown automatically rather than hardcoded by human engi.neers using modular parts stored in a global library of reusable subroutines a library evolved by learning high-performing models on thousands of previous tasks and datasets. as frequent problem-solving patterns are identified by the meta-learning system they will be turned into reusable subroutines much like functions and classes in software engineering and added to the global library. the process that searches over possible combinations of subroutines to grow new models will be a discrete search process (program synthesis) but it will be heavily guided by a form of program-space intuition provided by deep learning. this global subroutine library and associated model-growing system will be able to achieve some form of human-like extreme generalization given a new task or sit.uation the system will be able to assemble a new working model appropriate for the task using very little data thanks to rich program-like primitives that generalize well and extensive experience with similar tasks. in the same way humans can quickly learn to play a complex new video game if they have expe.rience with many previous games because the models derived from this previ.ous experience are abstract and program-like rather than a basic mapping between stimuli and action. as such this perpetually learning model-growing system can be interpreted as an artificial general intelligence (agi). but don t expect any singularitarian robot apocalypse to ensue that s pure fantasy coming from a long series of profound misunderstandings of both intelligence and technology. such a critique how.ever doesn t belong in this book. 14.6 staying up to date in a fast-moving field as final parting words i want to give you some pointers about how to keep learning and updating your knowledge and skills after you ve turned the last page of this book. the field of modern deep learning as we know it today is only a few years old despite a long slow prehistory stretching back decades. with an exponential increase in financial resources and research headcount since 2013 the field as a whole is now moving at a frenetic pace. what you ve learned in this book won t stay relevant for.ever and it isn t all you ll need for the rest of your career. fortunately there are plenty of free online resources that you can use to stay up to date and expand your horizons. here are a few. 14.6.1 practice on real-world problems using kaggle an effective way to acquire real-world experience is to try your hand at machine learning competitions on kaggle (https//kaggle.com). the only real way to learn is through prac.tice and actual coding that s the philosophy of this book and kaggle competitions are the natural continuation of this. on kaggle you ll find an array of constantly renewed data science competitions many of which involve deep learning prepared by companies interested in obtaining novel solutions to some of their most challenging machine learn.ing problems. fairly large monetary prizes are offered to top entrants. most competitions are won using either the xgboost library (for shallow machine learning) or keras (for deep learning). so you ll fit right in! by participating in a few competitions maybe as part of a team you ll become more familiar with the practical side of some of the advanced best practices described in this book especially hyperpa.rameter tuning avoiding validation-set overfitting and model ensembling. 14.6.2 read about the latest developments on arxiv deep learning research in contrast with some other scientific fields takes places com.pletely in the open. papers are made publicly and freely accessible as soon as they re finalized and a lot of related software is open source. arxiv (https//arxiv.org) pro.nounced archive (the x stands for the greek chi) is an open-access preprint server for physics mathematics and computer science research papers. it has become the de facto way to stay up to date on the bleeding edge of machine learning and deep learning. the large majority of deep learning researchers upload any paper they write to arxiv shortly after completion. this allows them to plant a flag and claim a specific finding without waiting for a conference acceptance (which takes months) which is necessary given the fast pace of research and the intense competition in the field. it also allows the field to move extremely fast all new findings are immediately available for all to see and to build on. an important downside is that the sheer quantity of new papers posted every day on arxiv makes it impossible to even skim them all and the fact that they aren t peer-reviewed makes it difficult to identify those that are both important and high quality. it s challenging and becoming increasingly more so to find the signal in the noise. but some tools can help in particular you can use google scholar (https//scholar.google.com) to keep track of publications by your favorite authors. 14.6.3 explore the keras ecosystem with over one million users as of late 2021 and still growing keras has a large ecosys.tem of tutorials guides and related open source projects your main reference for working with keras is the online documentation at https//keras.io. in particular you ll find extensive developer guides at https//keras.io/guides and you ll find dozens of high-quality keras code exam.ples at https//keras.io/examples. make sure to check them out! the keras source code can be found at https//github.com/keras-team/keras. you can ask for help and join deep learning discussions on the keras mailing list keras-users@googlegroups.com. you can follow me on twitter @fchollet. final words this is the end of deep learning with python second edition. i hope you ve learned a thing or two about machine learning deep learning keras and maybe even cognition in general. learning is a lifelong journey especially in the field of ai where we have far more unknowns on our hands than certitudes. so please go on learning question.ing and researching. never stop! because even given the progress made so far most of the fundamental questions in ai remain unanswered. many haven t even been properly asked yet. symbols @tf.function decorator 198 270 a ablation studies 251 abstraction poles of 455 458 cognition as combination of both kinds of abstraction 458 program-centric analogy 457 458 value-centric analogy 456 457 activation function 63 adapt() method of preprocess. ing layers 317 add() method of sequential class 174 adversarial examples 443 affine transform 46 agi (artificial general intelligence) 465 ai (artificial intelligence) 1 432 greater generality in 450 454 new target for 452 454 shortcut rule 450 452 overview of 2 3 promise of 12 13 various approaches to 432 see also deep learning ai summer 433 ai winter 12 algorithmic modules 465 analytical engine 3 annotations 96 anomaly detection 281 anthropomorphizing machine-learning models 443 445 arc (abstraction & reasoning corpus) dataset 453 arrow of time 137 arxiv 466 467 assign() method of variable class 77 automated hyperparameter tun. ing software 165 automatic differentiation 60 automatic shape inference 85 87 automatons 445 automl (automated machine learning) 419 420 average presence 211 b backpropagation algorithm 10 56 backward pass 57 bag-of-2-grams/bag-of-3. grams 314 bag-of-words models 313 batch() method of dataset class 218 batch axis 35 batch dimension 35 batches 96 batch gradient descent 53 batch normalization 255 257 438 index batchnormalization layer 194 256 344 batch training 82 bayesianoptimization tuner 415 bias vector 256 bidirectional layer 306 bidirectional recurrent layers 300 bidirectional rnns 304 307 bigrams with binary encoding 324 325 with tf-idf encoding 325 327 binary classification 97 106 437 building model 99 102 generating predictions on new data 105 imdb dataset 97 preparing data 98 99 validating approach 102 105 binary_crossentropy 102 437 binary encoding bigrams with 324 325 single words (unigrams) with 322 324 blas (basic linear algebra subprograms) 39 boston housing price dataset 114 broadcasting 40 41 broad generalization 449 build() method of layer class 84 174 415 469 cache() method of dataset class 429 __call__() method of layer class 63 86 call() method of layer class 84 182 358 callbacks 187 189 earlystopping and model-checkpoint callbacks 188 189 text generation 372 375 writing 189 190 cam (class activation map) visualization 273 categorical_crossentropy 108 437 categorical encoding 107 categories 27 causal padding 360 celeba dataset 404 405 chain rule 55 56 channels-first convention 37 channels-last convention 37 character-level tokenization 313 chatbots 350 classes 27 96 classification 281 binary classification 97 106 building model 99 102 generating predictions on new data 105 preparing data 98 99 validating approach 102 105 multiclass classification 106 113 building model 108 109 generating predictions on new data 111 112 handling labels and loss 112 preparing data 107 108 sufficiently large intermedi. ate layers 112 113 validating approach 109 111 cognitive automation 450 colaboratory 73 75 first steps with 73 installing packages with pip 74 using gpu runtime 75 columns 32 combinatorial explosion 463 compilation step 29 compile() method of model class 89 185 408 430 computation graphs 56 60 197 compute_dtype attribute of layer class 425 compute_mask() method of layer class 333 computer vision convnets 202 211 convolution operation 204 209 interpreting what convnets learn 261 279 max-pooling operation 209 211 modern architecture patterns 248 261 training from scratch on small datasets 211 224 essential tasks 238 240 image segmentation example 240 248 pretrained models 224 237 feature extraction 225 233 fine-tuning 234 237 concept drift 158 concept vectors for image editing 393 constant initializer 335 content function 384 conv1d (1d convolution) layer 84 291 438 conv2d (2d convolution) layer 84 202 245 290 438 conv3d (3d convolution) layer 291 438 convnets 202 211 438 convolution operation 204 209 border effects and padding 207 208 convolution strides 208 209 interpreting what convnets learn 261 279 visualizing convnet filters 268 272 visualizing heatmaps of class activation 273 279 visualizing intermediate activations 262 268 max-pooling operation 209 211 modern architecture patterns 248 261 batch normalization 255 257 depthwise separable convolutions 257 259 mini xception-like model 259 261 modularity hierarchy and reuse 249 251 residual connections 251 255 training from scratch on small datasets 211 224 data augmentation 221 224 data preprocessing 217 221 downloading data 212 215 model building 215 217 relevance of deep learning for small-data problems 212 convolutional base 225 convolutional networks 201 436 convolution kernel 206 convolution operation 204 209 border effects and padding 207 208 convolution strides 208 209 correlations 385 cost function 9 cudnn kernel 258 302 d data-annotation infrastructure 157 158 data augmentation 211 feature extraction with 231 233 feature extraction without 229 231 training convnets from scratch on small datasets 221 224 data distillation 28 data parallelism 425 data preparation classification binary classification 98 99 multiclass classification 107 108 model development 161 162 data preparation (continued) natural language processing 311 319 text splitting (tokenization) 313 314 text standardization 312 313 textvectorization layer 316 319 vocabulary indexing 314 315 regression 114 115 text generation 370 371 timeseries 285 287 data representativeness 137 dataset class 92 218 285 317 dataset curation 142 data type 33 dcgan (deep convolutional gan) 402 decision trees 15 16 decoders 350 399 deepdream 376 383 define task 154 dense layer 28 63 76 99 203 289 355 437 densely connected layers 84 densely connected networks 437 438 dense representations 330 dense sampling 142 448 deploy model 154 depth axis (channels axis) 205 depthwise separable convolution layer 438 depthwise separable convolutions 257 259 derivatives chaining 55 61 automatic differentiation with computation graphs 56 60 chain rule 55 56 gradient tape in tensorflow 60 61 defined 49 50 of tensor operations 51 52 develop model 154 dimensionality 31 dimensions 31 discriminator model 405 406 discriminator network 401 distance function 384 456 dot operation 55 dot product 41 dropout recurrent to fight overfitting 300 regularizing models 150 152 dropout layer 151 194 223 dtype attribute of layer class 33 e eager execution 78 early stopping 144 earlystopping callback 144 188 189 417 efficiency ratio 452 element-wise operations 38 40 embedding layer 331 332 encoder 350 evaluate() method of model class 92 183 event detection 281 exact structural match 455 expert systems 3 extreme generalization 130 447 f failure modes 166 feature engineering 17 143 144 feature extraction 181 225 233 with data augmentation 231 233 without data augmentation 229 231 feature extractor model 268 feature maps 205 feature normalization 194 features axis 35 feature selection 126 feedforward networks 293 filters 206 fit() method of model class 29 62 76 183 218 424 core keras apis 91 leveraging with custom train. ing loops 198 200 flatten layer 203 438 floating-point precision 422 424 forecasting 281 for loops 183 218 302 415 462 forward pass 57 fourier transform 281 freezing layers 231 fully connected layers 84 functional api 173 access to layer connectivity 180 181 example of 176 177 multi-input multi-output models overview of 178 179 training 179 g gan model 403 gans (generative adversarial networks) 391 401 411 adversarial network 408 410 celeba dataset 404 405 discriminator model 405 406 generator model 407 implementation tricks 403 404 schematic implementation 402 403 generalization 122 142 152 446 dataset curation 142 early stopping 144 feature engineering 143 144 local vs. extreme 446 448 nature of in deep learning 127 133 interpolation 129 130 manifold hypothesis 128 129 training data 132 133 why deep learning works 130 132 regularizing models 145 152 adding dropout 150 152 adding weight regularization 148 149 reducing network size 145 148 spectrum of 449 450 underfitting and overfitting 122 127 ambiguous features 124 noisy training data 123 rare features and spurious correlations 125 127 generative deep learning deepdream 376 383 generative adversarial networks 401 411 adversarial network 408 410 bag of tricks 403 404 generative deep learning (continued) celeba dataset 404 405 discriminator 405 406 generator 407 schematic gan implementation 402 403 image generation with varia.tional autoencoders 391 401 concept vectors for image editing 393 overview of 393 395 sampling from latent spaces of images 391 392 with keras 396 401 neural style transfer 383 391 content loss 384 style loss 384 385 with keras 385 391 text generation 366 376 callback with variable-temperature sampling 372 375 generating sequence data 367 history of 366 367 sampling strategy 368 369 with keras 369 372 generative learning 193 generator model 407 generator networks 401 403 geometric interpretation of deep learning 47 48 of tensor operations 44 47 geometric modules 465 geometric relationships 329 get_config() method of layer class 344 get_vocabulary() method of textvectorization layer 317 google colab 428 429 gpus gpu runtime 75 multi-gpu training 425 427 acquiring two or more gpus 425 426 single-host multi-device synchronous training 426 427 with mixed precision 422 425 floating-point precision 422 424 mixed-precision training 425 gradient-based optimization 48 61 backpropagation algorithm 55 61 automatic differentiation with computation graphs 56 60 chain rule 55 56 gradient tape in tensorflow 60 61 derivatives defined 49 50 of tensor operations 51 52 stochastic gradient descent 52 55 gradient boosting machines 15 16 gradient descent 49 268 432 gradient descent parameters 138 139 gradient propagation 22 gradienttape class 60 61 65 76 78 79 196 268 gradienttape.gradient() method 79 gradienttape.watch() method 79 gram matrix 384 greedy sampling 368 ground-truth 96 grouped convolutions 342 gru (gated recurrent unit) 303 gru layer 297 439 h hierarchical representations learning 7 history class 91 hold-out validation 134 135 house price prediction example 113 120 boston housing price dataset 114 building model 115 generating predictions on new data 119 preparing data 114 115 validating approach using k-fold validation 115 119 hsv (hue-saturation-value) format 5 hyperband tuner 415 hypermodel class 415 hyperparameter optimization 413 420 automated machine learning 419 420 crafting the right search space 418 419 kerastuner 414 418 hypothesis space 7 87 101 248 436 i if branches 462 if statements 156 image classification 239 image data 37 image_dataset_from_directory utility function 217 321 image segmentation 239 248 imdb dataset 97 320 322 import tensorflow 426 inference core keras apis 93 training vs. 194 information arbitrage 308 information-distillation 8 information distillation pipeline 267 information leaks 134 information location 396 information shortcut 252 initial state 355 input class 175 instance segmentation 240 interpolation 129 130 iterated k-fold validation with shuffling 136 j javascript library 168 jupyter notebooks 72 73 k kaggle 466 kaggle package 212 kaleidoscope hypothesis 454 keras core apis 84 93 configuring learning process 88 90 inference 93 layers 84 87 loss functions 90 keras (continued) models 87 88 monitoring loss and met.rics on validation data 91 92 deepdream with 377 382 deep-learning workspaces 71 75 colaboratory 73 75 jupyter notebooks 72 73 exploring ecosystem 467 history of 71 image generation with varia. tional autoencoders 396 401 model building apis 173 185 choosing 185 functional api 176 181 mixing and matching differ. ent components 184 model subclassing 182 184 sequential model 174 176 neural style transfer with 385 391 overview of 69 71 recurrent layers in 296 300 text generation with 369 372 preparing data 370 371 transformer-based sequence-to-sequence model 371 372 training and evaluation loops built-in 185 192 callbacks 187 189 complete 195 197 fit() method 198 200 low-level usage of metrics 195 monitoring and visualiza.tion with tensorboard 190 192 tf.function 197 198 training vs. inference 194 writing 192 200 writing metrics 186 187 workflows 173 keras.applications module 226 keras.applications.xception.pre. process_input function 274 keras.callbacks.callback class 189 keras.callbacks module 188 keras.callbacks.tensorboard callback 192 keras.datasets 154 keras.datasets.imdb 315 keras.metrics.mean metric 195 keras.metrics.metric class 186 keras.metrics module 186 kerastuner 414 418 kernel function 15 kernel methods 14 15 kernel trick 15 key attributes 32 34 k-fold validation iterated with shuffling 136 model evaluation 135 136 regression 115 119 l l1 regularization 148 l2 regularization 148 labels 27 96 language modeling 367 latent space 367 layer class 84 85 87 173 layer compatibility 85 layered representations learning 7 layernormalization layer 344 layers 28 84 87 432 access to layer connectivity 180 181 automatic shape inference 85 87 base layer class 84 85 embedding layer 331 332 recurrent layers in keras 296 300 stacking recurrent layers 303 304 sufficiently large intermediate layers 112 113 textvectorization layer 316 319 layer subclass 183 358 leakyrelu layer 404 learning 5 133 learning rate 53 lenet 14 lifelong learning 463 465 lightgbm library 19 linear classifiers 79 84 linear transformations 46 101 local generalization 130 447 local slope 51 logistic regression 13 113 loss functions 9 29 88 90 432 loss value 96 lstm (long short term mem.ory) algorithm 20 292 lstm layer 84 297 439 m machine learning 13 20 432 automated 419 420 decision trees 15 16 generalization 121 133 142 152 dataset curation 142 early stopping 144 feature engineering 143 144 nature of in deep learning 127 133 regularizing models 145 152 underfitting and overfitting 122 127 gradient boosting machines 15 16 improving model fit 138 141 increasing model capacity 140 141 leveraging better architec.ture priors 139 140 tuning key gradient descent parameters 138 139 kernel methods 14 15 learning rules and representa.tions from data 4 7 model evaluation 133 137 common-sense baseline 136 137 training validation and test sets 133 136 modern landscape of 18 20 neural networks 14 16 17 overview of 3 4 probabilistic modeling 13 14 random forests 15 16 see also deep learning machine learning workflow model deployment 165 170 deploying model as rest api 166 167 deploying model in browsers 168 deploying model on devices 167 168 explaining work to stake.holders and setting expectations 165 166 machine learning workflow (continued) maintaining model 170 monitoring model 169 170 optimization 169 shipping inference model 166 169 model development 161 165 beating baselines 163 164 choosing evaluation protocol 162 163 developing model that overfits 164 preparing data 161 162 regularizing and tuning model 165 task definition 155 160 choosing measure of success 160 collecting dataset 156 159 framing problem 155 156 understanding data 160 universal workflow 435 436 machine translation 350 mae (mean absolute error) 115 288 manifold hypothesis 128 129 manifolds 47 map() method of dataset class 218 219 masking 332 334 mask token 315 matmul operation 76 matrices 32 maximal presence 211 maximizing margin 15 maxpooling2d layer 202 245 max-pooling operation 209 211 meaningfully transform data 5 mean squared error 115 metrics 89 low-level usage of 195 monitoring loss and metrics on validation data 91 92 writing 186 187 metrics property 198 mhr (modularity-hierarchy. reuse) formula 249 mini-batches 96 mini-batch sgd (mini-batch sto. chastic gradient descent) 53 mirroredstrategy object 426 missing values 162 model() constructor 179 model architecture 419 modelcheckpoint callback 188 189 219 model class 87 398 model ensembling 420 421 model evaluation 133 137 common-sense baseline 136 137 keras training and evaluation loops built-in 185 192 callbacks 187 189 complete 195 197 fit() method 198 200 low-level usage of metrics 195 monitoring and visualiza.tion with tensorboard 190 192 tf.function 197 198 training vs. inference 194 writing 192 200 writing metrics 186 187 training validation and test sets 133 136 hold-out validation 134 135 iterated k-fold validation with shuffling 136 k-fold validation 135 136 model.fit() method 102 187 model.layers property 181 model parallelism 425 model.save() method 189 model subclass 182 408 model subclassing 182 184 example of 182 183 responsibility for model logic 183 184 model.summary() method 244 296 378 model training 421 430 gpu with mixed precision 422 425 floating-point precision 422 424 mixed-precision training 425 importance of training data 132 133 keras training and evaluation loops built-in 185 192 callbacks 187 189 complete 195 197 fit() method 198 200 low-level usage of metrics 195 monitoring and visualiza.tion with tensorboard 190 192 tf.function 197 198 training vs. inference 194 writing 192 200 writing metrics 186 187 multi-gpu training 425 427 acquiring two or more gpus 425 426 single-host multi-device synchronous training 426 427 multi-input multi-output models 179 noisy training data 123 pretrained models for com. puter vision 224 237 feature extraction 225 233 fine-tuning 234 237 tpu training 428 430 google colab 428 429 step fusing 430 training convnets from scratch on small datasets 211 224 data augmentation 221 224 data preprocessing 217 221 downloading data 212 215 model building 215 217 relevance of deep learning for small-data problems 212 modular subroutine reuse 463 465 momentum 54 monitoring loss and metrics on validation data 91 92 model deployment 169 170 with tensorboard 190 192 movie review classification example 97 106 building model 99 102 generating predictions on new data 105 imdb dataset 97 preparing data 98 99 validating approach 102 105 mse (mean squared error) 119 289 438 mse function 105 multiclass classification 96 106 113 building model 108 109 generating predictions on new data 111 112 handling labels and loss 112 preparing data 107 108 sufficiently large intermediate layers 112 113 validating approach 109 111 multi-head attention 341 342 multiheadattention layer 339 multi-hot encode 98 multi-input multi-output mod. els overview of 178 179 training 179 multilabel categorical classification 437 multilabel classification 96 multilabel multiclass classification 106 n naivedense class 63 84 naivedense layer 86 naivesequential class 63 ndim attribute 31 33 neural networks 7 16 17 434 early 14 example of 27 30 61 66 evaluating model 66 full training loop 65 66 reimplementing 63 64 running one training step 64 65 gradient-based optimization 48 61 backpropagation algorithm 55 61 derivatives 49 52 stochastic gradient descent 52 55 tensor operations 38 48 broadcasting 40 41 element-wise operations 38 40 geometric interpretation of 44 47 geometric interpretation of deep learning 47 48 tensor product 41 43 tensor reshaping 43 44 tensors 31 38 data batches 35 higher-rank tensors 32 image data 37 key attributes 32 34 manipulating in numpy 34 matrices 32 real-world examples of 35 scalars 31 timeseries data 36 vector data 35 36 vectors 31 video data 37 38 neural style transfer 383 391 content loss 384 style loss 384 385 with keras 385 391 newswire classification example 106 113 building model 108 109 generating predictions on new data 111 112 handling labels and loss 112 preparing data 107 108 reuters dataset 106 107 sufficiently large intermediate layers 112 113 validating approach 109 111 n-gram tokenization 313 nlp (natural language processing) 310 imdb dataset 320 322 overview of 309 311 preparing data 311 319 text splitting (tokenization) 313 314 text standardization 312 313 textvectorization layer 316 319 vocabulary indexing 314 315 sequences 327 336 example of 328 329 padding and masking 332 334 when to use 349 350 word embeddings 329 332 334 336 sequence-to-sequence learning 350 363 machine translation example 351 354 with rnns 354 358 with transformer 358 363 sets 322 327 bigrams with binary encoding 324 325 bigrams with tf-idf encoding 325 327 single words (unigrams) with binary encoding 322 324 when to use 349 350 transformer architecture 336 350 multi-head attention 341 342 self-attention 337 341 transformer encoder 342 348 non-representative data 158 159 non-trainable weights 194 notebooks 72 number of axes 32 numpy manipulating tensors in 34 numpy.dot() function 41 numpy library 32 o object detection 239 objective function 9 occam s razor principle 148 on_batch_* methods of callback class 190 one-hot encoding 107 on_epoch_* methods of call. back class 190 optimization 50 54 122 optimizer 10 29 89 output feature map 205 overfitting 30 104 122 127 ambiguous features 124 noisy training data 123 rare features and spurious correlations 125 127 using recurrent dropout to fight 300 p padding convolution operation 207 208 sequences 332 334 pattern recognition 456 pip installing packages with 74 plot_model() utility 180 positionalembedding layer 361 440 positional encoding 344 346 348 439 predict() method of model class 93 111 119 183 229 269 270 prediction 96 prefetch() method of dataset class 218 pretrained word embeddings 331 probabilistic modeling 13 14 probability distribution 108 program-centric analogies 455 program-space intuition 465 program subroutines 464 program synthesis 460 463 integrating deep-learning modules and algorithmic modules into hybrid systems 461 462 using deep learning to guide program search 463 progressive disclosure of complexity 173 prototypes 456 q query-key-value model 340 341 r random forests 15 16 randomized a/b testing 169 randomsearch tuner 415 reconstruction loss 395 recurrent dropout 300 recurrent layer 84 recurrent networks 436 redundancy in data 137 regression 113 120 438 building model 115 generating predictions on new data 119 preparing data 114 115 validating approach 115 119 regularization 133 regularization loss 395 regularizing models 145 152 adding dropout 150 152 adding weight regularization 148 149 reducing network size 145 148 reinforcement learning 193 relu activation 99 404 relu function 100 representativity 163 rescaling layer 215 reset_state() method of metric class 187 reset_state() method of model class 195 residual connections 251 255 298 438 response map 206 result() method of metric class 187 reuters dataset 106 107 rgb (red-green-blue) format 5 rmsprop optimizer 62 rnns (recurrent neural networks) 281 293 300 439 bidirectional 304 307 recurrent layers in keras 296 300 stacking recurrent layers 303 304 using recurrent dropout to fight overfitting 300 roc auc (receiver operating characteristic curve) 160 rows 32 rules 4 s samples 27 96 samples axis 35 scalar regression 96 scalars 31 scales 379 scaling 46 scaling-up model training 421 430 gpu with mixed precision 422 425 multi-gpu training 425 427 tpu training 428 430 scikit-learn library 19 search space 414 search_space_summary() method of tuner class 416 second-order gradients 79 segmentation masks 240 self-attention 337 341 self-supervised learning 193 398 semantic relationships 329 semantic segmentation 240 separableconv1d layer 291 separableconv2d layer 259 290 438 sequence data 35 sequence models 320 sequences 327 336 example of 328 329 padding and masking 332 334 when to use 349 350 word embeddings 329 331 learning with embedding layer 331 332 pretrained 334 336 sequence-to-sequence learning 439 sequence-to-sequence model 340 350 363 371 machine translation example 351 354 text generation 371 372 with rnns 354 358 with transformer 358 363 sequential model 63 64 76 173 176 221 sets 322 327 bigrams with binary encoding 324 325 bigrams with tf-idf encoding 325 327 single words (unigrams) with binary encoding 322 324 when to use 349 350 sgd optimizer 389 shallow learning 7 shortcut rule 450 452 shuffle() method of dataset class 218 sigmoid activation 106 215 437 similarity comparison 455 simple model 148 simplernn layer 296 439 single-label categorical classification 437 single-label multiclass classification 106 single words (unigrams) 322 324 smile vector 393 softmax activation 108 437 softmax classification layer 28 softmax operation 55 softmax temperature parameter 368 sparse_categorical_crossentropy function 112 203 spatial location 245 stacking recurrent layers 300 step fusing 430 stochastic gradient descent 52 55 stochastic sampling 368 structured representations 330 style function 384 subgraph isomorphism 457 summary() method of model class 174 supervised learning 192 svm (support vector machine) 14 symbolic ai 3 symbolic tensor 177 t tape.gradient() function 79 tape.watch() function 79 target leaking 160 targets 96 192 targets array 286 temperature-forecasting example 281 293 1d convolutional model 290 291 basic machine-learning model 289 290 first recurrent baseline 292 293 non-machine-learning baseline 288 preparing data 285 287 tensorboard monitoring and visualization with 190 192 tensorflow deep-learning workspaces 71 75 colaboratory 73 75 jupyter notebooks 72 73 first steps with 75 84 constant tensors and variables 76 78 gradienttape api 78 79 linear classifier in tensorflow 79 84 tensor operations 78 gradient tape in 60 61 history of 71 overview of 69 tensorflow serving 166 tensor operations 26 38 48 78 broadcasting 40 41 derivatives of 51 52 element-wise operations 38 40 geometric interpretation of 44 47 geometric interpretation of deep learning 47 48 tensor product 41 43 tensor reshaping 43 44 tensor product 41 43 tensor reshaping 43 44 tensors 31 38 constant 76 78 data batches 35 higher-rank tensors 32 key attributes 32 34 manipulating in numpy 34 matrices 32 real-world examples of 35 image data 37 timeseries data 36 vector data 35 36 video data 37 38 scalars 31 vectors 31 tensor slicing 34 test_step() method of model class 196 text_dataset_from_directory function 321 370 text generation 350 366 376 callback with variable-temperature sampling 372 375 generating sequence data 367 history of 366 367 sampling strategy 368 369 with keras 369 372 preparing data 370 371 transformer-based sequence-to-sequence model 371 372 text splitting (tokenization) 313 314 text standardization 312 313 text summarization 350 textures 385 textvectorization layer 316 319 370 tf.data.dataset class 196 217 427 tf.function 197 198 tf-idf encoding bigrams with 325 327 tf-idf normalization 325 tf.string tensors 317 tf.variable class 60 77 theory of mind 443 timeseries kinds of tasks 280 281 recurrent neural networks 293 300 bidirectional 304 307 recurrent layers in keras 296 300 stacking recurrent layers 303 304 using recurrent dropout to fight overfitting 300 temperature-forecasting example 281 293 1d convolutional model 290 291 basic machine-learning model 289 290 first recurrent baseline 292 293 non-machine-learning baseline 288 preparing data 285 287 timeseries data (sequence data) 36 timeseries data 35 tokenization 311 total variation loss 388 tpu (tensor processing unit) 21 428 tpustrategy scope 428 tpu training 428 430 google colab 428 429 step fusing 430 trainable parameters 48 trainable variables 79 trainable weights 194 training loop 10 48 train_step() method of model class 198 398 transformer architecture 336 350 439 440 multi-head attention 341 342 self-attention 337 341 sequence-to-sequence learning 358 363 sequence-to-sequence model 371 372 transformer decoder 358 361 transformer encoder 342 348 transformerdecoder 440 transformerencoder 439 transformers 436 translating a vector 45 turing test 3 u underfitting 122 127 ambiguous features 124 noisy training data 123 rare features and spurious correlations 125 127 update_state() method of metric class 186 vaes (variational autoencod.ers) image generation with 391 401 concept vectors for image editing 393 overview of 393 395 sampling from latent spaces of images 391 392 with keras 396 401 validation binary classification 102 105 hold-out validation 134 135 k-fold validation 115 119 135 136 monitoring loss and metrics on validation data 91 92 multiclass classification 109 111 validation data 92 validation metrics 162 value-centric analogies 455 value normalization 161 vanishing gradients 251 297 variable class 76 variable_dtype attribute of layer class 425 variable-temperature sampling 372 375 vector data 35 vectorized targets 433 vectorizing text 311 vector regression 96 vectors 31 concept vectors for image editing 393 textvectorizationlayer 316 319 vector data 35 36 vectorization 161 video data 37 38 vocabulary indexing 314 315 w weight decay 148 weight pruning 169 weight quantization 169 weight regularization 148 149 weights 9 48 84 432 word embeddings 329 331 learning with embedding layer 331 332 pretrained 334 336 word-level tokenization 313 x xgboost library 19 466 python/deep learning deep learning with pythonsecond edition fran ois chollet r ecent innovations in deep learning unlock exciting new software capabilities like automated language translation image recognition and more. deep learning is quickly becoming essential knowledge for every software developer and modern tools like keras and tensorflow put it within your reach even if you have no background in mathematics or data science. is book shows you how to get started. deep learning with python second edition introduces the eld of deep learning using python and the powerful keras library. in this revised and expanded new edition keras creator fran ois chollet o ers insights for both novice and experi.enced machine learning practitioners. as you move through this book you ll build your understanding through intuitive explanations crisp illustrations and clear examples. you ll quickly pick up the skills you need to start developing deep-learning applications. what s inside deep learning from rst principles image classi cation and image segmentation time series forecasting text classi cation and machine translation text generation neural style transfer and image generation for readers with intermediate python skills. no previous experience with keras tensorflow or machine learning is required. fran ois chollet is a software engineer at google and creator of the keras deep-learning library. register this print book to get free access to all ebook formats. visit https//www.manning.com/freebook chollet is a master of pedagogy and explains complex concepts with minimal fuss cutting through the math with practical python code. he is also an experienced ml researcher and his insights on various model architectures or training tips are a joy to read. martin g rner google immerse yourself into this exciting introduction to the topic with lots of real-world examples. a must read for every deep learning practitioner. sayak paul carted e modern classic just got better. edmon begolioak ridge national laboratory truly the bible of deep learning. yiannis paraskevopoulosuniversity of west attica isbn 978-1-61729-686-4 manning $59.99 / can $79.99 [including ebook] 